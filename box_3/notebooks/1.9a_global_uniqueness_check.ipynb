{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.9a: Global Uniqueness Check\n",
    "\n",
    "**Critical verification:** We've been analyzing 4 black holes in the core cluster. But have we actually verified that there are ONLY 4 unique vectors in the entire vocabulary?\n",
    "\n",
    "**Method:**\n",
    "1. Load W in native bfloat16 (no conversions)\n",
    "2. Find all unique rows via exact equality\n",
    "3. Group tokens by their vector (equivalence classes)\n",
    "4. Report global black hole statistics\n",
    "\n",
    "**Expected results (if our analysis is correct):**\n",
    "- Total unique vectors < 151,936\n",
    "- In core cluster: exactly 4 unique vectors\n",
    "- Populations: 866, 734, 329, 249\n",
    "\n",
    "**Nightmare scenario:**\n",
    "- Fewer than 4 unique vectors (our analysis overcounted)\n",
    "- Populations don't match (wrong grouping)\n",
    "- More black holes outside the core (incomplete analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to analyze\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ml_dtypes\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_bf16_to_numpy_bf16(tensor):\n",
    "    \"\"\"Convert PyTorch bfloat16 tensor to numpy array with ml_dtypes.bfloat16 dtype.\"\"\"\n",
    "    return tensor.cpu().view(torch.uint16).numpy().view(ml_dtypes.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W: torch.Size([151936, 2560])\n",
      "Data type: torch.bfloat16\n",
      "Total tokens: 151,936\n"
     ]
    }
   ],
   "source": [
    "# Load W in bfloat16 (NATIVE format, no conversions)\n",
    "W_path = Path(f\"../tensors/{MODEL_NAME}/W.safetensors\")\n",
    "W_bf16 = load_file(W_path)[\"W\"]\n",
    "\n",
    "print(f\"Loaded W: {W_bf16.shape}\")\n",
    "print(f\"Data type: {W_bf16.dtype}\")\n",
    "print(f\"Total tokens: {W_bf16.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Core cluster: 2,179 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load core mask for comparison\n",
    "core_path = Path(f\"../tensors/{MODEL_NAME}/1.8a_core.safetensors\")\n",
    "core_data = load_file(core_path)\n",
    "\n",
    "core_mask = core_data[\"core_mask\"].to(torch.bool)\n",
    "n_core = core_data[\"n_core\"].item()\n",
    "\n",
    "print(f\"\\nCore cluster: {n_core:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find All Unique Vectors (Global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding all unique vectors (global search)...\n",
      "\n",
      "This may take a minute...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping tokens: 100%|██████████| 151936/151936 [00:09<00:00, 15357.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Grouping complete\n",
      "\n",
      "Unique vectors: 149,849\n",
      "Total tokens: 151,936\n",
      "Degenerate tokens: 2,087 (1.37%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinding all unique vectors (global search)...\\n\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "# Convert entire W to numpy bfloat16 for hashing\n",
    "W_np_bf16 = torch_bf16_to_numpy_bf16(W_bf16)\n",
    "\n",
    "# Group tokens by their vector\n",
    "# Use tuple of vector as key (hashable)\n",
    "vector_groups = defaultdict(list)\n",
    "\n",
    "for token_id in tqdm(range(W_bf16.shape[0]), desc=\"Grouping tokens\"):\n",
    "    vector = W_np_bf16[token_id]\n",
    "    vector_key = tuple(vector)  # Convert to hashable tuple\n",
    "    vector_groups[vector_key].append(token_id)\n",
    "\n",
    "n_unique = len(vector_groups)\n",
    "n_total = W_bf16.shape[0]\n",
    "\n",
    "print(f\"\\n✓ Grouping complete\")\n",
    "print(f\"\\nUnique vectors: {n_unique:,}\")\n",
    "print(f\"Total tokens: {n_total:,}\")\n",
    "print(f\"Degenerate tokens: {n_total - n_unique:,} ({(n_total - n_unique) / n_total * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Black Holes (Vectors with Multiple Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding black holes (degenerate vectors)...\n",
      "\n",
      "Black holes found: 13\n",
      "Total degenerate tokens: 2,100\n",
      "\n",
      "Top 20 black holes by population:\n",
      "Rank  Population  Sample Token IDs\n",
      "------------------------------------------------------------\n",
      "   1         814  80091, 119346, 119348, 123806, 123828, ...\n",
      "   2         704  125, 177, 178, 179, 181, ...\n",
      "   3         306  124, 123876, 123948, 124076, 124129, ...\n",
      "   4         228  124350, 124658, 125147, 125460, 126425, ...\n",
      "   5          11  123939, 131955, 131957, 134988, 134991, ...\n",
      "   6          10  119349, 125087, 126630, 137856, 138110, ...\n",
      "   7           6  126268, 132713, 138041, 146501, 148028, ...\n",
      "   8           5  132383, 132398, 139050, 142718, 142719\n",
      "   9           4  135619, 138490, 140815, 143457\n",
      "  10           4  136831, 138068, 138072, 139278\n",
      "  11           3  180, 138979, 141503\n",
      "  12           3  126775, 140303, 147056\n",
      "  13           2  126816, 147836\n",
      "\n",
      "✓ Black hole analysis complete\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinding black holes (degenerate vectors)...\\n\")\n",
    "\n",
    "# Find all vectors with more than 1 token\n",
    "black_holes = [(vector_key, token_ids) for vector_key, token_ids in vector_groups.items() \n",
    "               if len(token_ids) > 1]\n",
    "\n",
    "# Sort by population (largest first)\n",
    "black_holes.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "n_black_holes = len(black_holes)\n",
    "total_degenerate = sum(len(token_ids) for _, token_ids in black_holes)\n",
    "\n",
    "print(f\"Black holes found: {n_black_holes}\")\n",
    "print(f\"Total degenerate tokens: {total_degenerate:,}\")\n",
    "print()\n",
    "\n",
    "# Show top 20 black holes\n",
    "print(\"Top 20 black holes by population:\")\n",
    "print(\"Rank  Population  Sample Token IDs\")\n",
    "print(\"-\" * 60)\n",
    "for i, (vector_key, token_ids) in enumerate(black_holes[:20], 1):\n",
    "    sample_ids = token_ids[:5]  # Show first 5 token IDs\n",
    "    sample_str = \", \".join(str(tid) for tid in sample_ids)\n",
    "    if len(token_ids) > 5:\n",
    "        sample_str += \", ...\"\n",
    "    print(f\"{i:4d}  {len(token_ids):10,}  {sample_str}\")\n",
    "\n",
    "print(f\"\\n✓ Black hole analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on Core Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing core cluster...\n",
      "\n",
      "Black holes with tokens in core: 13\n",
      "\n",
      "Black holes in core cluster:\n",
      "Rank  Total Pop  In Core  Sample Token IDs\n",
      "----------------------------------------------------------------------\n",
      "   1        814      814  80091, 119346, 119348, 123806, 123828, ...\n",
      "   2        704      704  125, 177, 178, 179, 181, ...\n",
      "   3        306      306  124, 123876, 123948, 124076, 124129, ...\n",
      "   4        228      228  124350, 124658, 125147, 125460, 126425, ...\n",
      "   5         11       11  123939, 131955, 131957, 134988, 134991, ...\n",
      "   6         10       10  119349, 125087, 126630, 137856, 138110, ...\n",
      "   7          6        6  126268, 132713, 138041, 146501, 148028, ...\n",
      "   8          5        5  132383, 132398, 139050, 142718, 142719\n",
      "   9          4        4  135619, 138490, 140815, 143457\n",
      "  10          4        4  136831, 138068, 138072, 139278\n",
      "\n",
      "✓ Core analysis complete\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnalyzing core cluster...\\n\")\n",
    "\n",
    "# Get core token IDs\n",
    "core_token_ids = torch.where(core_mask)[0].tolist()\n",
    "\n",
    "# Find which black holes are in the core\n",
    "core_black_holes = []\n",
    "for vector_key, token_ids in black_holes:\n",
    "    # Check if ANY token from this black hole is in the core\n",
    "    overlap = set(token_ids) & set(core_token_ids)\n",
    "    if overlap:\n",
    "        # Count how many tokens from this BH are in the core\n",
    "        n_in_core = len(overlap)\n",
    "        core_black_holes.append((vector_key, token_ids, n_in_core))\n",
    "\n",
    "print(f\"Black holes with tokens in core: {len(core_black_holes)}\")\n",
    "print()\n",
    "\n",
    "# Sort by number in core\n",
    "core_black_holes.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"Black holes in core cluster:\")\n",
    "print(\"Rank  Total Pop  In Core  Sample Token IDs\")\n",
    "print(\"-\" * 70)\n",
    "for i, (vector_key, token_ids, n_in_core) in enumerate(core_black_holes[:10], 1):\n",
    "    sample_ids = token_ids[:5]\n",
    "    sample_str = \", \".join(str(tid) for tid in sample_ids)\n",
    "    if len(token_ids) > 5:\n",
    "        sample_str += \", ...\"\n",
    "    print(f\"{i:4d}  {len(token_ids):9,}  {n_in_core:7,}  {sample_str}\")\n",
    "\n",
    "print(f\"\\n✓ Core analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification Against Expected Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFICATION: COMPARING TO EXPECTED RESULTS\n",
      "================================================================================\n",
      "\n",
      "Expected (from 1.8e):\n",
      "  Number of black holes in core: 4\n",
      "  Populations: [866, 734, 329, 249]\n",
      "  Total degenerate in core: 2,178\n",
      "\n",
      "Actual (from global uniqueness check):\n",
      "  Number of black holes in core: 13\n",
      "  Top 4 populations: [814, 704, 306, 228]\n",
      "  Total in top 4: 2,052\n",
      "\n",
      "✗ Number of black holes MISMATCH (expected 4, found 13)\n",
      "✗ Populations MISMATCH\n",
      "  Expected: [249, 329, 734, 866]\n",
      "  Found: [228, 306, 704, 814]\n",
      "✗ Total degenerate tokens MISMATCH (expected 2,178, found 2,052)\n",
      "\n",
      "================================================================================\n",
      "VERDICT: ⚠ DISCREPANCY DETECTED ⚠\n",
      "================================================================================\n",
      "\n",
      "The global uniqueness check does NOT match our expected results.\n",
      "Further investigation required.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION: COMPARING TO EXPECTED RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Expected results from 1.8e\n",
    "expected_populations = [866, 734, 329, 249]\n",
    "expected_total = sum(expected_populations)\n",
    "expected_n_bh = 4\n",
    "\n",
    "print(\"Expected (from 1.8e):\")\n",
    "print(f\"  Number of black holes in core: {expected_n_bh}\")\n",
    "print(f\"  Populations: {expected_populations}\")\n",
    "print(f\"  Total degenerate in core: {expected_total:,}\")\n",
    "print()\n",
    "\n",
    "# Actual results\n",
    "actual_populations = [n_in_core for _, _, n_in_core in core_black_holes[:4]]\n",
    "actual_total = sum(actual_populations)\n",
    "actual_n_bh = len([bh for bh in core_black_holes if bh[2] > 1])\n",
    "\n",
    "print(\"Actual (from global uniqueness check):\")\n",
    "print(f\"  Number of black holes in core: {actual_n_bh}\")\n",
    "print(f\"  Top 4 populations: {actual_populations}\")\n",
    "print(f\"  Total in top 4: {actual_total:,}\")\n",
    "print()\n",
    "\n",
    "# Check if they match\n",
    "if actual_n_bh == expected_n_bh:\n",
    "    print(f\"✓ Number of black holes MATCHES ({actual_n_bh})\")\n",
    "else:\n",
    "    print(f\"✗ Number of black holes MISMATCH (expected {expected_n_bh}, found {actual_n_bh})\")\n",
    "\n",
    "if sorted(actual_populations) == sorted(expected_populations):\n",
    "    print(f\"✓ Populations MATCH: {sorted(actual_populations)}\")\n",
    "else:\n",
    "    print(f\"✗ Populations MISMATCH\")\n",
    "    print(f\"  Expected: {sorted(expected_populations)}\")\n",
    "    print(f\"  Found: {sorted(actual_populations)}\")\n",
    "\n",
    "if actual_total == expected_total:\n",
    "    print(f\"✓ Total degenerate tokens MATCHES ({actual_total:,})\")\n",
    "else:\n",
    "    print(f\"✗ Total degenerate tokens MISMATCH (expected {expected_total:,}, found {actual_total:,})\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Overall verdict\n",
    "if (actual_n_bh == expected_n_bh and \n",
    "    sorted(actual_populations) == sorted(expected_populations) and \n",
    "    actual_total == expected_total):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"VERDICT: ✓✓✓ ANALYSIS VERIFIED ✓✓✓\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(\"The 4 black holes identified in 1.8e are EXACTLY the degenerate\")\n",
    "    print(\"vectors found by global uniqueness check in native bfloat16.\")\n",
    "    print()\n",
    "    print(\"Our analysis is CORRECT. The earlier float32 analysis showing\")\n",
    "    print(\"13 black holes was an artifact of precision mismatch.\")\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"VERDICT: ⚠ DISCREPANCY DETECTED ⚠\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(\"The global uniqueness check does NOT match our expected results.\")\n",
    "    print(\"Further investigation required.\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Global Uniqueness Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving global uniqueness data...\n",
      "\n",
      "✓ Saved to 1.9a_global_uniqueness.safetensors\n",
      "  token_to_black_hole: torch.Size([151936])\n",
      "  n_unique_vectors: 149849\n",
      "  n_black_holes: 13\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "print(\"\\nSaving global uniqueness data...\\n\")\n",
    "\n",
    "# Create a mapping: token_id -> black_hole_id (or -1 if unique)\n",
    "token_to_bh = torch.full((W_bf16.shape[0],), -1, dtype=torch.int64)\n",
    "\n",
    "for bh_id, (vector_key, token_ids) in enumerate(black_holes):\n",
    "    for token_id in token_ids:\n",
    "        token_to_bh[token_id] = bh_id\n",
    "\n",
    "# Count unique vectors\n",
    "n_unique_tensor = torch.tensor(n_unique, dtype=torch.int64)\n",
    "n_black_holes_tensor = torch.tensor(n_black_holes, dtype=torch.int64)\n",
    "\n",
    "# Save\n",
    "output_path = Path(f\"../tensors/{MODEL_NAME}/1.9a_global_uniqueness.safetensors\")\n",
    "save_file({\n",
    "    'token_to_black_hole': token_to_bh,  # -1 = unique, >=0 = black hole ID\n",
    "    'n_unique_vectors': n_unique_tensor,\n",
    "    'n_black_holes': n_black_holes_tensor,\n",
    "}, str(output_path))\n",
    "\n",
    "print(f\"✓ Saved to {output_path.name}\")\n",
    "print(f\"  token_to_black_hole: {token_to_bh.shape}\")\n",
    "print(f\"  n_unique_vectors: {n_unique}\")\n",
    "print(f\"  n_black_holes: {n_black_holes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
