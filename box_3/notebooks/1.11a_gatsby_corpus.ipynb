{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 1.11a: Gatsby Corpus Preparation\n",
    "\n",
    "**Goal:** Download The Great Gatsby from Project Gutenberg and prepare a clean ASCII corpus.\n",
    "\n",
    "## Processing Steps\n",
    "\n",
    "1. Download UTF-8 plain text from Project Gutenberg\n",
    "2. Strip Gutenberg front/back matter\n",
    "3. Unwrap hard-wrapped paragraphs (preserve paragraph breaks)\n",
    "4. Convert to ASCII using Unidecode (proper transliteration)\n",
    "5. Export clean ASCII corpus\n",
    "\n",
    "## Output\n",
    "\n",
    "- **Clean corpus:** `../data/gatsby_clean.txt`\n",
    "- Pure 7-bit ASCII encoding\n",
    "- One paragraph per line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source URL (Project Gutenberg - UTF-8 plain text)\n",
    "GATSBY_URL = \"https://www.gutenberg.org/files/64317/64317-0.txt\"\n",
    "\n",
    "# Output paths\n",
    "RAW_PATH = \"../data/gatsby_raw.txt\"\n",
    "CLEAN_PATH = \"../data/gatsby_clean.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Create Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created ../data directory\n"
     ]
    }
   ],
   "source": [
    "Path(\"../data\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"✓ Created ../data directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Download Raw Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Raw corpus already exists at ../data/gatsby_raw.txt\n",
      "\n",
      "Raw corpus stats:\n",
      "  Characters: 270,822\n",
      "  Lines: 6,407\n",
      "  Encoding: UTF-8\n"
     ]
    }
   ],
   "source": [
    "raw_path = Path(RAW_PATH)\n",
    "\n",
    "if raw_path.exists():\n",
    "    print(f\"✓ Raw corpus already exists at {RAW_PATH}\")\n",
    "else:\n",
    "    print(f\"Downloading from Project Gutenberg...\")\n",
    "    urllib.request.urlretrieve(GATSBY_URL, RAW_PATH)\n",
    "    print(f\"✓ Downloaded to {RAW_PATH}\")\n",
    "\n",
    "# Read raw text (UTF-8)\n",
    "with open(RAW_PATH, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"\\nRaw corpus stats:\")\n",
    "print(f\"  Characters: {len(raw_text):,}\")\n",
    "print(f\"  Lines: {len(raw_text.splitlines()):,}\")\n",
    "print(f\"  Encoding: UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Strip Gutenberg Front/Back Matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Stripped Gutenberg front/back matter\n",
      "  Novel length: 270,690 characters\n"
     ]
    }
   ],
   "source": [
    "# Find the Gutenberg delimiters\n",
    "start_pattern = r'\\*\\*\\* START OF (?:THIS|THE) PROJECT GUTENBERG EBOOK .+ \\*\\*\\*'\n",
    "start_match = re.search(start_pattern, raw_text, re.IGNORECASE)\n",
    "\n",
    "if not start_match:\n",
    "    raise ValueError(\"Could not find Gutenberg START marker\")\n",
    "\n",
    "end_pattern = r'\\*\\*\\* END OF (?:THIS|THE) PROJECT GUTENBERG EBOOK .+ \\*\\*\\*'\n",
    "end_match = re.search(end_pattern, raw_text, re.IGNORECASE)\n",
    "\n",
    "if not end_match:\n",
    "    raise ValueError(\"Could not find Gutenberg END marker\")\n",
    "\n",
    "# Extract novel text\n",
    "novel_text = raw_text[start_match.end():end_match.start()].strip()\n",
    "\n",
    "print(f\"✓ Stripped Gutenberg front/back matter\")\n",
    "print(f\"  Novel length: {len(novel_text):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Unwrap Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Unwrapped paragraphs\n",
      "  Before: 270,690 characters\n",
      "  After:  268,403 characters\n",
      "  Paragraphs: 1,650\n"
     ]
    }
   ],
   "source": [
    "# Normalize line endings first\n",
    "text = novel_text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "# Split into paragraphs (separated by blank lines)\n",
    "paragraphs = re.split(r'\\n\\n+', text)\n",
    "\n",
    "# Unwrap each paragraph (join lines with spaces)\n",
    "unwrapped = []\n",
    "for para in paragraphs:\n",
    "    # Join all lines in the paragraph with spaces\n",
    "    lines = para.split('\\n')\n",
    "    unwrapped_para = ' '.join(line.strip() for line in lines if line.strip())\n",
    "    if unwrapped_para:\n",
    "        unwrapped.append(unwrapped_para)\n",
    "\n",
    "# Join paragraphs with single newlines\n",
    "clean_text = '\\n'.join(unwrapped)\n",
    "\n",
    "print(f\"✓ Unwrapped paragraphs\")\n",
    "print(f\"  Before: {len(novel_text):,} characters\")\n",
    "print(f\"  After:  {len(clean_text):,} characters\")\n",
    "print(f\"  Paragraphs: {len(unwrapped):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Convert to ASCII (Unidecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted to ASCII using Unidecode\n",
      "  Non-ASCII characters before: 4,785\n",
      "  Non-ASCII characters after:  0\n",
      "  Character count: 268,403 → 268,928\n",
      "  ✓ Verified: 100% ASCII (all characters < 128)\n"
     ]
    }
   ],
   "source": [
    "# Count non-ASCII characters before conversion\n",
    "non_ascii_before = sum(1 for c in clean_text if ord(c) >= 128)\n",
    "\n",
    "# Convert UTF-8 to ASCII using Unidecode\n",
    "# This properly transliterates: smart quotes → straight quotes, em-dash → --, etc.\n",
    "ascii_text = unidecode(clean_text)\n",
    "\n",
    "# Verify it's pure ASCII\n",
    "non_ascii_after = sum(1 for c in ascii_text if ord(c) >= 128)\n",
    "\n",
    "print(f\"✓ Converted to ASCII using Unidecode\")\n",
    "print(f\"  Non-ASCII characters before: {non_ascii_before:,}\")\n",
    "print(f\"  Non-ASCII characters after:  {non_ascii_after:,}\")\n",
    "print(f\"  Character count: {len(clean_text):,} → {len(ascii_text):,}\")\n",
    "\n",
    "if non_ascii_after > 0:\n",
    "    raise ValueError(f\"Conversion failed: {non_ascii_after} non-ASCII characters remain!\")\n",
    "\n",
    "print(f\"  ✓ Verified: 100% ASCII (all characters < 128)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Save Clean Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved clean corpus to ../data/gatsby_clean.txt\n",
      "  Size: 268,928 characters\n",
      "  Lines: 1,650\n",
      "  Encoding: ASCII (pure 7-bit)\n"
     ]
    }
   ],
   "source": [
    "# Write clean corpus (ASCII encoding)\n",
    "with open(CLEAN_PATH, 'w', encoding='ascii') as f:\n",
    "    f.write(ascii_text)\n",
    "\n",
    "print(f\"✓ Saved clean corpus to {CLEAN_PATH}\")\n",
    "print(f\"  Size: {len(ascii_text):,} characters\")\n",
    "print(f\"  Lines: {len(ascii_text.splitlines()):,}\")\n",
    "print(f\"  Encoding: ASCII (pure 7-bit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 paragraphs:\n",
      "================================================================================\n",
      "1: The Great Gatsby by F. Scott Fitzgerald\n",
      "2: Table of Contents\n",
      "3: I II III IV V VI VII VIII IX\n",
      "4: Once again to Zelda\n",
      "5: Then wear the gold hat, if that will move her; If you can bounce high, bounce for her too, Till she ...\n",
      "6: Thomas Parke d'Invilliers\n",
      "7: I\n",
      "8: In my younger and more vulnerable years my father gave me some advice that I've been turning over in...\n",
      "9: \"Whenever you feel like criticizing anyone,\" he told me, \"just remember that all the people in this ...\n",
      "10: He didn't say any more, but we've always been unusually communicative in a reserved way, and I under...\n",
      "================================================================================\n",
      "\n",
      "Last 5 paragraphs:\n",
      "================================================================================\n",
      "1646: On the last night, with my trunk packed and my car sold to the grocer, I went over and looked at tha...\n",
      "1647: Most of the big shore places were closed now and there were hardly any lights except the shadowy, mo...\n",
      "1648: And as I sat there brooding on the old, unknown world, I thought of Gatsby's wonder when he first pi...\n",
      "1649: Gatsby believed in the green light, the orgiastic future that year by year recedes before us. It elu...\n",
      "1650: So we beat on, boats against the current, borne back ceaselessly into the past.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Show first 10 paragraphs\n",
    "lines = ascii_text.split('\\n')\n",
    "print(\"\\nFirst 10 paragraphs:\")\n",
    "print(\"=\" * 80)\n",
    "for i, line in enumerate(lines[:10]):\n",
    "    print(f\"{i+1}: {line[:100]}{'...' if len(line) > 100 else ''}\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Show last 5 paragraphs  \n",
    "print(\"Last 5 paragraphs:\")\n",
    "print(\"=\" * 80)\n",
    "for i, line in enumerate(lines[-5:]):\n",
    "    print(f\"{len(lines)-5+i+1}: {line[:100]}{'...' if len(line) > 100 else ''}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CORPUS PREPARATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Processing:\n",
      "  ✓ Downloaded UTF-8 text from Project Gutenberg\n",
      "  ✓ Stripped Gutenberg front/back matter\n",
      "  ✓ Unwrapped hard-wrapped paragraphs\n",
      "  ✓ Converted to ASCII using Unidecode library\n",
      "  ✓ Verified 100% ASCII (all bytes < 128)\n",
      "\n",
      "Output:\n",
      "  File: ../data/gatsby_clean.txt\n",
      "  Size: 268,928 characters\n",
      "  Paragraphs: 1,650\n",
      "  Encoding: ASCII (7-bit)\n",
      "\n",
      "Character coverage (ASCII 0-127):\n",
      "  Present: 79 / 128 (61.7%)\n",
      "  Missing: 49 / 128 (38.3%)\n",
      "\n",
      "Implication for training:\n",
      "  79 tokens will be trained (appear in corpus)\n",
      "  49 tokens will be untrained (never seen)\n",
      "\n",
      "Next steps:\n",
      "  → Use this corpus for ASCII-based training experiments\n",
      "  → All characters guaranteed to be in range [0, 127]\n",
      "  → Expect 49 untrained tokens to form structure\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CORPUS PREPARATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"Processing:\")\n",
    "print(f\"  ✓ Downloaded UTF-8 text from Project Gutenberg\")\n",
    "print(f\"  ✓ Stripped Gutenberg front/back matter\")\n",
    "print(f\"  ✓ Unwrapped hard-wrapped paragraphs\")\n",
    "print(f\"  ✓ Converted to ASCII using Unidecode library\")\n",
    "print(f\"  ✓ Verified 100% ASCII (all bytes < 128)\")\n",
    "print()\n",
    "print(f\"Output:\")\n",
    "print(f\"  File: {CLEAN_PATH}\")\n",
    "print(f\"  Size: {len(ascii_text):,} characters\")\n",
    "print(f\"  Paragraphs: {len(unwrapped):,}\")\n",
    "print(f\"  Encoding: ASCII (7-bit)\")\n",
    "print()\n",
    "\n",
    "# Analyze character coverage\n",
    "ascii_bytes = ascii_text.encode('ascii')\n",
    "unique_bytes = set(ascii_bytes)\n",
    "present_tokens = sorted(unique_bytes)\n",
    "missing_tokens = sorted(set(range(128)) - unique_bytes)\n",
    "\n",
    "print(f\"Character coverage (ASCII 0-127):\")\n",
    "print(f\"  Present: {len(present_tokens)} / 128 ({100*len(present_tokens)/128:.1f}%)\")\n",
    "print(f\"  Missing: {len(missing_tokens)} / 128 ({100*len(missing_tokens)/128:.1f}%)\")\n",
    "print()\n",
    "\n",
    "if len(missing_tokens) <= 20:\n",
    "    # Show missing chars if not too many\n",
    "    missing_chars = []\n",
    "    for b in missing_tokens:\n",
    "        if 32 <= b < 127:  # Printable\n",
    "            missing_chars.append(f\"{chr(b)} ({b})\")\n",
    "        else:  # Control char\n",
    "            missing_chars.append(f\"0x{b:02x} ({b})\")\n",
    "    print(f\"  Missing characters: {', '.join(missing_chars)}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Implication for training:\")\n",
    "print(f\"  {len(present_tokens)} tokens will be trained (appear in corpus)\")\n",
    "print(f\"  {len(missing_tokens)} tokens will be untrained (never seen)\")\n",
    "print()\n",
    "\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  → Use this corpus for ASCII-based training experiments\")\n",
    "print(f\"  → All characters guaranteed to be in range [0, 127]\")\n",
    "print(f\"  → Expect {len(missing_tokens)} untrained tokens to form structure\")\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
