{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7b: Cluster Outlier Detection (L2)\n",
    "\n",
    "**Hypothesis:** The cluster contains distant outliers - tokens far from the core cluster.\n",
    "\n",
    "**Evidence:** Median distance from centroid is 0.00089653, but max is 1.16091752.\n",
    "\n",
    "**Prediction:** The largest pairwise L2 distance will identify two distant tokens.\n",
    "\n",
    "**Test:** Compute all pairwise L2 distances, find argmax, check norms and distance between the pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to analyze\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Detect available device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W from ../tensors/Qwen3-4B-Instruct-2507/W.safetensors\n",
      "  Shape: torch.Size([151936, 2560])\n",
      "\n",
      "Loaded cluster from ../tensors/Qwen3-4B-Instruct-2507/1.6a_cluster_mask.safetensors\n",
      "  Cluster size: 2,248 tokens\n",
      "  Centroid norm: 0.37091014\n"
     ]
    }
   ],
   "source": [
    "# Load W\n",
    "W_path = Path(f\"../tensors/{MODEL_NAME}/W.safetensors\")\n",
    "W = load_file(W_path)[\"W\"].to(torch.float32)\n",
    "\n",
    "print(f\"Loaded W from {W_path}\")\n",
    "print(f\"  Shape: {W.shape}\")\n",
    "\n",
    "# Load cluster data from 1.6a\n",
    "cluster_path = Path(f\"../tensors/{MODEL_NAME}/1.6a_cluster_mask.safetensors\")\n",
    "cluster_data = load_file(cluster_path)\n",
    "\n",
    "cluster_mask = cluster_data[\"cluster_mask\"].to(torch.bool)\n",
    "cluster_token_ids = cluster_data[\"cluster_token_ids\"].to(torch.int64)\n",
    "centroid = cluster_data[\"centroid\"].to(torch.float32)\n",
    "n_cluster = cluster_data[\"n_cluster\"].item()\n",
    "\n",
    "print(f\"\\nLoaded cluster from {cluster_path}\")\n",
    "print(f\"  Cluster size: {n_cluster:,} tokens\")\n",
    "print(f\"  Centroid norm: {centroid.norm().item():.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Cluster Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2,248 cluster embeddings\n",
      "  Dimensionality: 2,560\n"
     ]
    }
   ],
   "source": [
    "# Get cluster embeddings\n",
    "W_cluster = W[cluster_mask]\n",
    "\n",
    "print(f\"Extracted {W_cluster.shape[0]:,} cluster embeddings\")\n",
    "print(f\"  Dimensionality: {W_cluster.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Pairwise L2 Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing pairwise L2 distances (batched to avoid OOM)...\n",
      "\n",
      "  Processed rows    0- 256, batch max: 1.60828805\n",
      "  Processed rows  256- 512, batch max: 1.16175008\n",
      "  Processed rows  512- 768, batch max: 1.16141784\n",
      "  Processed rows  768-1024, batch max: 1.16141856\n",
      "  Processed rows 1024-1280, batch max: 1.19637430\n",
      "  Processed rows 1280-1536, batch max: 1.25368178\n",
      "  Processed rows 1536-1792, batch max: 1.16413260\n",
      "  Processed rows 1792-2048, batch max: 1.21384871\n",
      "  Processed rows 2048-2248, batch max: 1.16141760\n",
      "\n",
      "✓ Computed pairwise L2 distances in batches of 256\n",
      "  Global maximum: 1.60828805 at (14, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing pairwise L2 distances (batched to avoid OOM)...\\n\")\n",
    "\n",
    "# Move to device for computation\n",
    "W_cluster_device = W_cluster.to(device)\n",
    "\n",
    "# Batch size for computing distances (to avoid allocating 48GB)\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Track global maximum\n",
    "global_max_l2 = -1.0\n",
    "global_i = -1\n",
    "global_j = -1\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Process in batches of rows\n",
    "    for batch_start in range(0, n_cluster, BATCH_SIZE):\n",
    "        batch_end = min(batch_start + BATCH_SIZE, n_cluster)\n",
    "        batch_size = batch_end - batch_start\n",
    "        \n",
    "        # Get batch of tokens: shape (batch_size, d)\n",
    "        batch_tokens = W_cluster_device[batch_start:batch_end]\n",
    "        \n",
    "        # Compute differences with ALL tokens: (batch_size, 1, d) - (1, n_cluster, d)\n",
    "        # This creates (batch_size, n_cluster, d) instead of (n_cluster, n_cluster, d)\n",
    "        diffs = batch_tokens.unsqueeze(1) - W_cluster_device.unsqueeze(0)\n",
    "        \n",
    "        # L2 for this batch: sqrt(sum(diffs^2)) = (batch_size, n_cluster)\n",
    "        l2_batch = diffs.norm(dim=2)\n",
    "        \n",
    "        # Find max in this batch\n",
    "        batch_max = l2_batch.max().item()\n",
    "        \n",
    "        if batch_max > global_max_l2:\n",
    "            # Update global maximum\n",
    "            global_max_l2 = batch_max\n",
    "            \n",
    "            # Find indices within batch\n",
    "            flat_idx = l2_batch.argmax().item()\n",
    "            local_i = flat_idx // n_cluster\n",
    "            local_j = flat_idx % n_cluster\n",
    "            \n",
    "            # Convert to global indices\n",
    "            global_i = batch_start + local_i\n",
    "            global_j = local_j\n",
    "        \n",
    "        print(f\"  Processed rows {batch_start:4d}-{batch_end:4d}, batch max: {batch_max:.8f}\")\n",
    "\n",
    "print(f\"\\n✓ Computed pairwise L2 distances in batches of {BATCH_SIZE}\")\n",
    "print(f\"  Global maximum: {global_max_l2:.8f} at ({global_i}, {global_j})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Maximum L2 Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting result...\n",
      "\n",
      "Maximum L2 distance: 1.60828805\n",
      "  Between tokens: 48494 and 71473\n",
      "  (Cluster indices: 14 and 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExtracting result...\\n\")\n",
    "\n",
    "# The global_i and global_j are already computed from the batched loop\n",
    "max_l2 = global_max_l2\n",
    "i = global_i\n",
    "j = global_j\n",
    "\n",
    "# Get actual token IDs\n",
    "token_i = cluster_token_ids[i].item()\n",
    "token_j = cluster_token_ids[j].item()\n",
    "\n",
    "print(f\"Maximum L2 distance: {max_l2:.8f}\")\n",
    "print(f\"  Between tokens: {token_i} and {token_j}\")\n",
    "print(f\"  (Cluster indices: {i} and {j})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HYPOTHESIS TEST\n",
      "============================================================\n",
      "\n",
      "Token 48494:\n",
      "  Norm: 1.16991878\n",
      "  Distance from centroid: 1.10977745\n",
      "\n",
      "Token 71473:\n",
      "  Norm: 1.21912193\n",
      "  Distance from centroid: 1.16093612\n",
      "\n",
      "Distance between the two tokens:\n",
      "  L2(48494, 71473): 1.60828757\n",
      "\n",
      "Centroid:\n",
      "  Norm: 0.37091014\n",
      "\n",
      "============================================================\n",
      "\n",
      "RESULT:\n",
      "  Outlier: token 71473\n",
      "    Norm: 1.21912193 (3.29× centroid)\n",
      "    Distance from centroid: 1.16093612\n",
      "\n",
      "  Other outlier: token 48494\n",
      "    Norm: 1.16991878 (3.15× centroid)\n",
      "    Distance from centroid: 1.10977745\n",
      "\n",
      "Both tokens are far from centroid: True\n",
      "Distance between them vs their distance from centroid:\n",
      "  1.60828757 vs ~1.13535678\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPOTHESIS TEST\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Get the two tokens' embeddings\n",
    "t_i = W[token_i]\n",
    "t_j = W[token_j]\n",
    "\n",
    "# Compute norms\n",
    "norm_i = t_i.norm().item()\n",
    "norm_j = t_j.norm().item()\n",
    "norm_centroid = centroid.norm().item()\n",
    "\n",
    "# Compute distances from centroid\n",
    "dist_i = (t_i - centroid).norm().item()\n",
    "dist_j = (t_j - centroid).norm().item()\n",
    "\n",
    "# Compute L2 distance between the two tokens\n",
    "dist_ij = (t_i - t_j).norm().item()\n",
    "\n",
    "print(f\"Token {token_i}:\")\n",
    "print(f\"  Norm: {norm_i:.8f}\")\n",
    "print(f\"  Distance from centroid: {dist_i:.8f}\")\n",
    "print()\n",
    "print(f\"Token {token_j}:\")\n",
    "print(f\"  Norm: {norm_j:.8f}\")\n",
    "print(f\"  Distance from centroid: {dist_j:.8f}\")\n",
    "print()\n",
    "print(f\"Distance between the two tokens:\")\n",
    "print(f\"  L2({token_i}, {token_j}): {dist_ij:.8f}\")\n",
    "print()\n",
    "print(f\"Centroid:\")\n",
    "print(f\"  Norm: {norm_centroid:.8f}\")\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Identify the outlier\n",
    "if norm_i > norm_j:\n",
    "    outlier_id = token_i\n",
    "    outlier_norm = norm_i\n",
    "    outlier_dist = dist_i\n",
    "    core_id = token_j\n",
    "    core_norm = norm_j\n",
    "    core_dist = dist_j\n",
    "else:\n",
    "    outlier_id = token_j\n",
    "    outlier_norm = norm_j\n",
    "    outlier_dist = dist_j\n",
    "    core_id = token_i\n",
    "    core_norm = norm_i\n",
    "    core_dist = dist_i\n",
    "\n",
    "print(f\"RESULT:\")\n",
    "print(f\"  Outlier: token {outlier_id}\")\n",
    "print(f\"    Norm: {outlier_norm:.8f} ({outlier_norm/norm_centroid:.2f}× centroid)\")\n",
    "print(f\"    Distance from centroid: {outlier_dist:.8f}\")\n",
    "print()\n",
    "print(f\"  Other outlier: token {core_id}\")\n",
    "print(f\"    Norm: {core_norm:.8f} ({core_norm/norm_centroid:.2f}× centroid)\")\n",
    "print(f\"    Distance from centroid: {core_dist:.8f}\")\n",
    "print()\n",
    "print(f\"Both tokens are far from centroid: {(outlier_norm > 2 * norm_centroid) and (core_norm > 2 * norm_centroid)}\")\n",
    "print(f\"Distance between them vs their distance from centroid:\")\n",
    "print(f\"  {dist_ij:.8f} vs ~{(dist_i + dist_j)/2:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangle Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRIANGLE GEOMETRY\n",
      "============================================================\n",
      "\n",
      "Triangle sides:\n",
      "  Centroid to 48494: 1.10977745\n",
      "  Centroid to 71473: 1.16093612\n",
      "  48494 to 71473: 1.60828757\n",
      "\n",
      "Interior angles:\n",
      "  At centroid: 90.16°\n",
      "  At token 48494: 46.21°\n",
      "  At token 71473: 43.63°\n",
      "  Sum: 180.00° (should be 180°)\n",
      "\n",
      "Verification (angle at centroid via dot product): 90.16°\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRIANGLE GEOMETRY\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# We have a triangle: centroid, token_i, token_j\n",
    "# Sides:\n",
    "#   a = dist_i (centroid to token_i)\n",
    "#   b = dist_j (centroid to token_j)  \n",
    "#   c = dist_ij (token_i to token_j)\n",
    "\n",
    "a = dist_i\n",
    "b = dist_j\n",
    "c = dist_ij\n",
    "\n",
    "print(f\"Triangle sides:\")\n",
    "print(f\"  Centroid to {token_i}: {a:.8f}\")\n",
    "print(f\"  Centroid to {token_j}: {b:.8f}\")\n",
    "print(f\"  {token_i} to {token_j}: {c:.8f}\")\n",
    "print()\n",
    "\n",
    "# Compute interior angles using law of cosines\n",
    "# Angle at centroid (between the two tokens as seen from centroid)\n",
    "cos_at_centroid = (a**2 + b**2 - c**2) / (2 * a * b)\n",
    "angle_at_centroid = torch.acos(torch.tensor(cos_at_centroid))\n",
    "angle_at_centroid_deg = torch.rad2deg(angle_at_centroid).item()\n",
    "\n",
    "# Angle at token_i (between centroid and token_j as seen from token_i)\n",
    "cos_at_i = (a**2 + c**2 - b**2) / (2 * a * c)\n",
    "angle_at_i = torch.acos(torch.tensor(cos_at_i))\n",
    "angle_at_i_deg = torch.rad2deg(angle_at_i).item()\n",
    "\n",
    "# Angle at token_j (between centroid and token_i as seen from token_j)\n",
    "cos_at_j = (b**2 + c**2 - a**2) / (2 * b * c)\n",
    "angle_at_j = torch.acos(torch.tensor(cos_at_j))\n",
    "angle_at_j_deg = torch.rad2deg(angle_at_j).item()\n",
    "\n",
    "print(f\"Interior angles:\")\n",
    "print(f\"  At centroid: {angle_at_centroid_deg:.2f}°\")\n",
    "print(f\"  At token {token_i}: {angle_at_i_deg:.2f}°\")\n",
    "print(f\"  At token {token_j}: {angle_at_j_deg:.2f}°\")\n",
    "print(f\"  Sum: {angle_at_centroid_deg + angle_at_i_deg + angle_at_j_deg:.2f}° (should be 180°)\")\n",
    "print()\n",
    "\n",
    "# Alternative computation using dot products (sanity check)\n",
    "# Angle at centroid using dot product\n",
    "vec_to_i = t_i - centroid\n",
    "vec_to_j = t_j - centroid\n",
    "cos_dot = (vec_to_i @ vec_to_j) / (vec_to_i.norm() * vec_to_j.norm())\n",
    "angle_dot = torch.acos(cos_dot)\n",
    "angle_dot_deg = torch.rad2deg(angle_dot).item()\n",
    "\n",
    "print(f\"Verification (angle at centroid via dot product): {angle_dot_deg:.2f}°\")\n",
    "print()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster statistics:\n",
      "  Norms:\n",
      "    Min: 0.37029281\n",
      "    Max: 1.21912193\n",
      "    Median: 0.37091675\n",
      "    Mean: 0.37223831\n",
      "\n",
      "  Distances from centroid:\n",
      "    Min: 0.00125120\n",
      "    Max: 1.16093612\n",
      "    Median: 0.00125222\n",
      "    Mean: 0.00374321\n"
     ]
    }
   ],
   "source": [
    "# Compute all norms and distances for context\n",
    "all_norms = W_cluster.norm(dim=1)\n",
    "all_distances = (W_cluster - centroid).norm(dim=1)\n",
    "\n",
    "print(\"\\nCluster statistics:\")\n",
    "print(f\"  Norms:\")\n",
    "print(f\"    Min: {all_norms.min().item():.8f}\")\n",
    "print(f\"    Max: {all_norms.max().item():.8f}\")\n",
    "print(f\"    Median: {all_norms.median().item():.8f}\")\n",
    "print(f\"    Mean: {all_norms.mean().item():.8f}\")\n",
    "print()\n",
    "print(f\"  Distances from centroid:\")\n",
    "print(f\"    Min: {all_distances.min().item():.8f}\")\n",
    "print(f\"    Max: {all_distances.max().item():.8f}\")\n",
    "print(f\"    Median: {all_distances.median().item():.8f}\")\n",
    "print(f\"    Mean: {all_distances.mean().item():.8f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
