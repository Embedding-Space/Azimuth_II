{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thimble 6: Ground State or Still Cooling? (t=6000, HDF5)\n",
    "\n",
    "**Purpose:** Distinguish quantum ground state equilibrium from continued cooling.\n",
    "\n",
    "## Context\n",
    "\n",
    "**Thimble 5** (t=0–3000) revealed:\n",
    "- **Individual tokens:** 99.9% frozen in late training (t=2400-3000)\n",
    "- **Global freeze:** Only 22.5% of timesteps (674/3000)\n",
    "- **Late training:** 64% globally frozen (t=2000-3000)\n",
    "- **55 freeze/thaw cycles** with longest run = 82 steps (ending at t=2999)\n",
    "\n",
    "**Two competing hypotheses:**\n",
    "\n",
    "1. **Ground state equilibrium:** System has reached minimum energy. The 64% global freeze / 36% jitter is the permanent quantum ground state. Statistics stabilize—no further cooling.\n",
    "\n",
    "2. **Still cooling:** System asymptotically approaching 100% global freeze. The trend from 22.5% (overall) → 64% (t=2000-3000) continues. Eventually reaches permanent Fimbulwinter.\n",
    "\n",
    "## Question\n",
    "\n",
    "By doubling observation window to t=6000, which hypothesis is correct?\n",
    "\n",
    "## Technical Changes\n",
    "\n",
    "**HDF5 streaming format:**\n",
    "- No memory accumulation (was 33 GB for safetensors)\n",
    "- Write incrementally to disk (~1 GB RAM during training)\n",
    "- Analysis notebooks load only needed slices\n",
    "- gzip compression (level 1, fast)\n",
    "- Chunked storage optimized for timestep access\n",
    "\n",
    "**Memory:** ~1 GB during training (vs. ~33 GB for safetensors)\n",
    "\n",
    "**Time:** ~7-8 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parameters set\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYERS = 2\n",
    "N_HEADS = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# Training\n",
    "NUM_STEPS = 6000  # ← DOUBLE OBSERVATION WINDOW\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# Optimizer (AdamW)\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "# Initialization\n",
    "INIT_SCALE = 0.02\n",
    "SEED = 42\n",
    "\n",
    "# Paths\n",
    "TOKENIZER_PATH = \"../data/flannel_tokenizer_chars.json\"\n",
    "CORPUS_PATH = \"../data/flannel_model_corpus.txt\"\n",
    "TOKEN_MASK_PATH = \"../tensors/Flannel/live_dead_tokens.safetensors\"\n",
    "OUTPUT_PATH = \"../tensors/Thimble/thimble_6.h5\"  # ← HDF5 FORMAT\n",
    "\n",
    "print(\"✓ Parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from tokenizers import Tokenizer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Safety Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MEMORY & DISK SAFETY CHECK (HDF5 STREAMING)\n",
      "================================================================================\n",
      "\n",
      "Disk space (uncompressed):\n",
      "  W:         7.68 GB\n",
      "  grad_W:    7.68 GB\n",
      "  momentum:  7.68 GB\n",
      "  variance:  7.68 GB\n",
      "  losses:    0.0000 GB\n",
      "  metadata:  0.0010 GB\n",
      "  ────────────────────────────────────────\n",
      "  Total:     30.73 GB\n",
      "  With gzip: ~21.51 GB (estimated)\n",
      "\n",
      "RAM during training (streaming):\n",
      "  Model+opt+act: 0.01 GB\n",
      "  Corpus:        0.01 GB\n",
      "  HDF5 buffer:   0.10 GB\n",
      "  Misc overhead: 0.50 GB\n",
      "  ────────────────────────────────────────\n",
      "  Total:         0.62 GB\n",
      "\n",
      "================================================================================\n",
      "✓ SAFE: Peak RAM (0.6 GB) within 24 GB budget\n",
      "  HDF5 streaming avoids 30.7 GB accumulation!\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MEMORY & DISK SAFETY CHECK (HDF5 STREAMING)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# HDF5 streaming: NO RAM accumulation\n",
    "bytes_bf16 = 2\n",
    "bytes_f32 = 4\n",
    "\n",
    "# Disk space (uncompressed estimate)\n",
    "disk_w = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16\n",
    "disk_grad = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16\n",
    "disk_momentum = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16\n",
    "disk_variance = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16\n",
    "disk_losses = (NUM_STEPS+1) * bytes_f32\n",
    "disk_metadata = 1e6  # Token masks, hyperparams\n",
    "\n",
    "total_disk_uncompressed = disk_w + disk_grad + disk_momentum + disk_variance + disk_losses + disk_metadata\n",
    "total_disk_compressed = total_disk_uncompressed * 0.7  # Estimate 30% compression\n",
    "\n",
    "print(f\"Disk space (uncompressed):\")\n",
    "print(f\"  W:         {disk_w/1e9:.2f} GB\")\n",
    "print(f\"  grad_W:    {disk_grad/1e9:.2f} GB\")\n",
    "print(f\"  momentum:  {disk_momentum/1e9:.2f} GB\")\n",
    "print(f\"  variance:  {disk_variance/1e9:.2f} GB\")\n",
    "print(f\"  losses:    {disk_losses/1e9:.4f} GB\")\n",
    "print(f\"  metadata:  {disk_metadata/1e9:.4f} GB\")\n",
    "print(f\"  {'─'*40}\")\n",
    "print(f\"  Total:     {total_disk_uncompressed/1e9:.2f} GB\")\n",
    "print(f\"  With gzip: ~{total_disk_compressed/1e9:.2f} GB (estimated)\")\n",
    "print()\n",
    "\n",
    "# RAM during training (streaming writes)\n",
    "model_params = VOCAB_SIZE * HIDDEN_DIM + N_LAYERS * (12 * HIDDEN_DIM**2)\n",
    "model_memory = model_params * bytes_bf16\n",
    "optimizer_memory = 2 * model_params * bytes_bf16\n",
    "activation_memory = BATCH_SIZE * MAX_SEQ_LEN * HIDDEN_DIM * N_LAYERS * 2 * bytes_bf16\n",
    "corpus_memory = 1371328 * 8\n",
    "hdf5_buffer = 100e6  # HDF5 write buffer\n",
    "misc_overhead = 500e6\n",
    "\n",
    "peak_ram = model_memory + optimizer_memory + activation_memory + corpus_memory + hdf5_buffer + misc_overhead\n",
    "\n",
    "print(f\"RAM during training (streaming):\")\n",
    "print(f\"  Model+opt+act: {(model_memory + optimizer_memory + activation_memory)/1e9:.2f} GB\")\n",
    "print(f\"  Corpus:        {corpus_memory/1e9:.2f} GB\")\n",
    "print(f\"  HDF5 buffer:   {hdf5_buffer/1e9:.2f} GB\")\n",
    "print(f\"  Misc overhead: {misc_overhead/1e9:.2f} GB\")\n",
    "print(f\"  {'─'*40}\")\n",
    "print(f\"  Total:         {peak_ram/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "if peak_ram <= 24e9:\n",
    "    print(f\"✓ SAFE: Peak RAM ({peak_ram/1e9:.1f} GB) within 24 GB budget\")\n",
    "    print(f\"  HDF5 streaming avoids {total_disk_uncompressed/1e9:.1f} GB accumulation!\")\n",
    "else:\n",
    "    print(f\"⚠️  WARNING: Peak RAM ({peak_ram/1e9:.1f} GB) exceeds 24 GB budget!\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"✓ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: ../data/flannel_tokenizer_chars.json\n",
      "  ✓ Vocabulary: 10,000 tokens\n",
      "\n",
      "Loading corpus: ../data/flannel_model_corpus.txt\n",
      "  ✓ Tokens: 1,371,328\n",
      "\n",
      "Loading token masks: ../tensors/Flannel/live_dead_tokens.safetensors\n",
      "  ✓ Live: 6,301 | Dead: 3,699\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "print(f\"Loading tokenizer: {TOKENIZER_PATH}\")\n",
    "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "print(f\"  ✓ Vocabulary: {tokenizer.get_vocab_size():,} tokens\\n\")\n",
    "\n",
    "# Corpus\n",
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "encoding = tokenizer.encode(corpus_text)\n",
    "tokens = encoding.ids\n",
    "corpus_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "print(f\"  ✓ Tokens: {len(tokens):,}\\n\")\n",
    "\n",
    "# Token masks\n",
    "print(f\"Loading token masks: {TOKEN_MASK_PATH}\")\n",
    "mask_data = load_file(TOKEN_MASK_PATH)\n",
    "live_mask = mask_data['live_mask'].bool()\n",
    "dead_mask = mask_data['dead_mask'].bool()\n",
    "live_ids = mask_data['live_indices'].long()\n",
    "dead_ids = mask_data['dead_indices'].long()\n",
    "n_live = live_mask.sum().item()\n",
    "n_dead = dead_mask.sum().item()\n",
    "print(f\"  ✓ Live: {n_live:,} | Dead: {n_dead:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset: 1,371,200 examples\n",
      "✓ DataLoader: 10,713 batches per epoch\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = TokenDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    generator=g,\n",
    "    worker_init_fn=seed_worker,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dataset: {len(dataset):,} examples\")\n",
    "print(f\"✓ DataLoader: {len(dataloader):,} batches per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model (BFLOAT16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "\n",
      "  Architecture: 2 layers, 2 heads, 64d embeddings\n",
      "  Parameters: 748,288\n",
      "  Device: mps\n",
      "  Dtype: torch.bfloat16 (BFLOAT16)\n",
      "\n",
      "✓ Model created\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating model...\\n\")\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYERS,\n",
    "    n_head=N_HEADS,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Initialize embedding weights with N(0, 0.02)\n",
    "with torch.no_grad():\n",
    "    nn.init.normal_(model.transformer.wte.weight, mean=0.0, std=INIT_SCALE)\n",
    "\n",
    "# Convert to bfloat16 and move to device\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"  Architecture: {N_LAYERS} layers, {N_HEADS} heads, {HIDDEN_DIM}d embeddings\")\n",
    "print(f\"  Parameters: {n_params:,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Dtype: {model.transformer.wte.weight.dtype} (BFLOAT16)\")\n",
    "print(f\"\\n✓ Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Optimizer: AdamW\n",
      "  Learning rate: 0.001\n",
      "  Betas: (0.9, 0.999)\n",
      "  Epsilon: 1e-08\n",
      "  Weight decay: 0.0\n",
      "\n",
      "  Optimizer states will be BFLOAT16 (matching param dtype)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(ADAM_BETA1, ADAM_BETA2),\n",
    "    eps=ADAM_EPSILON,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "print(f\"✓ Optimizer: AdamW\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Betas: ({ADAM_BETA1}, {ADAM_BETA2})\")\n",
    "print(f\"  Epsilon: {ADAM_EPSILON}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"\\n  Optimizer states will be BFLOAT16 (matching param dtype)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HDF5 File with Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating HDF5 file with streaming datasets...\n",
      "\n",
      "  Created datasets with shape: (6001, 10000, 64)\n",
      "  Chunking: (1, 10000, 64) (one timestep per chunk)\n",
      "  Compression: gzip level 1\n",
      "  Dtype: float16 (will represent bfloat16 data)\n",
      "\n",
      "✓ HDF5 file initialized (streaming writes)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreating HDF5 file with streaming datasets...\\n\")\n",
    "\n",
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Open HDF5 file for writing\n",
    "h5file = h5py.File(OUTPUT_PATH, 'w')\n",
    "\n",
    "# Create datasets with chunking and compression\n",
    "# Chunk size: (1, vocab, hidden) = one timestep at a time\n",
    "chunk_shape = (1, VOCAB_SIZE, HIDDEN_DIM)\n",
    "\n",
    "# Use float16 (HDF5 doesn't have native bfloat16, but we'll convert)\n",
    "# We store as float16, then cast to bfloat16 when loading\n",
    "W_dset = h5file.create_dataset(\n",
    "    'W', \n",
    "    shape=(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM),\n",
    "    dtype='float16',\n",
    "    chunks=chunk_shape,\n",
    "    compression='gzip',\n",
    "    compression_opts=1\n",
    ")\n",
    "\n",
    "grad_dset = h5file.create_dataset(\n",
    "    'grad_W',\n",
    "    shape=(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM),\n",
    "    dtype='float16',\n",
    "    chunks=chunk_shape,\n",
    "    compression='gzip',\n",
    "    compression_opts=1\n",
    ")\n",
    "\n",
    "momentum_dset = h5file.create_dataset(\n",
    "    'momentum_W',\n",
    "    shape=(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM),\n",
    "    dtype='float16',\n",
    "    chunks=chunk_shape,\n",
    "    compression='gzip',\n",
    "    compression_opts=1\n",
    ")\n",
    "\n",
    "variance_dset = h5file.create_dataset(\n",
    "    'variance_W',\n",
    "    shape=(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM),\n",
    "    dtype='float16',\n",
    "    chunks=chunk_shape,\n",
    "    compression='gzip',\n",
    "    compression_opts=1\n",
    ")\n",
    "\n",
    "loss_dset = h5file.create_dataset(\n",
    "    'losses',\n",
    "    shape=(NUM_STEPS+1,),\n",
    "    dtype='float32',\n",
    "    chunks=(1000,),\n",
    "    compression='gzip',\n",
    "    compression_opts=1\n",
    ")\n",
    "\n",
    "# Store metadata (token masks, hyperparameters)\n",
    "h5file.create_dataset('live_mask', data=live_mask.numpy())\n",
    "h5file.create_dataset('dead_mask', data=dead_mask.numpy())\n",
    "h5file.create_dataset('live_ids', data=live_ids.numpy())\n",
    "h5file.create_dataset('dead_ids', data=dead_ids.numpy())\n",
    "\n",
    "# Store hyperparameters as attributes\n",
    "h5file.attrs['vocab_size'] = VOCAB_SIZE\n",
    "h5file.attrs['hidden_dim'] = HIDDEN_DIM\n",
    "h5file.attrs['n_layers'] = N_LAYERS\n",
    "h5file.attrs['n_heads'] = N_HEADS\n",
    "h5file.attrs['num_steps'] = NUM_STEPS\n",
    "h5file.attrs['batch_size'] = BATCH_SIZE\n",
    "h5file.attrs['learning_rate'] = LEARNING_RATE\n",
    "h5file.attrs['weight_decay'] = WEIGHT_DECAY\n",
    "h5file.attrs['adam_beta1'] = ADAM_BETA1\n",
    "h5file.attrs['adam_beta2'] = ADAM_BETA2\n",
    "h5file.attrs['adam_epsilon'] = ADAM_EPSILON\n",
    "h5file.attrs['init_scale'] = INIT_SCALE\n",
    "h5file.attrs['seed'] = SEED\n",
    "h5file.attrs['n_live'] = n_live\n",
    "h5file.attrs['n_dead'] = n_dead\n",
    "\n",
    "print(f\"  Created datasets with shape: ({NUM_STEPS+1}, {VOCAB_SIZE}, {HIDDEN_DIM})\")\n",
    "print(f\"  Chunking: {chunk_shape} (one timestep per chunk)\")\n",
    "print(f\"  Compression: gzip level 1\")\n",
    "print(f\"  Dtype: float16 (will represent bfloat16 data)\")\n",
    "print(f\"\\n✓ HDF5 file initialized (streaming writes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with HDF5 Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "THIMBLE 6: GROUND STATE OR STILL COOLING? (t=0 → t=6000)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to synchronously get dataspace (identifier is not of specified type)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Record initial state (step 0)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mW_dset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m = model.transformer.wte.weight.data.cpu().float().numpy()  \u001b[38;5;66;03m# bfloat16 → float16 for storage\u001b[39;00m\n\u001b[32m      7\u001b[39m grad_dset[\u001b[32m0\u001b[39m] = np.zeros((VOCAB_SIZE, HIDDEN_DIM), dtype=np.float16)\n\u001b[32m      8\u001b[39m momentum_dset[\u001b[32m0\u001b[39m] = np.zeros((VOCAB_SIZE, HIDDEN_DIM), dtype=np.float16)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Azimuth_II/.venv/lib/python3.12/site-packages/h5py/_hl/dataset.py:1047\u001b[39m, in \u001b[36mDataset.__setitem__\u001b[39m\u001b[34m(self, args, val)\u001b[39m\n\u001b[32m   1044\u001b[39m     mtype = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m \u001b[38;5;66;03m# Perform the dataspace selection\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m selection = sel.select(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m, args, dataset=\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m selection.nselect == \u001b[32m0\u001b[39m:\n\u001b[32m   1050\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Azimuth_II/.venv/lib/python3.12/site-packages/h5py/_hl/dataset.py:539\u001b[39m, in \u001b[36mDataset.shape\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cache_props[\u001b[33m'\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m phil:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     shape = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;66;03m# If the file is read-only, cache the shape to speed-up future uses.\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[38;5;66;03m# This cache is invalidated by .refresh() when using SWMR.\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._readonly:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5d.pyx:205\u001b[39m, in \u001b[36mh5py.h5d.DatasetID.shape.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5d.pyx:206\u001b[39m, in \u001b[36mh5py.h5d.DatasetID.shape.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5d.pyx:372\u001b[39m, in \u001b[36mh5py.h5d.DatasetID.get_space\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: Unable to synchronously get dataspace (identifier is not of specified type)"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"THIMBLE 6: GROUND STATE OR STILL COOLING? (t=0 → t=6000)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Record initial state (step 0)\n",
    "W_dset[0] = model.transformer.wte.weight.data.cpu().float().numpy()  # bfloat16 → float16 for storage\n",
    "grad_dset[0] = np.zeros((VOCAB_SIZE, HIDDEN_DIM), dtype=np.float16)\n",
    "momentum_dset[0] = np.zeros((VOCAB_SIZE, HIDDEN_DIM), dtype=np.float16)\n",
    "variance_dset[0] = np.zeros((VOCAB_SIZE, HIDDEN_DIM), dtype=np.float16)\n",
    "loss_dset[0] = np.nan\n",
    "print(\"✓ Recorded initial state (t=0)\\n\")\n",
    "\n",
    "# Create infinite iterator over dataloader\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "\n",
    "for step in tqdm(range(1, NUM_STEPS+1), desc=\"Training\"):\n",
    "    # Get next batch\n",
    "    try:\n",
    "        batch = next(data_iter)\n",
    "    except StopIteration:\n",
    "        data_iter = iter(dataloader)\n",
    "        batch = next(data_iter)\n",
    "    \n",
    "    # Move batch to device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # === STREAM GRADIENTS TO HDF5 ===\n",
    "    grad_dset[step] = model.transformer.wte.weight.grad.cpu().float().numpy()\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # === STREAM WEIGHTS & OPTIMIZER STATE TO HDF5 ===\n",
    "    W_dset[step] = model.transformer.wte.weight.data.cpu().float().numpy()\n",
    "    \n",
    "    wte_param = model.transformer.wte.weight\n",
    "    if wte_param in optimizer.state:\n",
    "        opt_state = optimizer.state[wte_param]\n",
    "        momentum_dset[step] = opt_state['exp_avg'].cpu().float().numpy()\n",
    "        variance_dset[step] = opt_state['exp_avg_sq'].cpu().float().numpy()\n",
    "    else:\n",
    "        momentum_dset[step] = np.zeros((VOCAB_SIZE, HIDDEN_DIM), dtype=np.float16)\n",
    "        variance_dset[step] = np.zeros((VOCAB_SIZE, HIDDEN_DIM), dtype=np.float16)\n",
    "    \n",
    "    loss_dset[step] = loss.item()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Close HDF5 file\n",
    "h5file.close()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training complete\")\n",
    "print(f\"  Time: {elapsed:.1f}s ({elapsed/60:.1f} minutes)\")\n",
    "print(f\"  Final loss: {loss_dset[-1]:.4f}\")\n",
    "print(f\"✓ HDF5 file closed\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying output file...\n",
      "\n",
      "✓ File created successfully\n",
      "  Path: ../tensors/Thimble/thimble_6.h5\n",
      "  Size: 16.24 GB\n",
      "\n",
      "  Datasets:\n",
      "    W: (6001, 10000, 64) (float16)\n",
      "    dead_ids: (3699,)\n",
      "    dead_mask: (10000,)\n",
      "    grad_W: (6001, 10000, 64) (float16)\n",
      "    live_ids: (6301,)\n",
      "    live_mask: (10000,)\n",
      "    losses: (6001,) (float32)\n",
      "    momentum_W: (6001, 10000, 64) (float16)\n",
      "    variance_W: (6001, 10000, 64) (float16)\n",
      "\n",
      "  Attributes:\n",
      "    adam_beta1: 0.9\n",
      "    adam_beta2: 0.999\n",
      "    adam_epsilon: 1e-08\n",
      "    batch_size: 128\n",
      "    hidden_dim: 64\n",
      "    init_scale: 0.02\n",
      "    learning_rate: 0.001\n",
      "    n_dead: 3699\n",
      "    n_heads: 2\n",
      "    n_layers: 2\n",
      "    n_live: 6301\n",
      "    num_steps: 6000\n",
      "    seed: 42\n",
      "    vocab_size: 10000\n",
      "    weight_decay: 0.0\n",
      "\n",
      "  ✓ Test load successful: W[3000] shape=(10000, 64), dtype=torch.bfloat16\n",
      "\n",
      "✓ Output verification complete\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nVerifying output file...\\n\")\n",
    "\n",
    "file_size_bytes = Path(OUTPUT_PATH).stat().st_size\n",
    "file_size_gb = file_size_bytes / 1e9\n",
    "\n",
    "print(f\"✓ File created successfully\")\n",
    "print(f\"  Path: {OUTPUT_PATH}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Quick check: load a single timestep\n",
    "with h5py.File(OUTPUT_PATH, 'r') as f:\n",
    "    print(f\"  Datasets:\")\n",
    "    for key in f.keys():\n",
    "        if key in ['W', 'grad_W', 'momentum_W', 'variance_W']:\n",
    "            print(f\"    {key}: {f[key].shape} ({f[key].dtype})\")\n",
    "        elif key == 'losses':\n",
    "            print(f\"    {key}: {f[key].shape} ({f[key].dtype})\")\n",
    "        else:\n",
    "            print(f\"    {key}: {f[key].shape}\")\n",
    "    \n",
    "    print(f\"\\n  Attributes:\")\n",
    "    for key in f.attrs.keys():\n",
    "        print(f\"    {key}: {f.attrs[key]}\")\n",
    "    \n",
    "    # Test loading a single timestep\n",
    "    W_test = torch.from_numpy(f['W'][3000]).to(torch.bfloat16)\n",
    "    print(f\"\\n  ✓ Test load successful: W[3000] shape={tuple(W_test.shape)}, dtype={W_test.dtype}\")\n",
    "\n",
    "print(f\"\\n✓ Output verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "THIMBLE 6 COMPLETE: GROUND STATE OR STILL COOLING?\n",
      "================================================================================\n",
      "\n",
      "Trained for 6,000 steps with pure bfloat16 pipeline\n",
      "  Seed: 42\n",
      "  Batch size: 128\n",
      "  Learning rate: 0.001\n",
      "  Weight decay: 0.0\n",
      "\n",
      "Recorded at every step (HDF5 streaming):\n",
      "  • W: embedding weights\n",
      "  • grad_W: gradients\n",
      "  • momentum_W: Adam exp_avg\n",
      "  • variance_W: Adam exp_avg_sq\n",
      "  • losses: training loss\n",
      "  • Token masks: live/dead masks and IDs (self-contained)\n",
      "\n",
      "Data saved: ../tensors/Thimble/thimble_6.h5\n",
      "  Size: 16.24 GB (compressed)\n",
      "  Format: HDF5 with gzip compression\n",
      "  Training time: 14.0 minutes\n",
      "\n",
      "Next: Analyze to distinguish:\n",
      "  1. Ground state equilibrium (statistics stabilize)\n",
      "  2. Still cooling (global freeze % continues rising)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"THIMBLE 6 COMPLETE: GROUND STATE OR STILL COOLING?\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Trained for {NUM_STEPS:,} steps with pure bfloat16 pipeline\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print()\n",
    "print(f\"Recorded at every step (HDF5 streaming):\")\n",
    "print(f\"  • W: embedding weights\")\n",
    "print(f\"  • grad_W: gradients\")\n",
    "print(f\"  • momentum_W: Adam exp_avg\")\n",
    "print(f\"  • variance_W: Adam exp_avg_sq\")\n",
    "print(f\"  • losses: training loss\")\n",
    "print(f\"  • Token masks: live/dead masks and IDs (self-contained)\")\n",
    "print()\n",
    "print(f\"Data saved: {OUTPUT_PATH}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB (compressed)\")\n",
    "print(f\"  Format: HDF5 with gzip compression\")\n",
    "print(f\"  Training time: {elapsed/60:.1f} minutes\")\n",
    "print()\n",
    "print(f\"Next: Analyze to distinguish:\")\n",
    "print(f\"  1. Ground state equilibrium (statistics stabilize)\")\n",
    "print(f\"  2. Still cooling (global freeze % continues rising)\")\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
