{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.25b: Thimble 1 Accounting Check\n",
    "\n",
    "Simple validation: does the AdamW formula predict observed weight changes?\n",
    "\n",
    "## What we're testing\n",
    "\n",
    "- Load W[t] for t=0..1000 from Thimble 1\n",
    "- Compute observed ΔW[t] = W[t+1] - W[t]\n",
    "- Load recorded gradients, momentum, variance\n",
    "- Compute predicted ΔW[t] from AdamW formula\n",
    "- Compare them\n",
    "\n",
    "## Output\n",
    "\n",
    "Table showing for each timestep:\n",
    "- t\n",
    "- ||predicted ΔW|| (L2 norm)\n",
    "- ||observed ΔW|| (L2 norm)\n",
    "- ||difference|| (L2 norm)\n",
    "- cosine similarity\n",
    "- ratio (predicted/observed)\n",
    "\n",
    "Focus on dead tokens only to keep it simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W: torch.Size([1001, 10000, 64])\n",
      "Loaded grad_W: torch.Size([1001, 10000, 64])\n",
      "Loaded momentum_W: torch.Size([1001, 10000, 64])\n",
      "Loaded variance_W: torch.Size([1001, 10000, 64])\n",
      "Loaded losses: torch.Size([1001])\n",
      "\n",
      "Hyperparameters:\n",
      "  learning_rate: 0.0010000000474974513\n",
      "  weight_decay: 0.0\n",
      "  beta1: 0.8999999761581421\n",
      "  beta2: 0.9990000128746033\n",
      "  epsilon: 9.99999993922529e-09\n",
      "  num_steps: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load Thimble 1 data\n",
    "thimble_path = Path(\"../tensors/Thimble/thimble_1.safetensors\")\n",
    "data = load_file(str(thimble_path))\n",
    "\n",
    "# Extract trajectory tensors\n",
    "W = data['W']  # Shape: (1001, 10000, 64)\n",
    "grad_W = data['grad_W']  # Shape: (1001, 10000, 64)\n",
    "momentum_W = data['momentum_W']  # Shape: (1001, 10000, 64)\n",
    "variance_W = data['variance_W']  # Shape: (1001, 10000, 64)\n",
    "losses = data['losses']  # Shape: (1001,)\n",
    "\n",
    "# Extract hyperparameters\n",
    "LEARNING_RATE = data['learning_rate'].item()\n",
    "WEIGHT_DECAY = data['weight_decay'].item()\n",
    "BETA1 = data['adam_beta1'].item()\n",
    "BETA2 = data['adam_beta2'].item()\n",
    "EPSILON = data['adam_epsilon'].item()\n",
    "NUM_STEPS = data['num_steps'].item()\n",
    "\n",
    "print(f\"Loaded W: {W.shape}\")\n",
    "print(f\"Loaded grad_W: {grad_W.shape}\")\n",
    "print(f\"Loaded momentum_W: {momentum_W.shape}\")\n",
    "print(f\"Loaded variance_W: {variance_W.shape}\")\n",
    "print(f\"Loaded losses: {losses.shape}\")\n",
    "print()\n",
    "print(f\"Hyperparameters:\")\n",
    "print(f\"  learning_rate: {LEARNING_RATE}\")\n",
    "print(f\"  weight_decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  beta1: {BETA1}\")\n",
    "print(f\"  beta2: {BETA2}\")\n",
    "print(f\"  epsilon: {EPSILON}\")\n",
    "print(f\"  num_steps: {NUM_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dead Token Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dead tokens: 3699/10000\n"
     ]
    }
   ],
   "source": [
    "# Load dead token mask from Flannel directory\n",
    "mask_path = Path(\"../tensors/Flannel/live_dead_tokens.safetensors\")\n",
    "mask_data = load_file(str(mask_path))\n",
    "dead_mask = mask_data['dead_mask'].bool()\n",
    "\n",
    "print(f\"Dead tokens: {dead_mask.sum().item()}/{len(dead_mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Observed ΔW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed ΔW (dead tokens): torch.Size([1000, 3699, 64])\n"
     ]
    }
   ],
   "source": [
    "# Observed weight changes: W[t+1] - W[t]\n",
    "# W has shape (1001, 10000, 64), so observed_dW has shape (1000, 10000, 64)\n",
    "observed_dW = W[1:] - W[:-1]\n",
    "\n",
    "# Extract dead tokens only\n",
    "observed_dW_dead = observed_dW[:, dead_mask, :]  # Shape: (1000, 3699, 64)\n",
    "\n",
    "print(f\"Observed ΔW (dead tokens): {observed_dW_dead.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Predicted ΔW from AdamW\n",
    "\n",
    "AdamW update rule:\n",
    "\n",
    "$$\\Delta W[t] = -\\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} - \\lambda \\cdot W[t-1]$$\n",
    "\n",
    "where:\n",
    "- $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$ (bias-corrected momentum)\n",
    "- $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$ (bias-corrected variance)\n",
    "- $\\lambda$ is weight_decay (0.0 in Thimble 1, so second term vanishes)\n",
    "- $m_t$ and $v_t$ are the recorded momentum and variance states\n",
    "\n",
    "**Important:** The recorded states are AFTER the optimizer step at time t, so:\n",
    "- `momentum_W[t]` contains $m_t$ (state after step t)\n",
    "- `variance_W[t]` contains $v_t$ (state after step t)\n",
    "- These are the states used to compute the update that produces W[t+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted ΔW (dead tokens): torch.Size([1000, 3699, 64])\n"
     ]
    }
   ],
   "source": [
    "# Extract dead tokens from optimizer states\n",
    "# We need states at t=1..1000 to predict ΔW for steps 1..1000\n",
    "momentum_dead = momentum_W[1:, dead_mask, :]  # Shape: (1000, 3699, 64)\n",
    "variance_dead = variance_W[1:, dead_mask, :]  # Shape: (1000, 3699, 64)\n",
    "W_prev_dead = W[:-1, dead_mask, :]  # W[t-1] for weight decay term\n",
    "\n",
    "# Compute bias correction terms for each timestep\n",
    "# t goes from 1 to 1000 (AdamW uses 1-indexed steps)\n",
    "timesteps = torch.arange(1, NUM_STEPS + 1, dtype=torch.float32)\n",
    "bias_correction1 = 1 - BETA1 ** timesteps  # Shape: (1000,)\n",
    "bias_correction2 = 1 - BETA2 ** timesteps  # Shape: (1000,)\n",
    "\n",
    "# Reshape for broadcasting: (1000, 1, 1)\n",
    "bias_correction1 = bias_correction1.view(-1, 1, 1)\n",
    "bias_correction2 = bias_correction2.view(-1, 1, 1)\n",
    "\n",
    "# Apply bias correction\n",
    "m_hat = momentum_dead / bias_correction1\n",
    "v_hat = variance_dead / bias_correction2\n",
    "\n",
    "# Compute AdamW update\n",
    "adam_term = LEARNING_RATE * m_hat / (torch.sqrt(v_hat) + EPSILON)\n",
    "decay_term = WEIGHT_DECAY * W_prev_dead.float()\n",
    "\n",
    "predicted_dW_dead = -adam_term - decay_term\n",
    "\n",
    "print(f\"Predicted ΔW (dead tokens): {predicted_dW_dead.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Predicted vs Observed\n",
    "\n",
    "For each timestep, compute:\n",
    "1. L2 norm of predicted ΔW\n",
    "2. L2 norm of observed ΔW\n",
    "3. L2 norm of difference\n",
    "4. Cosine similarity\n",
    "5. Ratio (||predicted|| / ||observed||)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== First 20 timesteps ===\n",
      " t  norm_predicted  norm_observed  norm_difference  cosine_similarity    ratio\n",
      " 1        0.485381       0.479647         0.010112           0.999925 1.011954\n",
      " 2        0.383173       0.383000         0.016097           0.999145 1.000450\n",
      " 3        0.406970       0.408090         0.017424           0.999109 0.997255\n",
      " 4        0.426508       0.430362         0.017317           0.999233 0.991043\n",
      " 5        0.441335       0.445059         0.014930           0.999488 0.991632\n",
      " 6        0.452530       0.455810         0.012278           0.999688 0.992806\n",
      " 7        0.461319       0.462185         0.010693           0.999765 0.998125\n",
      " 8        0.468135       0.467225         0.010490           0.999780 1.001948\n",
      " 9        0.473129       0.471550         0.010939           0.999779 1.003348\n",
      "10        0.476528       0.474047         0.011538           0.999758 1.005233\n",
      "11        0.478597       0.476321         0.012327           0.999704 1.004779\n",
      "12        0.479543       0.478070         0.013283           0.999640 1.003082\n",
      "13        0.479554       0.478196         0.014270           0.999581 1.002840\n",
      "14        0.478703       0.477711         0.015137           0.999518 1.002077\n",
      "15        0.477080       0.476216         0.015933           0.999457 1.001815\n",
      "16        0.474734       0.474332         0.016557           0.999406 1.000847\n",
      "17        0.471705       0.470502         0.017025           0.999368 1.002556\n",
      "18        0.468053       0.467725         0.017691           0.999302 1.000701\n",
      "19        0.463826       0.466028         0.018614           0.999225 0.995275\n",
      "20        0.459058       0.460144         0.019598           0.999113 0.997639\n",
      "\n",
      "=== Last 20 timesteps ===\n",
      "   t  norm_predicted  norm_observed  norm_difference  cosine_similarity    ratio\n",
      " 981        0.086593       0.019022         0.086529           0.113179 4.552277\n",
      " 982        0.086779       0.018715         0.086724           0.110814 4.636986\n",
      " 983        0.086987       0.017195         0.086953           0.100858 5.059002\n",
      " 984        0.086959       0.016226         0.086933           0.094917 5.359171\n",
      " 985        0.087025       0.015895         0.087002           0.092749 5.474837\n",
      " 986        0.087032       0.018627         0.086976           0.110016 4.672312\n",
      " 987        0.087321       0.019190         0.087265           0.112800 4.550251\n",
      " 988        0.087386       0.022314         0.087297           0.131681 3.916301\n",
      " 989        0.087182       0.023737         0.087065           0.141032 3.672829\n",
      " 990        0.086956       0.022499         0.086856           0.133804 3.864799\n",
      " 991        0.086840       0.021403         0.086754           0.127217 4.057341\n",
      " 992        0.086884       0.024083         0.086815           0.141438 3.607710\n",
      " 993        0.086960       0.061066         0.086888           0.352290 1.424031\n",
      " 994        0.086872       0.058927         0.086806           0.340288 1.474247\n",
      " 995        0.086694       0.036235         0.086667           0.209712 2.392554\n",
      " 996        0.086629       0.018804         0.086602           0.109921 4.606926\n",
      " 997        0.086809       0.019902         0.086754           0.117401 4.361845\n",
      " 998        0.086883       0.019325         0.086836           0.113620 4.495988\n",
      " 999        0.087209       0.024083         0.087174           0.139539 3.621104\n",
      "1000        0.087487       0.031787         0.087449           0.182875 2.752292\n",
      "\n",
      "=== Summary Statistics ===\n",
      "                 t  norm_predicted  norm_observed  norm_difference  \\\n",
      "count  1000.000000     1000.000000    1000.000000      1000.000000   \n",
      "mean    500.500000        0.102071       0.066647         0.075841   \n",
      "std     288.819436        0.071116       0.086556         0.017025   \n",
      "min       1.000000        0.065198       0.000503         0.010112   \n",
      "25%     250.750000        0.083948       0.013259         0.068554   \n",
      "50%     500.500000        0.085714       0.049189         0.085245   \n",
      "75%     750.250000        0.087470       0.070000         0.086375   \n",
      "max    1000.000000        0.485381       0.479647         0.090158   \n",
      "\n",
      "       cosine_similarity        ratio  \n",
      "count        1000.000000  1000.000000  \n",
      "mean            0.376426     7.639179  \n",
      "std             0.293879    15.570129  \n",
      "min             0.002977     0.726439  \n",
      "25%             0.077683     1.138073  \n",
      "50%             0.374534     1.600461  \n",
      "75%             0.578283     6.438722  \n",
      "max             0.999925   169.635498  \n"
     ]
    }
   ],
   "source": [
    "# Flatten to (1000, 3699*64) for easier norm computation\n",
    "pred_flat = predicted_dW_dead.reshape(NUM_STEPS, -1)\n",
    "obs_flat = observed_dW_dead.float().reshape(NUM_STEPS, -1)\n",
    "\n",
    "# Compute metrics for each timestep\n",
    "norm_predicted = torch.norm(pred_flat, dim=1)\n",
    "norm_observed = torch.norm(obs_flat, dim=1)\n",
    "norm_difference = torch.norm(pred_flat - obs_flat, dim=1)\n",
    "\n",
    "# Cosine similarity\n",
    "cosine_sim = torch.sum(pred_flat * obs_flat, dim=1) / (norm_predicted * norm_observed + 1e-10)\n",
    "\n",
    "# Ratio\n",
    "ratio = norm_predicted / (norm_observed + 1e-10)\n",
    "\n",
    "# Create results table\n",
    "results = pd.DataFrame({\n",
    "    't': range(1, NUM_STEPS + 1),\n",
    "    'norm_predicted': norm_predicted.numpy(),\n",
    "    'norm_observed': norm_observed.numpy(),\n",
    "    'norm_difference': norm_difference.numpy(),\n",
    "    'cosine_similarity': cosine_sim.numpy(),\n",
    "    'ratio': ratio.numpy()\n",
    "})\n",
    "\n",
    "# Display first 20 rows, last 20 rows, and some summary stats\n",
    "print(\"\\n=== First 20 timesteps ===\")\n",
    "print(results.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Last 20 timesteps ===\")\n",
    "print(results.tail(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(results.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Full Results\n",
    "\n",
    "Save complete table for detailed inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved full results to ../tensors/Thimble/thimble_1_accounting_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV for easy inspection\n",
    "output_path = Path(\"../tensors/Thimble/thimble_1_accounting_results.csv\")\n",
    "results.to_csv(output_path, index=False)\n",
    "print(f\"\\nSaved full results to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
