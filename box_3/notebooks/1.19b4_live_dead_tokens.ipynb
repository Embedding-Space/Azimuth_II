{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.19b4: Identify Live vs Dead Tokens\n",
    "\n",
    "**Goal:** Determine *exactly* which tokens will be trained by tokenizing the actual model training corpus.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "In **1.19b2** we *predicted* that ~1,272 Thai tokens would be \"dead\" (never appear during training).\n",
    "\n",
    "But why predict when we can **measure**?\n",
    "\n",
    "By tokenizing our actual English-only training corpus with the Flannel tokenizer, we can determine with **perfect certainty**:\n",
    "- Which tokens appear (\"live\" - will receive gradient updates)\n",
    "- Which tokens never appear (\"dead\" - stay frozen at initialization)\n",
    "\n",
    "## This Notebook\n",
    "\n",
    "1. Load the Flannel tokenizer\n",
    "2. Load the model training corpus (English-only, from 1.18a)\n",
    "3. Tokenize the entire corpus\n",
    "4. Identify which tokens appear vs don't appear\n",
    "5. Create masks and indices for live/dead tokens\n",
    "6. Save for use in training and analysis\n",
    "\n",
    "## Why Masks?\n",
    "\n",
    "Saving boolean masks lets us:\n",
    "- Filter embeddings during analysis: `W[live_mask]` or `W[dead_mask]`\n",
    "- Count live vs dead: `live_mask.sum()` vs `dead_mask.sum()`\n",
    "- Validate experiments: \"Did exactly the dead tokens stay frozen?\"\n",
    "\n",
    "If we change the corpus or tokenizer, just rerun this notebook to update masks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Parameters set\n"
     ]
    }
   ],
   "source": [
    "# Input files\n",
    "TOKENIZER_PATH = \"../data/flannel_tokenizer_chars.json\"\n",
    "TRAINING_CORPUS_PATH = \"../data/flannel_model_corpus.txt\"  # From 1.18a\n",
    "\n",
    "# Output\n",
    "OUTPUT_PATH = \"../tensors/Flannel/live_dead_tokens.safetensors\"\n",
    "\n",
    "print(\"âœ“ Parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports complete\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import torch\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "print(\"âœ“ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from ../data/flannel_tokenizer_chars.json...\n",
      "\n",
      "âœ“ Loaded tokenizer\n",
      "  Vocabulary size: 10,000 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tokenizer from {TOKENIZER_PATH}...\\n\")\n",
    "\n",
    "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"âœ“ Loaded tokenizer\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training corpus from ../data/flannel_model_corpus.txt...\n",
      "\n",
      "âœ“ Loaded training corpus\n",
      "  Size: 5,257,019 bytes (5.01 MB)\n",
      "  Characters: 5,225,690\n"
     ]
    }
   ],
   "source": [
    "corpus_path = Path(TRAINING_CORPUS_PATH)\n",
    "\n",
    "if not corpus_path.exists():\n",
    "    raise FileNotFoundError(f\"Training corpus not found at {TRAINING_CORPUS_PATH}. Run 1.18a first.\")\n",
    "\n",
    "print(f\"\\nLoading training corpus from {TRAINING_CORPUS_PATH}...\\n\")\n",
    "\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "corpus_bytes = len(corpus_text.encode('utf-8'))\n",
    "corpus_mb = corpus_bytes / (1024 * 1024)\n",
    "corpus_chars = len(corpus_text)\n",
    "\n",
    "print(f\"âœ“ Loaded training corpus\")\n",
    "print(f\"  Size: {corpus_bytes:,} bytes ({corpus_mb:.2f} MB)\")\n",
    "print(f\"  Characters: {corpus_chars:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Corpus\n",
    "\n",
    "This is the key step: tokenize the entire training corpus to see which tokens actually appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing corpus...\n",
      "\n",
      "âœ“ Tokenization complete\n",
      "  Total tokens: 1,371,328\n",
      "  Unique tokens: 6,301\n",
      "  Tokens per character: 0.262\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTokenizing corpus...\\n\")\n",
    "\n",
    "# Tokenize\n",
    "encoding = tokenizer.encode(corpus_text)\n",
    "token_ids = encoding.ids\n",
    "\n",
    "print(f\"âœ“ Tokenization complete\")\n",
    "print(f\"  Total tokens: {len(token_ids):,}\")\n",
    "print(f\"  Unique tokens: {len(set(token_ids)):,}\")\n",
    "print(f\"  Tokens per character: {len(token_ids) / corpus_chars:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Live vs Dead Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying live vs dead tokens...\n",
      "\n",
      "âœ“ Classification complete\n",
      "  Live tokens: 6,301 (63.01%)\n",
      "  Dead tokens: 3,699 (36.99%)\n",
      "\n",
      "Token frequency statistics:\n",
      "  Min occurrences: 1\n",
      "  Max occurrences: 45,630\n",
      "  Mean occurrences: 217.6\n",
      "  Median occurrences: 68\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nIdentifying live vs dead tokens...\\n\")\n",
    "\n",
    "# Get set of tokens that appear in corpus\n",
    "tokens_in_corpus = set(token_ids)\n",
    "\n",
    "# Count occurrences\n",
    "token_counts = Counter(token_ids)\n",
    "\n",
    "# Classify every token in vocabulary\n",
    "live_token_ids = sorted(tokens_in_corpus)\n",
    "dead_token_ids = sorted(set(range(vocab_size)) - tokens_in_corpus)\n",
    "\n",
    "num_live = len(live_token_ids)\n",
    "num_dead = len(dead_token_ids)\n",
    "\n",
    "print(f\"âœ“ Classification complete\")\n",
    "print(f\"  Live tokens: {num_live:,} ({100*num_live/vocab_size:.2f}%)\")\n",
    "print(f\"  Dead tokens: {num_dead:,} ({100*num_dead/vocab_size:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Show token frequency distribution\n",
    "print(f\"Token frequency statistics:\")\n",
    "counts = list(token_counts.values())\n",
    "print(f\"  Min occurrences: {min(counts):,}\")\n",
    "print(f\"  Max occurrences: {max(counts):,}\")\n",
    "print(f\"  Mean occurrences: {sum(counts)/len(counts):.1f}\")\n",
    "print(f\"  Median occurrences: {sorted(counts)[len(counts)//2]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Masks and Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating masks and indices...\n",
      "\n",
      "âœ“ Created masks and indices\n",
      "  live_mask: torch.Size([10000]) bool tensor\n",
      "  dead_mask: torch.Size([10000]) bool tensor\n",
      "  live_indices: torch.Size([6301]) long tensor\n",
      "  dead_indices: torch.Size([3699]) long tensor\n",
      "  token_occurrence_counts: torch.Size([10000]) long tensor\n",
      "\n",
      "Verification:\n",
      "  live_mask.sum() = 6,301 (expected 6,301)\n",
      "  dead_mask.sum() = 3,699 (expected 3,699)\n",
      "  live_indices.numel() = 6,301\n",
      "  dead_indices.numel() = 3,699\n",
      "  token_occurrence_counts.sum() = 1,371,328 (total tokens)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCreating masks and indices...\\n\")\n",
    "\n",
    "# Create boolean masks\n",
    "live_mask = torch.zeros(vocab_size, dtype=torch.bool)\n",
    "live_mask[live_token_ids] = True\n",
    "\n",
    "dead_mask = ~live_mask\n",
    "\n",
    "# Create index tensors (for advanced indexing)\n",
    "live_indices = torch.tensor(live_token_ids, dtype=torch.long)\n",
    "dead_indices = torch.tensor(dead_token_ids, dtype=torch.long)\n",
    "\n",
    "# Create count tensor (how many times each token appears)\n",
    "token_occurrence_counts = torch.zeros(vocab_size, dtype=torch.long)\n",
    "for token_id, count in token_counts.items():\n",
    "    token_occurrence_counts[token_id] = count\n",
    "\n",
    "print(f\"âœ“ Created masks and indices\")\n",
    "print(f\"  live_mask: {live_mask.shape} bool tensor\")\n",
    "print(f\"  dead_mask: {dead_mask.shape} bool tensor\")\n",
    "print(f\"  live_indices: {live_indices.shape} long tensor\")\n",
    "print(f\"  dead_indices: {dead_indices.shape} long tensor\")\n",
    "print(f\"  token_occurrence_counts: {token_occurrence_counts.shape} long tensor\")\n",
    "print()\n",
    "\n",
    "# Verify\n",
    "print(f\"Verification:\")\n",
    "print(f\"  live_mask.sum() = {live_mask.sum().item():,} (expected {num_live:,})\")\n",
    "print(f\"  dead_mask.sum() = {dead_mask.sum().item():,} (expected {num_dead:,})\")\n",
    "print(f\"  live_indices.numel() = {live_indices.numel():,}\")\n",
    "print(f\"  dead_indices.numel() = {dead_indices.numel():,}\")\n",
    "print(f\"  token_occurrence_counts.sum() = {token_occurrence_counts.sum().item():,} (total tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Dead Tokens by Category\n",
    "\n",
    "Which categories from 1.19b2 are dead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing dead tokens by category...\n",
      "\n",
      "Dead tokens by category:\n",
      "  English     :    35 (  0.9% of dead tokens)\n",
      "  Thai        : 1,272 ( 34.4% of dead tokens)\n",
      "  Numeric     :     1 (  0.0% of dead tokens)\n",
      "  Whitespace  :     3 (  0.1% of dead tokens)\n",
      "  Punctuation :     3 (  0.1% of dead tokens)\n",
      "  Special     :     1 (  0.0% of dead tokens)\n",
      "  Other       : 2,384 ( 64.4% of dead tokens)\n",
      "\n",
      "Prediction vs Reality:\n",
      "  Predicted dead (from 1.19b2): 1,272 tokens\n",
      "  Actual dead: 3,699 tokens\n",
      "  Difference: 2,427 tokens\n",
      "  âš  Prediction was off by 2,427 tokens\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load demographics from 1.19b2\n",
    "demographics_path = \"../data/flannel_token_demographics.json\"\n",
    "if Path(demographics_path).exists():\n",
    "    print(f\"\\nAnalyzing dead tokens by category...\\n\")\n",
    "    \n",
    "    with open(demographics_path, 'r', encoding='utf-8') as f:\n",
    "        demographics = json.load(f)\n",
    "    \n",
    "    # Count dead tokens by category\n",
    "    dead_by_category = Counter()\n",
    "    for token_id in dead_token_ids:\n",
    "        token_id_str = str(token_id)\n",
    "        if token_id_str in demographics['token_classifications']:\n",
    "            category = demographics['token_classifications'][token_id_str]['category']\n",
    "            dead_by_category[category] += 1\n",
    "    \n",
    "    print(f\"Dead tokens by category:\")\n",
    "    for category in ['english', 'thai', 'mixed', 'numeric', 'whitespace', 'punctuation', 'special', 'other']:\n",
    "        count = dead_by_category[category]\n",
    "        if count > 0:\n",
    "            pct = 100 * count / num_dead\n",
    "            print(f\"  {category.capitalize():12s}: {count:5,} ({pct:5.1f}% of dead tokens)\")\n",
    "    \n",
    "    # Compare to prediction\n",
    "    print()\n",
    "    predicted_dead = demographics['dead_token_estimate']\n",
    "    print(f\"Prediction vs Reality:\")\n",
    "    print(f\"  Predicted dead (from 1.19b2): {predicted_dead:,} tokens\")\n",
    "    print(f\"  Actual dead: {num_dead:,} tokens\")\n",
    "    print(f\"  Difference: {abs(num_dead - predicted_dead):,} tokens\")\n",
    "    \n",
    "    if abs(num_dead - predicted_dead) < 100:\n",
    "        print(f\"  âœ“ Excellent prediction! (<100 token error)\")\n",
    "    elif abs(num_dead - predicted_dead) < 500:\n",
    "        print(f\"  âœ“ Good prediction (<500 token error)\")\n",
    "    else:\n",
    "        print(f\"  âš  Prediction was off by {abs(num_dead - predicted_dead):,} tokens\")\n",
    "else:\n",
    "    print(f\"\\n(Demographics data not found, skipping category analysis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "Most frequent live tokens (top 20):\n",
      "     16: '.'                  - 45,630 occurrences\n",
      "     14: ','                  - 45,066 occurrences\n",
      "   2806: 'the'                - 40,579 occurrences\n",
      "   2818: 'and'                - 25,487 occurrences\n",
      "   2815: 'to'                 - 23,515 occurrences\n",
      "   2822: 'of'                 - 22,989 occurrences\n",
      "     67: 'a'                  - 20,791 occurrences\n",
      "   2802: 'in'                 - 17,296 occurrences\n",
      "     85: 's'                  - 13,280 occurrences\n",
      "     15: '-'                  - 13,160 occurrences\n",
      "   2814: 'is'                 - 10,849 occurrences\n",
      "   2836: 'for'                - 9,035 occurrences\n",
      "   2805: 'on'                 - 8,278 occurrences\n",
      "   2850: 'that'               - 8,255 occurrences\n",
      "   2816: 'ing'                - 6,992 occurrences\n",
      "     43: 'I'                  - 6,917 occurrences\n",
      "    896: 'â€™'                  - 6,742 occurrences\n",
      "   2820: 'it'                 - 6,517 occurrences\n",
      "   2817: 'ed'                 - 6,508 occurrences\n",
      "   2865: 'with'               - 6,402 occurrences\n",
      "\n",
      "Least frequent live tokens (bottom 20):\n",
      "   8369: 'ught'               - 1 occurrences\n",
      "   2642: 'ï¼š'                  - 1 occurrences\n",
      "    940: 'â†‘'                  - 1 occurrences\n",
      "    973: 'â³'                  - 1 occurrences\n",
      "    181: 'Ã»'                  - 1 occurrences\n",
      "   5903: 'Manag'              - 1 occurrences\n",
      "    211: 'Ä«'                  - 1 occurrences\n",
      "    184: 'Ã¾'                  - 1 occurrences\n",
      "    496: 'Ù‰'                  - 1 occurrences\n",
      "    484: 'Ø·'                  - 1 occurrences\n",
      "    460: 'ØŒ'                  - 1 occurrences\n",
      "    467: 'Ø¦'                  - 1 occurrences\n",
      "    464: 'Ø£'                  - 1 occurrences\n",
      "    472: 'Ø«'                  - 1 occurrences\n",
      "   2742: 'ðŸ–¤'                  - 1 occurrences\n",
      "    217: 'Å'                  - 1 occurrences\n",
      "    189: 'Äƒ'                  - 1 occurrences\n",
      "   2762: 'ðŸ˜¦'                  - 1 occurrences\n",
      "    107: 'Âª'                  - 1 occurrences\n",
      "    108: 'Â«'                  - 1 occurrences\n",
      "\n",
      "Sample dead tokens (first 20):\n",
      "      0: '<|endoftext|>'\n",
      "      1: '\\n'\n",
      "      2: ' '\n",
      "     93: '{'\n",
      "     95: '}'\n",
      "     97: '\\xa0'\n",
      "    101: 'Â¤'\n",
      "    102: 'Â¥'\n",
      "    109: 'Â¬'\n",
      "    111: 'Â¯'\n",
      "    115: 'Â³'\n",
      "    117: 'Âµ'\n",
      "    118: 'Â¶'\n",
      "    120: 'Â¸'\n",
      "    121: 'Â¹'\n",
      "    122: 'Âº'\n",
      "    127: 'Â¿'\n",
      "    128: 'Ã€'\n",
      "    131: 'Ãƒ'\n",
      "    135: 'Ã‡'\n",
      "  ... and 3,679 more\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EXAMPLES\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Reverse vocab for lookups\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "print(f\"Most frequent live tokens (top 20):\")\n",
    "most_common = token_counts.most_common(20)\n",
    "for token_id, count in most_common:\n",
    "    token_str = id_to_token[token_id]\n",
    "    print(f\"  {token_id:5d}: {repr(token_str):20s} - {count:,} occurrences\")\n",
    "print()\n",
    "\n",
    "print(f\"Least frequent live tokens (bottom 20):\")\n",
    "least_common = sorted(token_counts.items(), key=lambda x: x[1])[:20]\n",
    "for token_id, count in least_common:\n",
    "    token_str = id_to_token[token_id]\n",
    "    print(f\"  {token_id:5d}: {repr(token_str):20s} - {count:,} occurrences\")\n",
    "print()\n",
    "\n",
    "print(f\"Sample dead tokens (first 20):\")\n",
    "for token_id in dead_token_ids[:20]:\n",
    "    token_str = id_to_token[token_id]\n",
    "    print(f\"  {token_id:5d}: {repr(token_str)}\")\n",
    "\n",
    "if len(dead_token_ids) > 20:\n",
    "    print(f\"  ... and {len(dead_token_ids) - 20:,} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Masks and Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving masks to ../tensors/Flannel/live_dead_tokens.safetensors...\n",
      "\n",
      "âœ“ Saved masks and indices\n",
      "  Path: ../tensors/Flannel/live_dead_tokens.safetensors\n",
      "  Size: 176.2 KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving masks to {OUTPUT_PATH}...\\n\")\n",
    "\n",
    "# Prepare data dictionary\n",
    "save_data = {\n",
    "    'live_mask': live_mask,\n",
    "    'dead_mask': dead_mask,\n",
    "    'live_indices': live_indices,\n",
    "    'dead_indices': dead_indices,\n",
    "    'token_occurrence_counts': token_occurrence_counts,\n",
    "}\n",
    "\n",
    "# Ensure directory exists\n",
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save\n",
    "save_file(save_data, str(OUTPUT_PATH))\n",
    "\n",
    "# Verify\n",
    "output_path = Path(OUTPUT_PATH)\n",
    "if output_path.exists():\n",
    "    output_kb = output_path.stat().st_size / 1024\n",
    "    print(f\"âœ“ Saved masks and indices\")\n",
    "    print(f\"  Path: {OUTPUT_PATH}\")\n",
    "    print(f\"  Size: {output_kb:.1f} KB\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Failed to save to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "Show how to use these masks in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "USAGE EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "How to use these masks in analysis:\n",
      "\n",
      "1. Filter W matrix to only live tokens:\n",
      "   from safetensors.torch import load_file\n",
      "   masks = load_file('../tensors/Flannel/live_dead_tokens.safetensors')\n",
      "   W_live = W[masks['live_mask']]\n",
      "   # Shape: (6301, hidden_dim)\n",
      "\n",
      "2. Filter to only dead tokens:\n",
      "   W_dead = W[masks['dead_mask']]\n",
      "   # Shape: (3699, hidden_dim)\n",
      "\n",
      "3. Get specific live tokens by index:\n",
      "   W_specific = W[masks['live_indices'][:100]]\n",
      "   # First 100 live tokens\n",
      "\n",
      "4. Check if a token is live:\n",
      "   token_id = 42\n",
      "   is_live = masks['live_mask'][token_id].item()\n",
      "   # True or False\n",
      "\n",
      "5. Count tokens by category:\n",
      "   num_live = masks['live_mask'].sum().item()\n",
      "   num_dead = masks['dead_mask'].sum().item()\n",
      "   # 6,301 live, 3,699 dead\n",
      "\n",
      "6. Weighted analysis by token frequency:\n",
      "   counts = masks['token_occurrence_counts']\n",
      "   live_counts = counts[masks['live_mask']]\n",
      "   # Analyze tokens weighted by how often they appear\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"USAGE EXAMPLES\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"How to use these masks in analysis:\\n\")\n",
    "\n",
    "print(f\"1. Filter W matrix to only live tokens:\")\n",
    "print(f\"   from safetensors.torch import load_file\")\n",
    "print(f\"   masks = load_file('{OUTPUT_PATH}')\")\n",
    "print(f\"   W_live = W[masks['live_mask']]\")\n",
    "print(f\"   # Shape: ({num_live}, hidden_dim)\\n\")\n",
    "\n",
    "print(f\"2. Filter to only dead tokens:\")\n",
    "print(f\"   W_dead = W[masks['dead_mask']]\")\n",
    "print(f\"   # Shape: ({num_dead}, hidden_dim)\\n\")\n",
    "\n",
    "print(f\"3. Get specific live tokens by index:\")\n",
    "print(f\"   W_specific = W[masks['live_indices'][:100]]\")\n",
    "print(f\"   # First 100 live tokens\\n\")\n",
    "\n",
    "print(f\"4. Check if a token is live:\")\n",
    "print(f\"   token_id = 42\")\n",
    "print(f\"   is_live = masks['live_mask'][token_id].item()\")\n",
    "print(f\"   # True or False\\n\")\n",
    "\n",
    "print(f\"5. Count tokens by category:\")\n",
    "print(f\"   num_live = masks['live_mask'].sum().item()\")\n",
    "print(f\"   num_dead = masks['dead_mask'].sum().item()\")\n",
    "print(f\"   # {num_live:,} live, {num_dead:,} dead\\n\")\n",
    "\n",
    "print(f\"6. Weighted analysis by token frequency:\")\n",
    "print(f\"   counts = masks['token_occurrence_counts']\")\n",
    "print(f\"   live_counts = counts[masks['live_mask']]\")\n",
    "print(f\"   # Analyze tokens weighted by how often they appear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LIVE/DEAD TOKEN ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Training corpus:\n",
      "  Path: ../data/flannel_model_corpus.txt\n",
      "  Size: 5.01 MB\n",
      "  Total tokens after tokenization: 1,371,328\n",
      "\n",
      "Tokenizer:\n",
      "  Path: ../data/flannel_tokenizer_chars.json\n",
      "  Vocabulary: 10,000 tokens\n",
      "\n",
      "Results (EMPIRICAL, not predicted):\n",
      "  Live tokens: 6,301 (63.0%)\n",
      "  Dead tokens: 3,699 (37.0%)\n",
      "\n",
      "Output:\n",
      "  Masks and indices: ../tensors/Flannel/live_dead_tokens.safetensors\n",
      "\n",
      "What this means:\n",
      "  âœ“ 6,301 tokens WILL receive gradient updates during training\n",
      "  âœ“ 3,699 tokens will NEVER appear in training\n",
      "  âœ“ Dead tokens should stay frozen at initialization\n",
      "  âœ“ Perfect validation: count frozen tokens after training\n",
      "\n",
      "Next steps:\n",
      "  â†’ Train Flannel 1 (notebook 1.20a)\n",
      "  â†’ Use these masks to analyze live vs dead token behavior\n",
      "  â†’ Validate: did dead tokens stay frozen?\n",
      "  â†’ If corpus or tokenizer changes, rerun this notebook\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"LIVE/DEAD TOKEN ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Training corpus:\")\n",
    "print(f\"  Path: {TRAINING_CORPUS_PATH}\")\n",
    "print(f\"  Size: {corpus_mb:.2f} MB\")\n",
    "print(f\"  Total tokens after tokenization: {len(token_ids):,}\\n\")\n",
    "\n",
    "print(f\"Tokenizer:\")\n",
    "print(f\"  Path: {TOKENIZER_PATH}\")\n",
    "print(f\"  Vocabulary: {vocab_size:,} tokens\\n\")\n",
    "\n",
    "print(f\"Results (EMPIRICAL, not predicted):\")\n",
    "print(f\"  Live tokens: {num_live:,} ({100*num_live/vocab_size:.1f}%)\")\n",
    "print(f\"  Dead tokens: {num_dead:,} ({100*num_dead/vocab_size:.1f}%)\\n\")\n",
    "\n",
    "print(f\"Output:\")\n",
    "print(f\"  Masks and indices: {OUTPUT_PATH}\\n\")\n",
    "\n",
    "print(f\"What this means:\")\n",
    "print(f\"  âœ“ {num_live:,} tokens WILL receive gradient updates during training\")\n",
    "print(f\"  âœ“ {num_dead:,} tokens will NEVER appear in training\")\n",
    "print(f\"  âœ“ Dead tokens should stay frozen at initialization\")\n",
    "print(f\"  âœ“ Perfect validation: count frozen tokens after training\\n\")\n",
    "\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  â†’ Train Flannel 1 (notebook 1.20a)\")\n",
    "print(f\"  â†’ Use these masks to analyze live vs dead token behavior\")\n",
    "print(f\"  â†’ Validate: did dead tokens stay frozen?\")\n",
    "print(f\"  â†’ If corpus or tokenizer changes, rerun this notebook\")\n",
    "print()\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
