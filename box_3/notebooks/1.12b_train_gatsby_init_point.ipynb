{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 1.12b: Train Lil Gatsby from Supermassive Black Hole (bfloat16-native)\n",
    "\n",
    "**Goal:** Initialize all token embeddings to the same point (supermassive BH) plus optional Gaussian noise in **native bfloat16**, then train and watch what happens.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "If untrained tokens experience only \"thermal jostling\" (small gradient updates from being wrong), starting from a supermassive BH might produce a spongecrystal-like structure:\n",
    "\n",
    "1. **Trained tokens** get large gradients → boil off and disperse\n",
    "2. **Untrained tokens** get tiny gradients → stay in thermal bath, freeze into lattice\n",
    "3. **Optimal σ** exists where initial cloud is small enough to stay coherent but large enough to explore lattice neighborhood\n",
    "\n",
    "## Initialization Strategy (bfloat16-native)\n",
    "\n",
    "**Pure bfloat16 throughout** (matching Qwen 3's native training precision):\n",
    "\n",
    "- Pick random point from N(0, 0.02) **in bfloat16** (matching GPT-2 init scale)\n",
    "- Add Gaussian noise N(0, σ) **in bfloat16**\n",
    "- Initialize all W vectors to `bh_center + noise`\n",
    "- σ = 0: Perfect supermassive BH (all tokens identical)\n",
    "- σ > 0: Thermal cloud around BH\n",
    "\n",
    "**No float32 intermediate step** - pure bfloat16 from generation to training.\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "- **Vocabulary:** 128 tokens (ASCII)\n",
    "- **Context window:** 128 tokens\n",
    "- **Hidden dimensions:** 64\n",
    "- **Layers:** 2\n",
    "- **Attention heads:** 2\n",
    "- **Weight tying:** E = W^T (matches Qwen 3 4B)\n",
    "- **Total parameters:** ~117k\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "- **Steps:** 10,000\n",
    "- **Batch size:** 1 (with gradient accumulation if needed)\n",
    "- **Optimizer:** Adam (lr=0.001, β₁=0.9, β₂=0.999)\n",
    "- **Weight decay:** 0.0 (no regularization)\n",
    "- **Precision:** Native bfloat16\n",
    "\n",
    "## Data Recording (Every Step)\n",
    "\n",
    "- **Embeddings:** (10,001, 128, 64) bfloat16\n",
    "- **Gradients:** (10,001, 128, 64) bfloat16  \n",
    "- **Adam momentum:** (10,001, 128, 64) bfloat16\n",
    "- **Adam variance:** (10,001, 128, 64) bfloat16\n",
    "- **Logits:** (10,001, 128) bfloat16\n",
    "- **Loss:** (10,001,) bfloat16\n",
    "\n",
    "Total: ~660 MB saved as safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "VOCAB_SIZE = 128\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYER = 2\n",
    "N_HEAD = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION = 1\n",
    "NUM_TRAIN_STEPS = 10000\n",
    "LEARNING_RATE = 1e-3  # 0.001 for Adam\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# Optimizer: Adam\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "# Initialization (SUPERMASSIVE BLACK HOLE - bfloat16 native)\n",
    "INIT_SCALE = 0.02  # Match GPT-2 default initialization scale\n",
    "SIGMA = 0.0        # Gaussian noise std dev (0 = perfect BH, >0 = thermal cloud)\n",
    "\n",
    "# Data\n",
    "CORPUS_PATH = \"../data/gatsby_clean.txt\"\n",
    "OUTPUT_DIR = \"../tensors/Lil_Gatsby\"\n",
    "OUTPUT_FILE = f\"1.12b_training_data_sigma{SIGMA:.0e}.safetensors\"\n",
    "\n",
    "# Instrumentation\n",
    "RECORD_EVERY_N_STEPS = 1  # Record every step\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "import time\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus: ../data/gatsby_clean.txt\n",
      "  Total bytes: 268,928\n",
      "✓ Corpus on device\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "\n",
    "with open(CORPUS_PATH, 'r', encoding='ascii') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "# Convert to byte array (only keep bytes < VOCAB_SIZE)\n",
    "corpus_bytes = [b for b in corpus_text.encode('ascii') if b < VOCAB_SIZE]\n",
    "\n",
    "print(f\"  Total bytes: {len(corpus_bytes):,}\")\n",
    "\n",
    "# Pre-load corpus to device\n",
    "corpus_tensor = torch.tensor(corpus_bytes, dtype=torch.long, device=device)\n",
    "print(f\"✓ Corpus on device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset: 268,800 examples\n"
     ]
    }
   ],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = ByteDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "print(f\"✓ Dataset: {len(dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model created: 116,480 parameters\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✓ Model created: {total_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## bfloat16-Native Initialization (SUPERMASSIVE BLACK HOLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INITIALIZING SUPERMASSIVE BLACK HOLE (bfloat16-native)\n",
      "================================================================================\n",
      "\n",
      "Black hole center (bfloat16):\n",
      "  First 8 dims: [0.01806640625, 0.00445556640625, 0.0029144287109375, 0.01470947265625, -0.00830078125, -0.0022125244140625, 0.00909423828125, -0.06640625]\n",
      "  Norm: 0.156989\n",
      "\n",
      "σ = 0e+00 → Perfect supermassive black hole\n",
      "  All 128 tokens initialized to identical bfloat16 vector\n",
      "\n",
      "✓ Initialized embeddings (pure bfloat16)\n",
      "  Shape: torch.Size([128, 64])\n",
      "  Dtype: torch.bfloat16\n",
      "\n",
      "Initial embedding statistics:\n",
      "  Max pairwise distance: 0.000000\n",
      "  Mean pairwise distance: 0.000000\n",
      "  ✓ All vectors identical (as expected for σ=0)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"INITIALIZING SUPERMASSIVE BLACK HOLE (bfloat16-native)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Generate BH center directly in bfloat16\n",
    "bh_center_bf16 = (torch.randn(HIDDEN_DIM, dtype=torch.float32, device=device) * INIT_SCALE).to(torch.bfloat16)\n",
    "\n",
    "print(f\"Black hole center (bfloat16):\")\n",
    "print(f\"  First 8 dims: {bh_center_bf16[:8].cpu().float().tolist()}\")\n",
    "print(f\"  Norm: {torch.norm(bh_center_bf16.float()).item():.6f}\")\n",
    "print()\n",
    "\n",
    "# Initialize all W vectors to BH center + noise (all in bfloat16)\n",
    "if SIGMA == 0.0:\n",
    "    # Perfect supermassive BH - all tokens identical\n",
    "    init_bf16 = bh_center_bf16.unsqueeze(0).expand(VOCAB_SIZE, HIDDEN_DIM).clone()\n",
    "    print(f\"σ = {SIGMA:.0e} → Perfect supermassive black hole\")\n",
    "    print(f\"  All {VOCAB_SIZE} tokens initialized to identical bfloat16 vector\")\n",
    "else:\n",
    "    # Thermal cloud around BH (noise generated in bfloat16)\n",
    "    noise_bf16 = (torch.randn(VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32, device=device) * SIGMA).to(torch.bfloat16)\n",
    "    init_bf16 = bh_center_bf16.unsqueeze(0) + noise_bf16\n",
    "    print(f\"σ = {SIGMA:.0e} → Thermal cloud around black hole\")\n",
    "    print(f\"  Initial cloud size (std): {noise_bf16.float().std().item():.6f}\")\n",
    "    print(f\"  Initial cloud radius (max dist from center): {torch.norm(noise_bf16.float(), dim=1).max().item():.6f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Assign to model\n",
    "with torch.no_grad():\n",
    "    model.transformer.wte.weight[:] = init_bf16\n",
    "\n",
    "print(f\"✓ Initialized embeddings (pure bfloat16)\")\n",
    "print(f\"  Shape: {model.transformer.wte.weight.shape}\")\n",
    "print(f\"  Dtype: {model.transformer.wte.weight.dtype}\")\n",
    "print()\n",
    "\n",
    "# Verify initialization\n",
    "W_check = model.transformer.wte.weight.cpu().float()\n",
    "pairwise_dists = torch.cdist(W_check, W_check)\n",
    "max_dist = pairwise_dists.max().item()\n",
    "mean_dist = pairwise_dists[torch.triu(torch.ones_like(pairwise_dists), diagonal=1) == 1].mean().item()\n",
    "\n",
    "print(f\"Initial embedding statistics:\")\n",
    "print(f\"  Max pairwise distance: {max_dist:.6f}\")\n",
    "print(f\"  Mean pairwise distance: {mean_dist:.6f}\")\n",
    "\n",
    "if SIGMA == 0.0:\n",
    "    if max_dist < 1e-6:\n",
    "        print(f\"  ✓ All vectors identical (as expected for σ=0)\")\n",
    "    else:\n",
    "        print(f\"  ⚠️  Quantization artifacts: max_dist = {max_dist:.6f} (expected ~0)\")\n",
    "\n",
    "print()\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Comprehensive Recorder\n",
    "\n",
    "Records everything at every step, holds in RAM until training completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Recorder class defined\n"
     ]
    }
   ],
   "source": [
    "class ComprehensiveRecorder:\n",
    "    \"\"\"Records embeddings, gradients, optimizer state, logits, loss at every step in bfloat16.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim, record_every_n):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.record_every_n = record_every_n\n",
    "        \n",
    "        # Storage (lists of tensors, keep in RAM)\n",
    "        self.recorded_steps = []\n",
    "        self.embeddings = []      # [n_recorded, vocab_size, hidden_dim]\n",
    "        self.grads = []           # [n_recorded, vocab_size, hidden_dim]\n",
    "        self.momentum = []        # [n_recorded, vocab_size, hidden_dim]\n",
    "        self.variance = []        # [n_recorded, vocab_size, hidden_dim]\n",
    "        self.logits = []          # [n_recorded, vocab_size]\n",
    "        self.losses = []          # [n_recorded]\n",
    "        \n",
    "        # Temporary storage\n",
    "        self.current_step = 0\n",
    "        self.recorded_initial = False\n",
    "        self.grad_before = None\n",
    "        self.loss_value = None\n",
    "        self.logits_sample = None\n",
    "    \n",
    "    def record_initial_state(self, model, optimizer):\n",
    "        \"\"\"Record step 0: initial state before training.\"\"\"\n",
    "        if not self.recorded_initial:\n",
    "            W = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "            \n",
    "            # Step 0: no gradients, no optimizer state yet (zeros)\n",
    "            self.recorded_steps.append(0)\n",
    "            self.embeddings.append(W)\n",
    "            self.grads.append(torch.zeros_like(W))\n",
    "            self.momentum.append(torch.zeros_like(W))\n",
    "            self.variance.append(torch.zeros_like(W))\n",
    "            self.logits.append(torch.zeros(self.vocab_size, dtype=torch.bfloat16))\n",
    "            self.losses.append(torch.tensor(float('nan'), dtype=torch.bfloat16))  # No loss yet\n",
    "            \n",
    "            self.recorded_initial = True\n",
    "            self.current_step = 1\n",
    "            \n",
    "            print(f\"✓ Recorded initial state (step 0)\")\n",
    "    \n",
    "    def record_before_step(self, model, loss, logits):\n",
    "        \"\"\"Call after forward/backward, before optimizer step.\"\"\"\n",
    "        if self.current_step % self.record_every_n == 0:\n",
    "            # Capture gradients in bfloat16\n",
    "            if model.transformer.wte.weight.grad is not None:\n",
    "                self.grad_before = model.transformer.wte.weight.grad.clone().cpu().bfloat16()\n",
    "            else:\n",
    "                self.grad_before = torch.zeros(self.vocab_size, self.hidden_dim, dtype=torch.bfloat16)\n",
    "            \n",
    "            # Capture loss\n",
    "            self.loss_value = loss.item()\n",
    "            \n",
    "            # Capture logits from first sequence, last position in bfloat16\n",
    "            self.logits_sample = logits[0, -1, :].detach().cpu().bfloat16()\n",
    "    \n",
    "    def record_after_step(self, model, optimizer):\n",
    "        \"\"\"Call after optimizer step.\"\"\"\n",
    "        if self.current_step % self.record_every_n == 0:\n",
    "            if self.grad_before is not None and self.loss_value is not None:\n",
    "                # Capture embeddings in bfloat16\n",
    "                W = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "\n",
    "                # Capture optimizer state (Adam momentum and variance)\n",
    "                param = model.transformer.wte.weight\n",
    "                if param in optimizer.state:\n",
    "                    state = optimizer.state[param]\n",
    "                    # Get state tensors if they exist, convert to bfloat16\n",
    "                    mom_src = state.get('exp_avg', None)\n",
    "                    var_src = state.get('exp_avg_sq', None)\n",
    "                    mom = mom_src.clone().cpu().bfloat16() if mom_src is not None else torch.zeros_like(W)\n",
    "                    var = var_src.clone().cpu().bfloat16() if var_src is not None else torch.zeros_like(W)\n",
    "                else:\n",
    "                    mom = torch.zeros_like(W)\n",
    "                    var = torch.zeros_like(W)\n",
    "\n",
    "                # Store everything\n",
    "                self.recorded_steps.append(self.current_step)\n",
    "                self.embeddings.append(W)\n",
    "                self.grads.append(self.grad_before)\n",
    "                self.momentum.append(mom)\n",
    "                self.variance.append(var)\n",
    "                self.logits.append(self.logits_sample)\n",
    "                self.losses.append(torch.tensor(self.loss_value, dtype=torch.bfloat16))\n",
    "\n",
    "                # Clear temp storage\n",
    "                self.grad_before = None\n",
    "                self.loss_value = None\n",
    "                self.logits_sample = None\n",
    "                \n",
    "                # Progress indicator every 1000 steps\n",
    "                if self.current_step % 1000 == 0:\n",
    "                    print(f\"  Recorded step {self.current_step:,}\")\n",
    "\n",
    "        self.current_step += 1\n",
    "    \n",
    "    def get_data(self):\n",
    "        \"\"\"Return recorded data as stacked tensors.\"\"\"\n",
    "        print(f\"\\nStacking {len(self.embeddings)} recorded states...\")\n",
    "        \n",
    "        return {\n",
    "            'recorded_steps': torch.tensor(self.recorded_steps, dtype=torch.long),\n",
    "            'embeddings': torch.stack(self.embeddings) if self.embeddings else torch.tensor([]),\n",
    "            'grads': torch.stack(self.grads) if self.grads else torch.tensor([]),\n",
    "            'momentum': torch.stack(self.momentum) if self.momentum else torch.tensor([]),\n",
    "            'variance': torch.stack(self.variance) if self.variance else torch.tensor([]),\n",
    "            'logits': torch.stack(self.logits) if self.logits else torch.tensor([]),\n",
    "            'losses': torch.stack(self.losses) if self.losses else torch.tensor([]),\n",
    "        }\n",
    "\n",
    "print(\"✓ Recorder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Custom Trainer with Instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ InstrumentedTrainer defined\n"
     ]
    }
   ],
   "source": [
    "class InstrumentedTrainer(Trainer):\n",
    "    def __init__(self, recorder, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.recorder = recorder\n",
    "        self.last_logits = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"Override to capture logits.\"\"\"\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Store logits for recorder\n",
    "        self.last_logits = outputs.logits\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        \"\"\"Override to inject recording.\"\"\"\n",
    "        # Standard forward + backward\n",
    "        loss = super().training_step(model, inputs, num_items_in_batch)\n",
    "        \n",
    "        # Record BEFORE optimizer step\n",
    "        self.recorder.record_before_step(model, loss, self.last_logits)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time=None, **kwargs):\n",
    "        \"\"\"Override to record AFTER optimizer step.\"\"\"\n",
    "        # Record AFTER optimizer updates parameters\n",
    "        self.recorder.record_after_step(model, self.optimizer)\n",
    "        \n",
    "        # Call parent\n",
    "        super()._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, **kwargs)\n",
    "\n",
    "print(\"✓ InstrumentedTrainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer ready (Adam, bf16=True)\n"
     ]
    }
   ],
   "source": [
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "recorder = ComprehensiveRecorder(VOCAB_SIZE, HIDDEN_DIM, RECORD_EVERY_N_STEPS)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=NUM_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    adam_beta1=ADAM_BETA1,\n",
    "    adam_beta2=ADAM_BETA2,\n",
    "    adam_epsilon=ADAM_EPSILON,\n",
    "    optim=\"adamw_torch\",  # Adam optimizer\n",
    "    logging_steps=1000,\n",
    "    save_steps=NUM_TRAIN_STEPS + 1,  # Don't save checkpoints\n",
    "    save_total_limit=0,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    "    bf16=True,  # Native bfloat16 training\n",
    "    seed=RANDOM_SEED,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = InstrumentedTrainer(\n",
    "    recorder=recorder,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer ready (Adam, bf16=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Record Initial State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Recorded initial state (step 0)\n"
     ]
    }
   ],
   "source": [
    "recorder.record_initial_state(model, trainer.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "**This will take ~2 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING 10,000-STEP TRAINING RUN\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Initialization: Supermassive BH (σ = 0e+00, bfloat16-native)\n",
      "  Optimizer: Adam\n",
      "  Learning rate: 0.001\n",
      "  Adam beta1: 0.9\n",
      "  Adam beta2: 0.999\n",
      "  Weight decay: 0.0\n",
      "  Precision: bfloat16 (native)\n",
      "  Steps: 10,000 (plus initial step 0)\n",
      "  Recording every: 1 step(s)\n",
      "  Expected records: 10,001\n",
      "  Expected file size: ~660 MB\n",
      "  Estimated runtime: ~2 minutes\n",
      "\n",
      "Recording:\n",
      "  - Embeddings (positions)\n",
      "  - Gradients (forces)\n",
      "  - Momentum (Adam first moment)\n",
      "  - Variance (Adam second moment)\n",
      "  - Logits (predictions)\n",
      "  - Loss\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 01:34, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.953500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.944700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.940700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.936800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Recorded step 1,000\n",
      "  Recorded step 2,000\n",
      "  Recorded step 3,000\n",
      "  Recorded step 4,000\n",
      "  Recorded step 5,000\n",
      "  Recorded step 6,000\n",
      "  Recorded step 7,000\n",
      "  Recorded step 8,000\n",
      "  Recorded step 9,000\n",
      "  Recorded step 10,000\n",
      "\n",
      "================================================================================\n",
      "✓ Training complete\n",
      "  Elapsed time: 1.6 minutes (95.6 seconds)\n",
      "  Throughput: 104.6 steps/second\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STARTING 10,000-STEP TRAINING RUN\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Initialization: Supermassive BH (σ = {SIGMA:.0e}, bfloat16-native)\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Adam beta1: {ADAM_BETA1}\")\n",
    "print(f\"  Adam beta2: {ADAM_BETA2}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Precision: bfloat16 (native)\")\n",
    "print(f\"  Steps: {NUM_TRAIN_STEPS:,} (plus initial step 0)\")\n",
    "print(f\"  Recording every: {RECORD_EVERY_N_STEPS} step(s)\")\n",
    "print(f\"  Expected records: {NUM_TRAIN_STEPS // RECORD_EVERY_N_STEPS + 1:,}\")\n",
    "print(f\"  Expected file size: ~660 MB\")\n",
    "print(f\"  Estimated runtime: ~2 minutes\")\n",
    "print(f\"\\nRecording:\")\n",
    "print(f\"  - Embeddings (positions)\")\n",
    "print(f\"  - Gradients (forces)\")\n",
    "print(f\"  - Momentum (Adam first moment)\")\n",
    "print(f\"  - Variance (Adam second moment)\")\n",
    "print(f\"  - Logits (predictions)\")\n",
    "print(f\"  - Loss\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training complete\")\n",
    "print(f\"  Elapsed time: {elapsed/60:.1f} minutes ({elapsed:.1f} seconds)\")\n",
    "print(f\"  Throughput: {NUM_TRAIN_STEPS / elapsed:.1f} steps/second\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Save Recorded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data for save...\n",
      "\n",
      "\n",
      "Stacking 10001 recorded states...\n",
      "Saving to: ../tensors/Lil_Gatsby/1.12b_training_data_sigma0e+00.safetensors\n",
      "This may take a minute...\n",
      "\n",
      "✓ Saved successfully\n",
      "  File: ../tensors/Lil_Gatsby/1.12b_training_data_sigma0e+00.safetensors\n",
      "  Size: 0.66 GB (658.1 MB)\n",
      "  Save time: 2.1 seconds\n",
      "  Recorded steps: 10,001\n",
      "  Step range: 0 to 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nPreparing data for save...\\n\")\n",
    "\n",
    "recorded_data = recorder.get_data()\n",
    "\n",
    "save_dict = {\n",
    "    'recorded_steps': recorded_data['recorded_steps'],\n",
    "    'embeddings': recorded_data['embeddings'],\n",
    "    'grads': recorded_data['grads'],\n",
    "    'momentum': recorded_data['momentum'],\n",
    "    'variance': recorded_data['variance'],\n",
    "    'logits': recorded_data['logits'],\n",
    "    'losses': recorded_data['losses'],\n",
    "    'bh_center': bh_center_bf16.cpu(),\n",
    "    'init_scale': torch.tensor(INIT_SCALE, dtype=torch.float32),\n",
    "    'sigma': torch.tensor(SIGMA, dtype=torch.float32),\n",
    "    'learning_rate': torch.tensor(LEARNING_RATE, dtype=torch.float32),\n",
    "    'weight_decay': torch.tensor(WEIGHT_DECAY, dtype=torch.float32),\n",
    "    'adam_beta1': torch.tensor(ADAM_BETA1, dtype=torch.float32),\n",
    "    'adam_beta2': torch.tensor(ADAM_BETA2, dtype=torch.float32),\n",
    "}\n",
    "\n",
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILE\n",
    "\n",
    "print(f\"Saving to: {output_path}\")\n",
    "print(f\"This may take a minute...\\n\")\n",
    "\n",
    "save_start = time.time()\n",
    "save_file(save_dict, str(output_path))\n",
    "save_elapsed = time.time() - save_start\n",
    "\n",
    "file_size_mb = output_path.stat().st_size / 1e6\n",
    "file_size_gb = file_size_mb / 1000\n",
    "\n",
    "print(f\"✓ Saved successfully\")\n",
    "print(f\"  File: {output_path}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB ({file_size_mb:.1f} MB)\")\n",
    "print(f\"  Save time: {save_elapsed:.1f} seconds\")\n",
    "print(f\"  Recorded steps: {len(recorded_data['recorded_steps']):,}\")\n",
    "print(f\"  Step range: {recorded_data['recorded_steps'][0]} to {recorded_data['recorded_steps'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Verify Data Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data shapes:\n",
      "  embeddings: torch.Size([10001, 128, 64])\n",
      "  grads: torch.Size([10001, 128, 64])\n",
      "  momentum: torch.Size([10001, 128, 64])\n",
      "  variance: torch.Size([10001, 128, 64])\n",
      "  logits: torch.Size([10001, 128])\n",
      "  losses: torch.Size([10001])\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  embeddings: {recorded_data['embeddings'].shape}\")\n",
    "print(f\"  grads: {recorded_data['grads'].shape}\")\n",
    "print(f\"  momentum: {recorded_data['momentum'].shape}\")\n",
    "print(f\"  variance: {recorded_data['variance'].shape}\")\n",
    "print(f\"  logits: {recorded_data['logits'].shape}\")\n",
    "print(f\"  losses: {recorded_data['losses'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Quick Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Embedding evolution:\n",
      "  Step 0 centroid norm: 0.156989\n",
      "  Step 10000 centroid norm: 0.162018\n",
      "\n",
      "Pairwise distances:\n",
      "  Step 0 max: 0.000000 (should be ~0 for σ=0)\n",
      "  Step 10000 max: 1.0715\n",
      "\n",
      "Gradient magnitudes (first step):\n",
      "  Mean: 8.051573e-02\n",
      "  Max: 8.757275e-01\n",
      "\n",
      "Adam state (after warmup):\n",
      "  Momentum at step 100: 1.042301e-03\n",
      "  Variance at step 100: 8.490872e-06\n",
      "\n",
      "Loss trajectory:\n",
      "  Step 1: 4.8438\n",
      "  Step 10000: 2.9062\n",
      "  Reduction: 1.9375\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"VERIFICATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "embeddings = recorded_data['embeddings']\n",
    "grads = recorded_data['grads']\n",
    "logits_vec = recorded_data['logits']\n",
    "losses = recorded_data['losses']\n",
    "momentum = recorded_data['momentum']\n",
    "variance = recorded_data['variance']\n",
    "\n",
    "print(f\"Embedding evolution:\")\n",
    "print(f\"  Step 0 centroid norm: {embeddings[0].float().mean(dim=0).norm().item():.6f}\")\n",
    "print(f\"  Step {NUM_TRAIN_STEPS} centroid norm: {embeddings[-1].float().mean(dim=0).norm().item():.6f}\")\n",
    "\n",
    "print(f\"\\nPairwise distances:\")\n",
    "step0_dists = torch.cdist(embeddings[0].float(), embeddings[0].float())\n",
    "stepN_dists = torch.cdist(embeddings[-1].float(), embeddings[-1].float())\n",
    "print(f\"  Step 0 max: {step0_dists.max().item():.6f} (should be ~0 for σ=0)\")\n",
    "print(f\"  Step {NUM_TRAIN_STEPS} max: {stepN_dists.max().item():.4f}\")\n",
    "\n",
    "print(f\"\\nGradient magnitudes (first step):\")\n",
    "grad_norms_step1 = torch.norm(grads[1].float(), p=2, dim=1)\n",
    "print(f\"  Mean: {grad_norms_step1.mean().item():.6e}\")\n",
    "print(f\"  Max: {grad_norms_step1.max().item():.6e}\")\n",
    "\n",
    "print(f\"\\nAdam state (after warmup):\")\n",
    "print(f\"  Momentum at step 100: {momentum[100].float().abs().mean().item():.6e}\")\n",
    "print(f\"  Variance at step 100: {variance[100].float().abs().mean().item():.6e}\")\n",
    "\n",
    "print(f\"\\nLoss trajectory:\")\n",
    "print(f\"  Step 1: {losses[1].float().item():.4f}\")\n",
    "print(f\"  Step {NUM_TRAIN_STEPS}: {losses[-1].float().item():.4f}\")\n",
    "print(f\"  Reduction: {(losses[1].float() - losses[-1].float()).item():.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Model: 2L, 2H, 64D (116,480 params)\n",
      "Training: 10,000 steps, batch 1\n",
      "Initialization: Supermassive BH (σ = 0e+00, bfloat16-native)\n",
      "Final loss: 2.9062\n",
      "\n",
      "Data saved: ../tensors/Lil_Gatsby/1.12b_training_data_sigma0e+00.safetensors\n",
      "Size: 658.1 MB\n",
      "\n",
      "Next steps:\n",
      "  1. Extract W[0], W[1], W[2], W[10000] from saved data\n",
      "  2. Run 1.13a on each to check for lattice structure\n",
      "  3. Look for spongecrystal formation at different timesteps\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Model: {N_LAYER}L, {N_HEAD}H, {HIDDEN_DIM}D ({total_params:,} params)\")\n",
    "print(f\"Training: {NUM_TRAIN_STEPS:,} steps, batch {BATCH_SIZE}\")\n",
    "print(f\"Initialization: Supermassive BH (σ = {SIGMA:.0e}, bfloat16-native)\")\n",
    "print(f\"Final loss: {losses[-1].float().item():.4f}\")\n",
    "print()\n",
    "print(f\"Data saved: {output_path}\")\n",
    "print(f\"Size: {file_size_mb:.1f} MB\")\n",
    "print()\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  1. Extract W[0], W[1], W[2], W[10000] from saved data\")\n",
    "print(f\"  2. Run 1.13a on each to check for lattice structure\")\n",
    "print(f\"  3. Look for spongecrystal formation at different timesteps\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
