{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.12a: Train Lil Gatsby from Default Initialization (CONTROL)\n",
    "\n",
    "**Goal:** Train Lil Gatsby using **standard GPT-2 initialization** (normal distribution) as the control experiment.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "This is our **control run**—no special initialization, just the standard `torch.nn.init.normal_()` that GPT-2 uses.\n",
    "\n",
    "We expect:\n",
    "1. **No lattice structure** formation (tokens start spread out)\n",
    "2. **No spongecrystal** (untrained tokens don't cluster)\n",
    "3. **Random hypersphere** distribution at equilibrium\n",
    "\n",
    "This provides the baseline to compare against:\n",
    "- 1.12b: Supermassive black hole initialization (all tokens at same point)\n",
    "\n",
    "## Initialization Strategy (bfloat16-native)\n",
    "\n",
    "**Standard GPT-2 initialization** (pure bfloat16 throughout):\n",
    "\n",
    "- Each embedding initialized independently: `N(0, 0.02)`  \n",
    "- Generated in float32, immediately converted to bfloat16\n",
    "- No special structure or correlation\n",
    "\n",
    "**No float32 intermediate step** - pure bfloat16 from generation to training.\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "- **Vocabulary:** 128 tokens (ASCII)\n",
    "- **Context window:** 128 tokens\n",
    "- **Hidden dimensions:** 64\n",
    "- **Layers:** 2\n",
    "- **Attention heads:** 2\n",
    "- **Weight tying:** E = W^T (matches Qwen 3 4B)\n",
    "- **Total parameters:** ~117k\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "**Identical to 1.12b except initialization:**\n",
    "\n",
    "- **Steps:** 10,000\n",
    "- **Batch size:** 1 (with gradient accumulation if needed)\n",
    "- **Optimizer:** Adam (lr=0.001, β₁=0.9, β₂=0.999)\n",
    "- **Weight decay:** 0.0 (no regularization)\n",
    "- **Precision:** Native bfloat16\n",
    "\n",
    "## Data Recording (Every Step)\n",
    "\n",
    "- **Embeddings:** (10,001, 128, 64) bfloat16\n",
    "- **Gradients:** (10,001, 128, 64) bfloat16  \n",
    "- **Adam momentum:** (10,001, 128, 64) bfloat16\n",
    "- **Adam variance:** (10,001, 128, 64) bfloat16\n",
    "- **Logits:** (10,001, 128) bfloat16\n",
    "- **Loss:** (10,001,) bfloat16\n",
    "\n",
    "Total: ~660 MB saved as safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "VOCAB_SIZE = 128\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYER = 2\n",
    "N_HEAD = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION = 1\n",
    "NUM_TRAIN_STEPS = 50000\n",
    "LEARNING_RATE = 1e-3  # 0.001 for Adam\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# Optimizer: Adam\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "# Initialization (STANDARD GPT-2 - bfloat16 native)\n",
    "INIT_SCALE = 0.02  # GPT-2 default initialization scale\n",
    "\n",
    "# Data\n",
    "CORPUS_PATH = \"../data/fineweb_ascii.txt\"\n",
    "OUTPUT_DIR = \"../tensors/Lil_Gatsby\"\n",
    "OUTPUT_FILE = \"1.12a_training_data_default_init_50000.safetensors\"\n",
    "\n",
    "# Instrumentation\n",
    "RECORD_EVERY_N_STEPS = 1  # Record every step\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "import time\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus: ../data/fineweb_ascii.txt\n",
      "  Total bytes: 2,000,000\n",
      "✓ Corpus on device\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "\n",
    "with open(CORPUS_PATH, 'r', encoding='ascii') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "# Convert to byte array (only keep bytes < VOCAB_SIZE)\n",
    "corpus_bytes = [b for b in corpus_text.encode('ascii') if b < VOCAB_SIZE]\n",
    "\n",
    "print(f\"  Total bytes: {len(corpus_bytes):,}\")\n",
    "\n",
    "# Pre-load corpus to device\n",
    "corpus_tensor = torch.tensor(corpus_bytes, dtype=torch.long, device=device)\n",
    "print(f\"✓ Corpus on device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset: 1,999,872 examples\n"
     ]
    }
   ],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = ByteDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "print(f\"✓ Dataset: {len(dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model created: 116,480 parameters\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✓ Model created: {total_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bfloat16-Native Initialization (STANDARD GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INITIALIZING WITH STANDARD GPT-2 INIT (bfloat16-native)\n",
      "================================================================================\n",
      "\n",
      "Initialization: N(0, 0.02)\n",
      "  Shape: torch.Size([128, 64])\n",
      "  Each token initialized independently\n",
      "\n",
      "✓ Initialized embeddings (pure bfloat16)\n",
      "  Shape: torch.Size([128, 64])\n",
      "  Dtype: torch.bfloat16\n",
      "\n",
      "Initial embedding statistics:\n",
      "  Centroid norm: 0.015438\n",
      "  Mean radius from centroid: 0.158978\n",
      "  Max radius from centroid: 0.196582\n",
      "  Max pairwise distance: 0.298239\n",
      "  Mean pairwise distance: 0.225726\n",
      "\n",
      "  ✓ Tokens distributed in hypersphere (as expected for default init)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"INITIALIZING WITH STANDARD GPT-2 INIT (bfloat16-native)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Standard GPT-2 initialization: each embedding drawn independently from N(0, 0.02)\n",
    "# Generate in float32, immediately convert to bfloat16\n",
    "init_f32 = torch.randn(VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32, device=device) * INIT_SCALE\n",
    "init_bf16 = init_f32.to(torch.bfloat16)\n",
    "\n",
    "print(f\"Initialization: N(0, {INIT_SCALE})\")\n",
    "print(f\"  Shape: {init_bf16.shape}\")\n",
    "print(f\"  Each token initialized independently\")\n",
    "print()\n",
    "\n",
    "# Assign to model\n",
    "with torch.no_grad():\n",
    "    model.transformer.wte.weight[:] = init_bf16\n",
    "\n",
    "print(f\"✓ Initialized embeddings (pure bfloat16)\")\n",
    "print(f\"  Shape: {model.transformer.wte.weight.shape}\")\n",
    "print(f\"  Dtype: {model.transformer.wte.weight.dtype}\")\n",
    "print()\n",
    "\n",
    "# Verify initialization\n",
    "W_check = model.transformer.wte.weight.cpu().float()\n",
    "pairwise_dists = torch.cdist(W_check, W_check)\n",
    "max_dist = pairwise_dists.max().item()\n",
    "mean_dist = pairwise_dists[torch.triu(torch.ones_like(pairwise_dists), diagonal=1) == 1].mean().item()\n",
    "centroid = W_check.mean(dim=0)\n",
    "centroid_norm = torch.norm(centroid).item()\n",
    "radii = torch.norm(W_check - centroid, dim=1)\n",
    "mean_radius = radii.mean().item()\n",
    "max_radius = radii.max().item()\n",
    "\n",
    "print(f\"Initial embedding statistics:\")\n",
    "print(f\"  Centroid norm: {centroid_norm:.6f}\")\n",
    "print(f\"  Mean radius from centroid: {mean_radius:.6f}\")\n",
    "print(f\"  Max radius from centroid: {max_radius:.6f}\")\n",
    "print(f\"  Max pairwise distance: {max_dist:.6f}\")\n",
    "print(f\"  Mean pairwise distance: {mean_dist:.6f}\")\n",
    "print()\n",
    "print(f\"  ✓ Tokens distributed in hypersphere (as expected for default init)\")\n",
    "\n",
    "print()\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Recorder\n",
    "\n",
    "Records everything at every step, holds in RAM until training completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Recorder class defined\n"
     ]
    }
   ],
   "source": [
    "class ComprehensiveRecorder:\n",
    "    \"\"\"Records embeddings, gradients, optimizer state, logits, loss at every step in bfloat16.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim, record_every_n):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.record_every_n = record_every_n\n",
    "        \n",
    "        # Storage (lists of tensors, keep in RAM)\n",
    "        self.recorded_steps = []\n",
    "        self.embeddings = []      # [n_recorded, vocab_size, hidden_dim]\n",
    "        self.grads = []           # [n_recorded, vocab_size, hidden_dim]\n",
    "        self.momentum = []        # [n_recorded, vocab_size, hidden_dim]\n",
    "        self.variance = []        # [n_recorded, vocab_size, hidden_dim]\n",
    "        self.logits = []          # [n_recorded, vocab_size]\n",
    "        self.losses = []          # [n_recorded]\n",
    "        \n",
    "        # Temporary storage\n",
    "        self.current_step = 0\n",
    "        self.recorded_initial = False\n",
    "        self.grad_before = None\n",
    "        self.loss_value = None\n",
    "        self.logits_sample = None\n",
    "    \n",
    "    def record_initial_state(self, model, optimizer):\n",
    "        \"\"\"Record step 0: initial state before training.\"\"\"\n",
    "        if not self.recorded_initial:\n",
    "            W = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "            \n",
    "            # Step 0: no gradients, no optimizer state yet (zeros)\n",
    "            self.recorded_steps.append(0)\n",
    "            self.embeddings.append(W)\n",
    "            self.grads.append(torch.zeros_like(W))\n",
    "            self.momentum.append(torch.zeros_like(W))\n",
    "            self.variance.append(torch.zeros_like(W))\n",
    "            self.logits.append(torch.zeros(self.vocab_size, dtype=torch.bfloat16))\n",
    "            self.losses.append(torch.tensor(float('nan'), dtype=torch.bfloat16))  # No loss yet\n",
    "            \n",
    "            self.recorded_initial = True\n",
    "            self.current_step = 1\n",
    "            \n",
    "            print(f\"✓ Recorded initial state (step 0)\")\n",
    "    \n",
    "    def record_before_step(self, model, loss, logits):\n",
    "        \"\"\"Call after forward/backward, before optimizer step.\"\"\"\n",
    "        if self.current_step % self.record_every_n == 0:\n",
    "            # Capture gradients in bfloat16\n",
    "            if model.transformer.wte.weight.grad is not None:\n",
    "                self.grad_before = model.transformer.wte.weight.grad.clone().cpu().bfloat16()\n",
    "            else:\n",
    "                self.grad_before = torch.zeros(self.vocab_size, self.hidden_dim, dtype=torch.bfloat16)\n",
    "            \n",
    "            # Capture loss\n",
    "            self.loss_value = loss.item()\n",
    "            \n",
    "            # Capture logits from first sequence, last position in bfloat16\n",
    "            self.logits_sample = logits[0, -1, :].detach().cpu().bfloat16()\n",
    "    \n",
    "    def record_after_step(self, model, optimizer):\n",
    "        \"\"\"Call after optimizer step.\"\"\"\n",
    "        if self.current_step % self.record_every_n == 0:\n",
    "            if self.grad_before is not None and self.loss_value is not None:\n",
    "                # Capture embeddings in bfloat16\n",
    "                W = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "\n",
    "                # Capture optimizer state (Adam momentum and variance)\n",
    "                param = model.transformer.wte.weight\n",
    "                if param in optimizer.state:\n",
    "                    state = optimizer.state[param]\n",
    "                    # Get state tensors if they exist, convert to bfloat16\n",
    "                    mom_src = state.get('exp_avg', None)\n",
    "                    var_src = state.get('exp_avg_sq', None)\n",
    "                    mom = mom_src.clone().cpu().bfloat16() if mom_src is not None else torch.zeros_like(W)\n",
    "                    var = var_src.clone().cpu().bfloat16() if var_src is not None else torch.zeros_like(W)\n",
    "                else:\n",
    "                    mom = torch.zeros_like(W)\n",
    "                    var = torch.zeros_like(W)\n",
    "\n",
    "                # Store everything\n",
    "                self.recorded_steps.append(self.current_step)\n",
    "                self.embeddings.append(W)\n",
    "                self.grads.append(self.grad_before)\n",
    "                self.momentum.append(mom)\n",
    "                self.variance.append(var)\n",
    "                self.logits.append(self.logits_sample)\n",
    "                self.losses.append(torch.tensor(self.loss_value, dtype=torch.bfloat16))\n",
    "\n",
    "                # Clear temp storage\n",
    "                self.grad_before = None\n",
    "                self.loss_value = None\n",
    "                self.logits_sample = None\n",
    "                \n",
    "                # Progress indicator every 1000 steps\n",
    "                if self.current_step % 1000 == 0:\n",
    "                    print(f\"  Recorded step {self.current_step:,}\")\n",
    "\n",
    "        self.current_step += 1\n",
    "    \n",
    "    def get_data(self):\n",
    "        \"\"\"Return recorded data as stacked tensors.\"\"\"\n",
    "        print(f\"\\nStacking {len(self.embeddings)} recorded states...\")\n",
    "        \n",
    "        return {\n",
    "            'recorded_steps': torch.tensor(self.recorded_steps, dtype=torch.long),\n",
    "            'embeddings': torch.stack(self.embeddings) if self.embeddings else torch.tensor([]),\n",
    "            'grads': torch.stack(self.grads) if self.grads else torch.tensor([]),\n",
    "            'momentum': torch.stack(self.momentum) if self.momentum else torch.tensor([]),\n",
    "            'variance': torch.stack(self.variance) if self.variance else torch.tensor([]),\n",
    "            'logits': torch.stack(self.logits) if self.logits else torch.tensor([]),\n",
    "            'losses': torch.stack(self.losses) if self.losses else torch.tensor([]),\n",
    "        }\n",
    "\n",
    "print(\"✓ Recorder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Trainer with Instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ InstrumentedTrainer defined\n"
     ]
    }
   ],
   "source": [
    "class InstrumentedTrainer(Trainer):\n",
    "    def __init__(self, recorder, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.recorder = recorder\n",
    "        self.last_logits = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"Override to capture logits.\"\"\"\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Store logits for recorder\n",
    "        self.last_logits = outputs.logits\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        \"\"\"Override to inject recording.\"\"\"\n",
    "        # Standard forward + backward\n",
    "        loss = super().training_step(model, inputs, num_items_in_batch)\n",
    "        \n",
    "        # Record BEFORE optimizer step\n",
    "        self.recorder.record_before_step(model, loss, self.last_logits)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time=None, **kwargs):\n",
    "        \"\"\"Override to record AFTER optimizer step.\"\"\"\n",
    "        # Record AFTER optimizer updates parameters\n",
    "        self.recorder.record_after_step(model, self.optimizer)\n",
    "        \n",
    "        # Call parent\n",
    "        super()._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, **kwargs)\n",
    "\n",
    "print(\"✓ InstrumentedTrainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer ready (Adam, bf16=True)\n"
     ]
    }
   ],
   "source": [
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "recorder = ComprehensiveRecorder(VOCAB_SIZE, HIDDEN_DIM, RECORD_EVERY_N_STEPS)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=NUM_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    adam_beta1=ADAM_BETA1,\n",
    "    adam_beta2=ADAM_BETA2,\n",
    "    adam_epsilon=ADAM_EPSILON,\n",
    "    optim=\"adamw_torch\",  # Adam optimizer\n",
    "    logging_steps=1000,\n",
    "    save_steps=NUM_TRAIN_STEPS + 1,  # Don't save checkpoints\n",
    "    save_total_limit=0,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    "    bf16=True,  # Native bfloat16 training\n",
    "    seed=RANDOM_SEED,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = InstrumentedTrainer(\n",
    "    recorder=recorder,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer ready (Adam, bf16=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Initial State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Recorded initial state (step 0)\n"
     ]
    }
   ],
   "source": [
    "recorder.record_initial_state(model, trainer.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "**This will take ~2 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING TRAINING RUN\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Initialization: STANDARD GPT-2 (N(0, 0.02), bfloat16-native)\n",
      "  Optimizer: Adam\n",
      "  Learning rate: 0.001\n",
      "  Adam beta1: 0.9\n",
      "  Adam beta2: 0.999\n",
      "  Weight decay: 0.0\n",
      "  Precision: bfloat16 (native)\n",
      "  Steps: 50,000 (plus initial step 0)\n",
      "  Recording every: 1 step(s)\n",
      "  Expected records: 50,001\n",
      "\n",
      "Recording:\n",
      "  - Embeddings (positions)\n",
      "  - Gradients (forces)\n",
      "  - Momentum (Adam first moment)\n",
      "  - Variance (Adam second moment)\n",
      "  - Logits (predictions)\n",
      "  - Loss\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50000' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50000/50000 07:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.957900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.945300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.943400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.938700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.906200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.897200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.867900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.862200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.854600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.846900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.831300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.831000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.816300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.809300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.795100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.788300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.777500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.772400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.771800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>2.765900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.771800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>2.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.765800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>2.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>2.758300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>2.754600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>2.764100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>2.748800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>2.747700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>2.741300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>2.741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>2.760700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>2.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>2.743200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>2.749800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>2.747900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>2.756200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>2.754200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>2.732600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>2.751300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>2.740800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Recorded step 1,000\n",
      "  Recorded step 2,000\n",
      "  Recorded step 3,000\n",
      "  Recorded step 4,000\n",
      "  Recorded step 5,000\n",
      "  Recorded step 6,000\n",
      "  Recorded step 7,000\n",
      "  Recorded step 8,000\n",
      "  Recorded step 9,000\n",
      "  Recorded step 10,000\n",
      "  Recorded step 11,000\n",
      "  Recorded step 12,000\n",
      "  Recorded step 13,000\n",
      "  Recorded step 14,000\n",
      "  Recorded step 15,000\n",
      "  Recorded step 16,000\n",
      "  Recorded step 17,000\n",
      "  Recorded step 18,000\n",
      "  Recorded step 19,000\n",
      "  Recorded step 20,000\n",
      "  Recorded step 21,000\n",
      "  Recorded step 22,000\n",
      "  Recorded step 23,000\n",
      "  Recorded step 24,000\n",
      "  Recorded step 25,000\n",
      "  Recorded step 26,000\n",
      "  Recorded step 27,000\n",
      "  Recorded step 28,000\n",
      "  Recorded step 29,000\n",
      "  Recorded step 30,000\n",
      "  Recorded step 31,000\n",
      "  Recorded step 32,000\n",
      "  Recorded step 33,000\n",
      "  Recorded step 34,000\n",
      "  Recorded step 35,000\n",
      "  Recorded step 36,000\n",
      "  Recorded step 37,000\n",
      "  Recorded step 38,000\n",
      "  Recorded step 39,000\n",
      "  Recorded step 40,000\n",
      "  Recorded step 41,000\n",
      "  Recorded step 42,000\n",
      "  Recorded step 43,000\n",
      "  Recorded step 44,000\n",
      "  Recorded step 45,000\n",
      "  Recorded step 46,000\n",
      "  Recorded step 47,000\n",
      "  Recorded step 48,000\n",
      "  Recorded step 49,000\n",
      "  Recorded step 50,000\n",
      "\n",
      "================================================================================\n",
      "✓ Training complete\n",
      "  Elapsed time: 7.8 minutes (470.6 seconds)\n",
      "  Throughput: 106.3 steps/second\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STARTING TRAINING RUN\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Initialization: STANDARD GPT-2 (N(0, {INIT_SCALE}), bfloat16-native)\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Adam beta1: {ADAM_BETA1}\")\n",
    "print(f\"  Adam beta2: {ADAM_BETA2}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Precision: bfloat16 (native)\")\n",
    "print(f\"  Steps: {NUM_TRAIN_STEPS:,} (plus initial step 0)\")\n",
    "print(f\"  Recording every: {RECORD_EVERY_N_STEPS} step(s)\")\n",
    "print(f\"  Expected records: {NUM_TRAIN_STEPS // RECORD_EVERY_N_STEPS + 1:,}\")\n",
    "print(f\"\\nRecording:\")\n",
    "print(f\"  - Embeddings (positions)\")\n",
    "print(f\"  - Gradients (forces)\")\n",
    "print(f\"  - Momentum (Adam first moment)\")\n",
    "print(f\"  - Variance (Adam second moment)\")\n",
    "print(f\"  - Logits (predictions)\")\n",
    "print(f\"  - Loss\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training complete\")\n",
    "print(f\"  Elapsed time: {elapsed/60:.1f} minutes ({elapsed:.1f} seconds)\")\n",
    "print(f\"  Throughput: {NUM_TRAIN_STEPS / elapsed:.1f} steps/second\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Recorded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data for save...\n",
      "\n",
      "\n",
      "Stacking 50001 recorded states...\n",
      "Saving to: ../tensors/Lil_Gatsby/1.12a_training_data_default_init_50000.safetensors\n",
      "This may take a minute...\n",
      "\n",
      "✓ Saved successfully\n",
      "  File: ../tensors/Lil_Gatsby/1.12a_training_data_default_init_50000.safetensors\n",
      "  Size: 3.29 GB (3290.2 MB)\n",
      "  Save time: 2.8 seconds\n",
      "  Recorded steps: 50,001\n",
      "  Step range: 0 to 50000\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nPreparing data for save...\\n\")\n",
    "\n",
    "recorded_data = recorder.get_data()\n",
    "\n",
    "save_dict = {\n",
    "    'recorded_steps': recorded_data['recorded_steps'],\n",
    "    'embeddings': recorded_data['embeddings'],\n",
    "    'grads': recorded_data['grads'],\n",
    "    'momentum': recorded_data['momentum'],\n",
    "    'variance': recorded_data['variance'],\n",
    "    'logits': recorded_data['logits'],\n",
    "    'losses': recorded_data['losses'],\n",
    "    'init_scale': torch.tensor(INIT_SCALE, dtype=torch.float32),\n",
    "    'learning_rate': torch.tensor(LEARNING_RATE, dtype=torch.float32),\n",
    "    'weight_decay': torch.tensor(WEIGHT_DECAY, dtype=torch.float32),\n",
    "    'adam_beta1': torch.tensor(ADAM_BETA1, dtype=torch.float32),\n",
    "    'adam_beta2': torch.tensor(ADAM_BETA2, dtype=torch.float32),\n",
    "}\n",
    "\n",
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILE\n",
    "\n",
    "print(f\"Saving to: {output_path}\")\n",
    "print(f\"This may take a minute...\\n\")\n",
    "\n",
    "save_start = time.time()\n",
    "save_file(save_dict, str(output_path))\n",
    "save_elapsed = time.time() - save_start\n",
    "\n",
    "file_size_mb = output_path.stat().st_size / 1e6\n",
    "file_size_gb = file_size_mb / 1000\n",
    "\n",
    "print(f\"✓ Saved successfully\")\n",
    "print(f\"  File: {output_path}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB ({file_size_mb:.1f} MB)\")\n",
    "print(f\"  Save time: {save_elapsed:.1f} seconds\")\n",
    "print(f\"  Recorded steps: {len(recorded_data['recorded_steps']):,}\")\n",
    "print(f\"  Step range: {recorded_data['recorded_steps'][0]} to {recorded_data['recorded_steps'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data shapes:\n",
      "  embeddings: torch.Size([50001, 128, 64])\n",
      "  grads: torch.Size([50001, 128, 64])\n",
      "  momentum: torch.Size([50001, 128, 64])\n",
      "  variance: torch.Size([50001, 128, 64])\n",
      "  logits: torch.Size([50001, 128])\n",
      "  losses: torch.Size([50001])\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  embeddings: {recorded_data['embeddings'].shape}\")\n",
    "print(f\"  grads: {recorded_data['grads'].shape}\")\n",
    "print(f\"  momentum: {recorded_data['momentum'].shape}\")\n",
    "print(f\"  variance: {recorded_data['variance'].shape}\")\n",
    "print(f\"  logits: {recorded_data['logits'].shape}\")\n",
    "print(f\"  losses: {recorded_data['losses'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Embedding evolution:\n",
      "  Step 0 centroid norm: 0.015438\n",
      "  Step 50000 centroid norm: 0.120630\n",
      "\n",
      "Pairwise distances:\n",
      "  Step 0 max: 0.2982\n",
      "  Step 50000 max: 1.4692\n",
      "\n",
      "Gradient magnitudes (first step):\n",
      "  Mean: 5.657926e-02\n",
      "  Max: 5.724860e-01\n",
      "\n",
      "Adam state (after warmup):\n",
      "  Momentum at step 100: 1.136351e-03\n",
      "  Variance at step 100: 7.653446e-06\n",
      "\n",
      "Loss trajectory:\n",
      "  Step 1: 4.8750\n",
      "  Step 50000: 2.9688\n",
      "  Reduction: 1.9062\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"VERIFICATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "embeddings = recorded_data['embeddings']\n",
    "grads = recorded_data['grads']\n",
    "logits_vec = recorded_data['logits']\n",
    "losses = recorded_data['losses']\n",
    "momentum = recorded_data['momentum']\n",
    "variance = recorded_data['variance']\n",
    "\n",
    "print(f\"Embedding evolution:\")\n",
    "print(f\"  Step 0 centroid norm: {embeddings[0].float().mean(dim=0).norm().item():.6f}\")\n",
    "print(f\"  Step {NUM_TRAIN_STEPS} centroid norm: {embeddings[-1].float().mean(dim=0).norm().item():.6f}\")\n",
    "\n",
    "print(f\"\\nPairwise distances:\")\n",
    "step0_dists = torch.cdist(embeddings[0].float(), embeddings[0].float())\n",
    "stepN_dists = torch.cdist(embeddings[-1].float(), embeddings[-1].float())\n",
    "print(f\"  Step 0 max: {step0_dists.max().item():.4f}\")\n",
    "print(f\"  Step {NUM_TRAIN_STEPS} max: {stepN_dists.max().item():.4f}\")\n",
    "\n",
    "print(f\"\\nGradient magnitudes (first step):\")\n",
    "grad_norms_step1 = torch.norm(grads[1].float(), p=2, dim=1)\n",
    "print(f\"  Mean: {grad_norms_step1.mean().item():.6e}\")\n",
    "print(f\"  Max: {grad_norms_step1.max().item():.6e}\")\n",
    "\n",
    "print(f\"\\nAdam state (after warmup):\")\n",
    "print(f\"  Momentum at step 100: {momentum[100].float().abs().mean().item():.6e}\")\n",
    "print(f\"  Variance at step 100: {variance[100].float().abs().mean().item():.6e}\")\n",
    "\n",
    "print(f\"\\nLoss trajectory:\")\n",
    "print(f\"  Step 1: {losses[1].float().item():.4f}\")\n",
    "print(f\"  Step {NUM_TRAIN_STEPS}: {losses[-1].float().item():.4f}\")\n",
    "print(f\"  Reduction: {(losses[1].float() - losses[-1].float()).item():.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY (CONTROL RUN)\n",
      "================================================================================\n",
      "\n",
      "Model: 2L, 2H, 64D (116,480 params)\n",
      "Training: 50,000 steps, batch 1\n",
      "Initialization: STANDARD GPT-2 (N(0, 0.02), bfloat16-native)\n",
      "Final loss: 2.9688\n",
      "\n",
      "Data saved: ../tensors/Lil_Gatsby/1.12a_training_data_default_init_50000.safetensors\n",
      "Size: 3290.2 MB\n",
      "\n",
      "Next steps:\n",
      "  1. Run 1.13a on final state to check for lattice structure (expect null)\n",
      "  2. Run 1.13d to analyze bounding hypersphere and flux dynamics\n",
      "  3. Look for precursors to structure formation\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY (CONTROL RUN)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Model: {N_LAYER}L, {N_HEAD}H, {HIDDEN_DIM}D ({total_params:,} params)\")\n",
    "print(f\"Training: {NUM_TRAIN_STEPS:,} steps, batch {BATCH_SIZE}\")\n",
    "print(f\"Initialization: STANDARD GPT-2 (N(0, {INIT_SCALE}), bfloat16-native)\")\n",
    "print(f\"Final loss: {losses[-1].float().item():.4f}\")\n",
    "print()\n",
    "print(f\"Data saved: {output_path}\")\n",
    "print(f\"Size: {file_size_mb:.1f} MB\")\n",
    "print()\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  1. Run 1.13a on final state to check for lattice structure (expect null)\")\n",
    "print(f\"  2. Run 1.13d to analyze bounding hypersphere and flux dynamics\")\n",
    "print(f\"  3. Look for precursors to structure formation\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
