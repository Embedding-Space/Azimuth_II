{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.12a: Train Lil Gatsby\n",
    "\n",
    "**Goal:** Train a tiny GPT-2 style model on The Great Gatsby and record everything.\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "- **Vocabulary:** 128 tokens (ASCII)\n",
    "- **Context window:** 128 tokens\n",
    "- **Hidden dimensions:** 64\n",
    "- **Layers:** 2\n",
    "- **Attention heads:** 2\n",
    "- **Weight tying:** E = W^T (matches Qwen 3 4B)\n",
    "- **Total parameters:** ~49k\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "- **Steps:** 10,000\n",
    "- **Batch size:** 32\n",
    "- **Optimizer:** Adam (lr=0.001)\n",
    "- **Precision:** bfloat16 for model, float32 for optimizer\n",
    "\n",
    "## Data Recording (Every Step)\n",
    "\n",
    "Pure PyTorch tensors, no HDF5:\n",
    "- **W (unembedding):** (10001, 128, 64) bf16\n",
    "- **Gradients:** (10001, 128, 64) bf16\n",
    "- **Adam momentum:** (10001, 128, 64) f32\n",
    "- **Adam variance:** (10001, 128, 64) f32\n",
    "- **Logits (mean):** (10001, 128) f32\n",
    "- **Loss:** (10001,) f32\n",
    "\n",
    "Total: ~650 MB saved as safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "VOCAB_SIZE = 128\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYERS = 2\n",
    "N_HEADS = 2\n",
    "CONTEXT_LENGTH = 128\n",
    "\n",
    "# Training\n",
    "N_STEPS = 10000\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Data paths\n",
    "CORPUS_PATH = \"../data/gatsby_clean.txt\"\n",
    "TOKENIZER_PATH = \"../data/tokenizers/ascii_128\"\n",
    "OUTPUT_PATH = \"../tensors/Lil_Gatsby/1.12a_training_data.safetensors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedTokenizerFast, GPT2Config, GPT2LMHeadModel\n",
    "from safetensors.torch import save_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded corpus\n",
      "  Characters: 266,252\n",
      "  Tokens: 266,252\n",
      "  Vocab size: 128\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_PATH)\n",
    "\n",
    "# Load and tokenize corpus\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "tokens = tokenizer.encode(corpus_text)\n",
    "\n",
    "print(f\"✓ Loaded corpus\")\n",
    "print(f\"  Characters: {len(corpus_text):,}\")\n",
    "print(f\"  Tokens: {len(tokens):,}\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created dataset: 266,124 examples\n"
     ]
    }
   ],
   "source": [
    "class GatsbyDataset(Dataset):\n",
    "    def __init__(self, tokens, context_length):\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        self.context_length = context_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.context_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.tokens[idx:idx + self.context_length]\n",
    "        y = self.tokens[idx + 1:idx + self.context_length + 1]\n",
    "        return x, y\n",
    "\n",
    "dataset = GatsbyDataset(tokens, CONTEXT_LENGTH)\n",
    "\n",
    "# Don't create DataLoader yet - wait until after we're done with tokenizer\n",
    "print(f\"✓ Created dataset: {len(dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created model: 116,480 parameters\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=CONTEXT_LENGTH,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYERS,\n",
    "    n_head=N_HEADS,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    use_cache=False,\n",
    "    tie_word_embeddings=True,  # E = W^T\n",
    "    # Explicitly set loss_type to avoid warning\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config).to(device).to(torch.bfloat16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✓ Created model: {n_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Storage Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Allocated storage tensors on mps\n",
      "  Total memory: ~943 MB\n",
      "✓ Created DataLoader\n"
     ]
    }
   ],
   "source": [
    "# Pre-allocate tensors for all steps ON GPU (or device)\n",
    "n_steps_total = N_STEPS + 1\n",
    "\n",
    "# Store on device to avoid constant GPU→CPU transfers\n",
    "W_history = torch.zeros(n_steps_total, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16, device=device)\n",
    "grads_history = torch.zeros(n_steps_total, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16, device=device)\n",
    "momentum_history = torch.zeros(n_steps_total, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32, device=device)\n",
    "variance_history = torch.zeros(n_steps_total, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32, device=device)\n",
    "logits_history = torch.zeros(n_steps_total, VOCAB_SIZE, dtype=torch.float32, device=device)\n",
    "loss_history = torch.zeros(n_steps_total, dtype=torch.float32, device=device)\n",
    "\n",
    "print(f\"✓ Allocated storage tensors on {device}\")\n",
    "print(f\"  Total memory: ~{(W_history.nbytes + grads_history.nbytes + momentum_history.nbytes + variance_history.nbytes + logits_history.nbytes + loss_history.nbytes) / 1024**2:.0f} MB\")\n",
    "\n",
    "# NOW create DataLoader (after all tokenizer usage is done)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    generator=torch.Generator().manual_seed(RANDOM_SEED)\n",
    ")\n",
    "\n",
    "print(f\"✓ Created DataLoader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "\n",
      "✓ Recorded step 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10000/10000 [01:31<00:00, 108.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training complete! Final loss: 1.9609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Record step 0 (initialization)\n",
    "with torch.no_grad():\n",
    "    W_history[0] = model.lm_head.weight  # No .cpu() - keep on device\n",
    "    # Gradients, momentum, variance are zero at step 0\n",
    "    # Get dummy logits\n",
    "    dummy_x, _ = next(iter(dataloader))\n",
    "    dummy_out = model(dummy_x.to(device))\n",
    "    logits_history[0] = dummy_out.logits[:, -1, :].mean(dim=0).float()\n",
    "\n",
    "print(\"✓ Recorded step 0\\n\")\n",
    "\n",
    "# Training loop\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "for step in tqdm(range(1, N_STEPS + 1), desc=\"Training\"):\n",
    "    # Get batch\n",
    "    try:\n",
    "        x, y = next(data_iter)\n",
    "    except StopIteration:\n",
    "        data_iter = iter(dataloader)\n",
    "        x, y = next(data_iter)\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x, labels=y)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Record data BEFORE optimizer step (all on device, no transfers!)\n",
    "    with torch.no_grad():\n",
    "        # W matrix\n",
    "        W_history[step] = model.lm_head.weight\n",
    "        \n",
    "        # Gradients\n",
    "        if model.lm_head.weight.grad is not None:\n",
    "            grads_history[step] = model.lm_head.weight.grad\n",
    "        \n",
    "        # Adam state (if it exists)\n",
    "        if model.lm_head.weight in optimizer.state:\n",
    "            state = optimizer.state[model.lm_head.weight]\n",
    "            if 'exp_avg' in state:\n",
    "                momentum_history[step] = state['exp_avg'].float()\n",
    "            if 'exp_avg_sq' in state:\n",
    "                variance_history[step] = state['exp_avg_sq'].float()\n",
    "        \n",
    "        # Mean logits\n",
    "        logits_history[step] = outputs.logits[:, -1, :].mean(dim=0).float()\n",
    "        \n",
    "        # Loss\n",
    "        loss_history[step] = loss.float()\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"\\n✓ Training complete! Final loss: {loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving data to CPU for saving...\n",
      "✓ Saved training data: ../tensors/Lil_Gatsby/1.12a_training_data.safetensors\n",
      "  File size: 942.5 MB\n"
     ]
    }
   ],
   "source": [
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Move everything to CPU for saving\n",
    "print(\"Moving data to CPU for saving...\")\n",
    "save_file(\n",
    "    {\n",
    "        'W': W_history.cpu(),\n",
    "        'grads': grads_history.cpu(),\n",
    "        'momentum': momentum_history.cpu(),\n",
    "        'variance': variance_history.cpu(),\n",
    "        'logits': logits_history.cpu(),\n",
    "        'loss': loss_history.cpu()\n",
    "    },\n",
    "    OUTPUT_PATH,\n",
    "    metadata={\n",
    "        'model': 'GPT2',\n",
    "        'vocab_size': str(VOCAB_SIZE),\n",
    "        'hidden_dim': str(HIDDEN_DIM),\n",
    "        'n_layers': str(N_LAYERS),\n",
    "        'n_heads': str(N_HEADS),\n",
    "        'n_steps': str(N_STEPS),\n",
    "        'batch_size': str(BATCH_SIZE),\n",
    "        'learning_rate': str(LEARNING_RATE),\n",
    "        'final_loss': str(loss_history[-1].item())\n",
    "    }\n",
    ")\n",
    "\n",
    "file_size_mb = Path(OUTPUT_PATH).stat().st_size / 1024**2\n",
    "print(f\"✓ Saved training data: {OUTPUT_PATH}\")\n",
    "print(f\"  File size: {file_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INFERENCE TEST\n",
      "================================================================================\n",
      "\n",
      "Prompt: 'In my younger'\n",
      "Output: 'I n   m y   y o u n g e r   a o   i   o   u l n t r   f o   r n a e <0x00> h g g t r n s a   r s r r s r'\n",
      "\n",
      "Prompt: 'Gatsby'\n",
      "Output: 'G a t s b y a l s y   n t o t i e , w r n n n l . <0x00> I h k c   t e l e h t e   h m a e   e o R s a i'\n",
      "\n",
      "Prompt: 'The green light'\n",
      "Output: 'T h e   g r e e n   l i g h t   t m e   t k t t o s   r y r b   h a r   i   o   o   s o n t d s o e'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INFERENCE TEST\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompts = [\n",
    "    \"In my younger\",\n",
    "    \"Gatsby\",\n",
    "    \"The green light\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    input_ids = torch.tensor([tokenizer.encode(prompt)]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Output: '{tokenizer.decode(output[0])}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Model: 2L, 2H, 64D (116,480 params)\n",
      "Training: 10,000 steps, batch 32\n",
      "Final loss: 1.9609\n",
      "\n",
      "Data saved: ../tensors/Lil_Gatsby/1.12a_training_data.safetensors\n",
      "Size: 942.5 MB\n",
      "\n",
      "Next: 1.13x - Analyze dead token dynamics\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"Model: {N_LAYERS}L, {N_HEADS}H, {HIDDEN_DIM}D ({n_params:,} params)\")\n",
    "print(f\"Training: {N_STEPS:,} steps, batch {BATCH_SIZE}\")\n",
    "print(f\"Final loss: {loss_history[-1]:.4f}\")\n",
    "print()\n",
    "print(f\"Data saved: {OUTPUT_PATH}\")\n",
    "print(f\"Size: {file_size_mb:.1f} MB\")\n",
    "print()\n",
    "print(f\"Next: 1.13x - Analyze dead token dynamics\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
