{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.13a: Lattice Structure Search\n",
    "\n",
    "**Goal:** Find black holes and orthogonally-adjacent lattice neighbors.\n",
    "\n",
    "## Method\n",
    "\n",
    "### Stage 1: Black Hole Detection\n",
    "Use `torch.unique()` to find duplicate vectors.\n",
    "\n",
    "### Stage 2: Deduplication\n",
    "Keep one representative per black hole centroid to reduce search space.\n",
    "\n",
    "### Stage 3: Isolated Token Exclusion (ε-sphere filter)\n",
    "For each token, find neighbors within ε = ULP × √D (worst-case diagonal distance).\n",
    "Exclude tokens with no neighbors.\n",
    "\n",
    "### Stage 4: Orthogonal Neighbor Detection\n",
    "For candidate pairs within ε:\n",
    "1. **Geometric filter**: L∞ = L1 (exactly one dimension differs)\n",
    "2. **ULP distance check**: Verify distance ≈ 1 ULP\n",
    "3. **Bit-level verification**: Same sign, same exponent, mantissa differs by 1\n",
    "\n",
    "## Design Goals\n",
    "\n",
    "- **Memory-efficient**: No massive allocations, batch processing\n",
    "- **Hardware accelerated**: Explicit device management for GPU/MPS\n",
    "- **Scalable**: Works for Qwen (151k tokens × 2560D)\n",
    "- **Correct**: Finds nothing in random Gaussian, detects spongecrystal in Qwen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor to analyze\n",
    "# TENSOR_FILE = \"../tensors/Qwen3-4B-Instruct-2507/W.safetensors\"\n",
    "# TENSOR_FILE = \"../tensors/Qwen2.5-3B-Instruct/W.safetensors\"\n",
    "# TENSOR_FILE = \"../tensors/Lil_Gatsby/1.12a_training_data.safetensors\"\n",
    "TENSOR_FILE = \"../tensors/Wordybird/1.12c_wordybird_1.safetensors\"\n",
    "TENSOR_KEY = \"embeddings\"\n",
    "TENSOR_INDEX = -1 # None = load full tensor\n",
    "\n",
    "# Stage 3 parameters\n",
    "BATCH_SIZE = 100  # For distance computations (100 × 150k × 4 bytes ≈ 60 MB per batch)\n",
    "\n",
    "# Stage 4 parameters\n",
    "ULP_TOLERANCE = 0.01  # Allow 1% tolerance when checking ULP distances\n",
    "MAX_EXAMPLES = 10  # How many example tokens to show in output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import ml_dtypes\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Detect available device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded W from 1.12c_wordybird_1.safetensors\n",
      "  Shape: torch.Size([50257, 64])\n",
      "  Dtype: torch.bfloat16\n",
      "  Device: mps:0\n",
      "  Memory: ~0.01 GB\n",
      "\n",
      "Analyzing 50,257 vectors in 64 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Load tensor (safetensors loads to CPU by default)\n",
    "data = load_file(TENSOR_FILE)\n",
    "W = data[TENSOR_KEY]\n",
    "\n",
    "# Apply indexing if specified\n",
    "if TENSOR_INDEX is not None:\n",
    "    W = W[TENSOR_INDEX]\n",
    "\n",
    "# Move to device for hardware acceleration\n",
    "W = W.to(device)\n",
    "\n",
    "n_vectors, n_dims = W.shape\n",
    "\n",
    "print(f\"✓ Loaded W from {Path(TENSOR_FILE).name}\")\n",
    "print(f\"  Shape: {W.shape}\")\n",
    "print(f\"  Dtype: {W.dtype}\")\n",
    "print(f\"  Device: {W.device}\")\n",
    "print(f\"  Memory: ~{W.element_size() * W.numel() / 1024**3:.2f} GB\")\n",
    "print()\n",
    "print(f\"Analyzing {n_vectors:,} vectors in {n_dims:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def get_max_exponent(vector_bf16):\n",
    "    \"\"\"\n",
    "    Get the maximum exponent across all dimensions of a bfloat16 vector.\n",
    "    \n",
    "    Args:\n",
    "        vector_bf16: (D,) tensor of bfloat16 values (on any device)\n",
    "    \n",
    "    Returns:\n",
    "        int: maximum exponent value\n",
    "    \"\"\"\n",
    "    values_uint16 = vector_bf16.view(torch.int16).to(torch.int64) & 0xFFFF\n",
    "    exponents = (values_uint16 >> 7) & 0xFF\n",
    "    return exponents.max().item()\n",
    "\n",
    "def decode_bfloat16_bits(value_bf16):\n",
    "    \"\"\"\n",
    "    Decode a single bfloat16 value into its bit components.\n",
    "    \n",
    "    Args:\n",
    "        value_bf16: scalar torch.bfloat16 value (on any device)\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'sign', 'exponent', 'mantissa' as integers\n",
    "    \"\"\"\n",
    "    # Convert to bytes and interpret as uint16\n",
    "    bits_uint16 = value_bf16.view(torch.int16).cpu().numpy().astype(np.uint16).item()\n",
    "    bits_binary = format(bits_uint16, '016b')\n",
    "    \n",
    "    sign_bit = bits_binary[0]\n",
    "    exponent_bits = bits_binary[1:9]\n",
    "    mantissa_bits = bits_binary[9:16]\n",
    "    \n",
    "    sign = int(sign_bit)\n",
    "    exponent = int(exponent_bits, 2)\n",
    "    mantissa = int(mantissa_bits, 2)\n",
    "    \n",
    "    return {\n",
    "        'bits_uint16': bits_uint16,\n",
    "        'bits_binary': bits_binary,\n",
    "        'sign': sign,\n",
    "        'exponent': exponent,\n",
    "        'mantissa': mantissa,\n",
    "        'sign_bit': sign_bit,\n",
    "        'exponent_bits': exponent_bits,\n",
    "        'mantissa_bits': mantissa_bits\n",
    "    }\n",
    "\n",
    "def compute_ulp_at_exponent(exponent):\n",
    "    \"\"\"\n",
    "    Compute ULP (unit in last place) for bfloat16 at given exponent.\n",
    "    \n",
    "    bfloat16 has 7 mantissa bits, so ULP = 2^(exponent - 127 - 7) = 2^(exponent - 134)\n",
    "    \"\"\"\n",
    "    return 2.0 ** (exponent - 134)\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Black Hole Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 1: BLACK HOLE DETECTION\n",
      "================================================================================\n",
      "\n",
      "Finding unique vectors...\n",
      "  ✓ Found 50,257 unique vectors\n",
      "  ✓ 0 vectors are duplicates\n",
      "\n",
      "Black hole tokens: 0 (0.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STAGE 1: BLACK HOLE DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# torch.unique not implemented on MPS in Torch 2.8, use CPU\n",
    "print(\"Finding unique vectors...\")\n",
    "W_cpu = W.cpu()\n",
    "W_unique, inverse_indices, counts = torch.unique(W_cpu, dim=0, return_inverse=True, return_counts=True)\n",
    "\n",
    "n_unique = len(W_unique)\n",
    "n_duplicates = n_vectors - n_unique\n",
    "\n",
    "print(f\"  ✓ Found {n_unique:,} unique vectors\")\n",
    "print(f\"  ✓ {n_duplicates:,} vectors are duplicates\")\n",
    "print()\n",
    "\n",
    "# Count tokens participating in black holes\n",
    "duplicate_mask = counts > 1\n",
    "n_black_hole_centroids = duplicate_mask.sum().item()\n",
    "\n",
    "black_hole_tokens = []\n",
    "if n_black_hole_centroids > 0:\n",
    "    print(f\"Found {n_black_hole_centroids} black hole centroids\")\n",
    "    print(\"Counting tokens...\")\n",
    "    \n",
    "    black_hole_unique_ids = duplicate_mask.nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    for unique_id in tqdm(black_hole_unique_ids, desc=\"Processing\"):\n",
    "        # Find all tokens that map to this unique vector\n",
    "        tokens = (inverse_indices == unique_id).nonzero(as_tuple=True)[0].tolist()\n",
    "        black_hole_tokens.extend(tokens)\n",
    "    \n",
    "    print()\n",
    "\n",
    "n_black_hole_tokens = len(black_hole_tokens)\n",
    "\n",
    "print(f\"Black hole tokens: {n_black_hole_tokens:,} ({100 * n_black_hole_tokens / n_vectors:.2f}%)\")\n",
    "if n_black_hole_tokens > 0:\n",
    "    print(f\"  Organized into {n_black_hole_centroids} centroids\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 2: DEDUPLICATION\n",
      "================================================================================\n",
      "\n",
      "Creating deduplicated token set...\n",
      "  ✓ Deduplicated: 50,257 → 50,257 tokens\n",
      "  ✓ Reduced search space by 0 tokens (0.00%)\n",
      "  ✓ W_dedup on device: mps:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 2: DEDUPLICATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Keep one representative per unique vector\n",
    "print(\"Creating deduplicated token set...\")\n",
    "representative_tokens = []\n",
    "for unique_id in range(n_unique):\n",
    "    # Get first token that maps to this unique vector\n",
    "    token_id = (inverse_indices == unique_id).nonzero(as_tuple=True)[0][0].item()\n",
    "    representative_tokens.append(token_id)\n",
    "\n",
    "# Index W (which is on device) and keep on device\n",
    "W_dedup = W[representative_tokens]\n",
    "n_dedup = len(representative_tokens)\n",
    "\n",
    "print(f\"  ✓ Deduplicated: {n_vectors:,} → {n_dedup:,} tokens\")\n",
    "print(f\"  ✓ Reduced search space by {n_vectors - n_dedup:,} tokens ({100 * (n_vectors - n_dedup) / n_vectors:.2f}%)\")\n",
    "print(f\"  ✓ W_dedup on device: {W_dedup.device}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Isolated Token Exclusion (ε-sphere filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 3: ISOLATED TOKEN EXCLUSION (ε-sphere filter)\n",
      "================================================================================\n",
      "\n",
      "Using ε = 8 × ULP (worst-case 64D diagonal)\n",
      "\n",
      "Memory estimate: 0.02 GB per batch (batch size = 100)\n",
      "\n",
      "Finding candidate pairs within ε...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 503/503 [00:47<00:00, 10.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ✓ Found 0 candidate pairs within ε\n",
      "  ✓ Excluded 50,257 isolated tokens (100.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 3: ISOLATED TOKEN EXCLUSION (ε-sphere filter)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Compute epsilon multiplier (worst-case diagonal distance)\n",
    "EPSILON_MULTIPLIER = math.ceil(math.sqrt(n_dims))\n",
    "print(f\"Using ε = {EPSILON_MULTIPLIER} × ULP (worst-case {n_dims}D diagonal)\")\n",
    "print()\n",
    "\n",
    "# Estimate memory usage\n",
    "batch_memory_gb = (BATCH_SIZE * n_dedup * 4) / 1024**3  # float32\n",
    "print(f\"Memory estimate: {batch_memory_gb:.2f} GB per batch (batch size = {BATCH_SIZE})\")\n",
    "\n",
    "if batch_memory_gb > 0.5:\n",
    "    print(f\"  ⚠️  Large memory usage! Consider reducing BATCH_SIZE if this crashes.\")\n",
    "\n",
    "print()\n",
    "print(\"Finding candidate pairs within ε...\")\n",
    "\n",
    "candidate_pairs = []\n",
    "isolated_tokens = set(range(n_dedup))\n",
    "\n",
    "# Pre-convert W_dedup to float32 once (stays on device)\n",
    "W_dedup_float = W_dedup.float()\n",
    "\n",
    "for i in tqdm(range(0, n_dedup, BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_end = min(i + BATCH_SIZE, n_dedup)\n",
    "    batch = W_dedup_float[i:batch_end]  # Already float32, on device\n",
    "    \n",
    "    # Compute distances to all deduplicated tokens (stays on device)\n",
    "    distances = torch.cdist(batch, W_dedup_float)  # (B, N) on device\n",
    "    \n",
    "    for b in range(batch.shape[0]):\n",
    "        token_i = i + b\n",
    "        \n",
    "        # Compute epsilon for this token\n",
    "        max_exp = get_max_exponent(W_dedup[token_i])\n",
    "        ulp = compute_ulp_at_exponent(max_exp)\n",
    "        epsilon = ulp * EPSILON_MULTIPLIER\n",
    "        \n",
    "        # Find neighbors within epsilon\n",
    "        neighbors = (distances[b] < epsilon).nonzero(as_tuple=True)[0]\n",
    "        neighbors = neighbors[neighbors != token_i]  # Exclude self\n",
    "        \n",
    "        if len(neighbors) > 0:\n",
    "            # This token has neighbors - not isolated\n",
    "            isolated_tokens.discard(token_i)\n",
    "            \n",
    "            # Add pairs (only j > i to avoid duplicates)\n",
    "            for j in neighbors.tolist():\n",
    "                if j > token_i:\n",
    "                    candidate_pairs.append((token_i, j))\n",
    "\n",
    "n_isolated = len(isolated_tokens)\n",
    "n_candidates = len(candidate_pairs)\n",
    "\n",
    "print()\n",
    "print(f\"  ✓ Found {n_candidates:,} candidate pairs within ε\")\n",
    "print(f\"  ✓ Excluded {n_isolated:,} isolated tokens ({100 * n_isolated / n_dedup:.2f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Lattice Neighbor Detection\n",
    "\n",
    "Three-step verification for candidate pairs:\n",
    "1. Geometric filter (L∞ = L1 for orthogonal, general distance check for diagonal)\n",
    "2. ULP distance check\n",
    "3. Bit-level verification (classify as orthogonal or diagonal based on dimensionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 4: LATTICE NEIGHBOR DETECTION\n",
      "================================================================================\n",
      "\n",
      "No candidate pairs to check. Skipping Stage 4.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 4: LATTICE NEIGHBOR DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "if n_candidates == 0:\n",
    "    print(\"No candidate pairs to check. Skipping Stage 4.\\n\")\n",
    "    orthogonal_pairs = []\n",
    "    diagonal_pairs = []\n",
    "else:\n",
    "    # Step 4a: Geometric filter\n",
    "    print(\"Step 4a: Geometric filter...\")\n",
    "    print(\"  (L∞ = L1 suggests orthogonal, but keeping all candidates for full check)\")\n",
    "    lattice_candidates = candidate_pairs  # Keep all ε-sphere candidates\n",
    "    \n",
    "    print(f\"  ✓ {len(lattice_candidates):,} pairs to verify\")\n",
    "    print()\n",
    "    \n",
    "    # Step 4b: Bit-level verification across ALL dimensions\n",
    "    print(\"Step 4b: Full bit-level verification...\")\n",
    "    orthogonal_pairs = []\n",
    "    diagonal_pairs = []\n",
    "    \n",
    "    for i, j in tqdm(lattice_candidates, desc=\"  Checking\"):\n",
    "        # Decode ALL dimensions for both vectors\n",
    "        vec_i = W_dedup[i]\n",
    "        vec_j = W_dedup[j]\n",
    "        \n",
    "        # Convert to uint16 for bit manipulation\n",
    "        bits_i = vec_i.view(torch.int16).cpu().numpy().astype(np.uint16)\n",
    "        bits_j = vec_j.view(torch.int16).cpu().numpy().astype(np.uint16)\n",
    "        \n",
    "        # Extract sign, exponent, mantissa for all dimensions\n",
    "        signs_i = (bits_i >> 15) & 0x1\n",
    "        signs_j = (bits_j >> 15) & 0x1\n",
    "        exps_i = (bits_i >> 7) & 0xFF\n",
    "        exps_j = (bits_j >> 7) & 0xFF\n",
    "        mants_i = bits_i & 0x7F\n",
    "        mants_j = bits_j & 0x7F\n",
    "        \n",
    "        # Find dimensions where vectors are lattice neighbors\n",
    "        same_sign = signs_i == signs_j\n",
    "        same_exp = exps_i == exps_j\n",
    "        mant_diff = np.abs(mants_i.astype(np.int16) - mants_j.astype(np.int16))\n",
    "        \n",
    "        # Lattice neighbor in dimension d: same sign, same exponent, mantissa differs by 1\n",
    "        lattice_neighbor_dims = same_sign & same_exp & (mant_diff == 1)\n",
    "        neighbor_dims = np.where(lattice_neighbor_dims)[0]\n",
    "        \n",
    "        n_diff = len(neighbor_dims)\n",
    "        \n",
    "        if n_diff == 1:\n",
    "            # Orthogonal neighbor (differs in exactly 1 dimension)\n",
    "            orthogonal_pairs.append((i, j, neighbor_dims[0]))\n",
    "        elif n_diff > 1:\n",
    "            # Diagonal neighbor (differs in multiple dimensions)\n",
    "            diagonal_pairs.append((i, j, neighbor_dims.tolist(), n_diff))\n",
    "    \n",
    "    print(f\"  ✓ {len(orthogonal_pairs):,} orthogonal pairs (1D)\")\n",
    "    print(f\"  ✓ {len(diagonal_pairs):,} diagonal pairs (2D+)\")\n",
    "    print()\n",
    "\n",
    "# Extract unique tokens that participate in lattice structure\n",
    "orthogonal_tokens = set()\n",
    "for i, j, dim in orthogonal_pairs:\n",
    "    orthogonal_tokens.add(i)\n",
    "    orthogonal_tokens.add(j)\n",
    "\n",
    "diagonal_tokens = set()\n",
    "for i, j, dims, n_diff in diagonal_pairs:\n",
    "    diagonal_tokens.add(i)\n",
    "    diagonal_tokens.add(j)\n",
    "\n",
    "n_orthogonal_tokens = len(orthogonal_tokens)\n",
    "n_diagonal_tokens = len(diagonal_tokens)\n",
    "n_lattice_tokens = len(orthogonal_tokens | diagonal_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "\n",
      "Saving to: ../tensors/Wordybird/1.13a_lattice_structure.safetensors\n",
      "\n",
      "✓ Saved lattice structure data:\n",
      "  Black holes: 0 tokens\n",
      "  Orthogonal: 0 tokens, 0 edges\n",
      "  Diagonal: 0 tokens, 0 edges\n",
      "  Total lattice: 0 tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Map deduplicated indices back to original token IDs\n",
    "representative_tokens_tensor = torch.tensor(representative_tokens, dtype=torch.int64)\n",
    "\n",
    "# Black hole tokens (already in original token ID space)\n",
    "black_hole_token_ids = torch.tensor(sorted(black_hole_tokens), dtype=torch.int64)\n",
    "black_hole_mask = torch.zeros(n_vectors, dtype=torch.bool)\n",
    "black_hole_mask[black_hole_token_ids] = True\n",
    "\n",
    "# Orthogonal neighbor tokens (map from deduplicated to original)\n",
    "orthogonal_token_ids_dedup = torch.tensor(sorted(orthogonal_tokens), dtype=torch.int64)\n",
    "orthogonal_token_ids = representative_tokens_tensor[orthogonal_token_ids_dedup]\n",
    "orthogonal_mask = torch.zeros(n_vectors, dtype=torch.bool)\n",
    "orthogonal_mask[orthogonal_token_ids] = True\n",
    "\n",
    "# Diagonal neighbor tokens (map from deduplicated to original)\n",
    "diagonal_token_ids_dedup = torch.tensor(sorted(diagonal_tokens), dtype=torch.int64)\n",
    "diagonal_token_ids = representative_tokens_tensor[diagonal_token_ids_dedup]\n",
    "diagonal_mask = torch.zeros(n_vectors, dtype=torch.bool)\n",
    "diagonal_mask[diagonal_token_ids] = True\n",
    "\n",
    "# All lattice tokens (union)\n",
    "lattice_token_ids_dedup = torch.tensor(sorted(orthogonal_tokens | diagonal_tokens), dtype=torch.int64)\n",
    "lattice_token_ids = representative_tokens_tensor[lattice_token_ids_dedup]\n",
    "lattice_mask = torch.zeros(n_vectors, dtype=torch.bool)\n",
    "lattice_mask[lattice_token_ids] = True\n",
    "\n",
    "# Edge lists for graph construction\n",
    "# Store as pairs of original token IDs\n",
    "orthogonal_edges = torch.tensor(\n",
    "    [(representative_tokens[i], representative_tokens[j]) for i, j, _ in orthogonal_pairs],\n",
    "    dtype=torch.int64\n",
    ")\n",
    "\n",
    "diagonal_edges = torch.tensor(\n",
    "    [(representative_tokens[i], representative_tokens[j]) for i, j, _, _ in diagonal_pairs],\n",
    "    dtype=torch.int64\n",
    ")\n",
    "\n",
    "# Prepare save dictionary\n",
    "save_dict = {\n",
    "    # Black holes\n",
    "    'black_hole_token_ids': black_hole_token_ids,\n",
    "    'black_hole_mask': black_hole_mask,\n",
    "    'n_black_hole_tokens': torch.tensor(n_black_hole_tokens, dtype=torch.int64),\n",
    "    'n_black_hole_centroids': torch.tensor(n_black_hole_centroids, dtype=torch.int64),\n",
    "    \n",
    "    # Orthogonal neighbors\n",
    "    'orthogonal_token_ids': orthogonal_token_ids,\n",
    "    'orthogonal_mask': orthogonal_mask,\n",
    "    'orthogonal_edges': orthogonal_edges,\n",
    "    \n",
    "    # Diagonal neighbors\n",
    "    'diagonal_token_ids': diagonal_token_ids,\n",
    "    'diagonal_mask': diagonal_mask,\n",
    "    'diagonal_edges': diagonal_edges,\n",
    "    \n",
    "    # All lattice structure\n",
    "    'lattice_token_ids': lattice_token_ids,\n",
    "    'lattice_mask': lattice_mask,\n",
    "    \n",
    "    # Deduplication mapping (for reference)\n",
    "    'representative_tokens': representative_tokens_tensor,\n",
    "    'inverse_indices': inverse_indices,\n",
    "}\n",
    "\n",
    "# Determine output path based on input\n",
    "model_name = Path(TENSOR_FILE).parent.name\n",
    "output_path = Path(f\"../tensors/{model_name}/1.13a_lattice_structure.safetensors\")\n",
    "\n",
    "print(f\"Saving to: {output_path}\")\n",
    "save_file(save_dict, str(output_path))\n",
    "\n",
    "print()\n",
    "print(\"✓ Saved lattice structure data:\")\n",
    "print(f\"  Black holes: {len(black_hole_token_ids)} tokens\")\n",
    "print(f\"  Orthogonal: {len(orthogonal_token_ids)} tokens, {len(orthogonal_edges)} edges\")\n",
    "print(f\"  Diagonal: {len(diagonal_token_ids)} tokens, {len(diagonal_edges)} edges\")\n",
    "print(f\"  Total lattice: {len(lattice_token_ids)} tokens\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Input: 50,257 vectors × 64 dimensions\n",
      "\n",
      "Black holes:\n",
      "  0 tokens (0.00%)\n",
      "\n",
      "Lattice neighbors:\n",
      "  Orthogonal: 0 pairs, 0 tokens\n",
      "  Diagonal:   0 pairs, 0 tokens\n",
      "  Total:      0 unique tokens in lattice structure (0.00% of deduplicated)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(f\"Input: {n_vectors:,} vectors × {n_dims:,} dimensions\")\n",
    "print()\n",
    "\n",
    "print(f\"Black holes:\")\n",
    "print(f\"  {n_black_hole_tokens:,} tokens ({100 * n_black_hole_tokens / n_vectors:.2f}%)\")\n",
    "if n_black_hole_tokens > 0:\n",
    "    print(f\"  {n_black_hole_centroids} centroids\")\n",
    "print()\n",
    "\n",
    "print(f\"Lattice neighbors:\")\n",
    "print(f\"  Orthogonal: {len(orthogonal_pairs):,} pairs, {n_orthogonal_tokens:,} tokens\")\n",
    "print(f\"  Diagonal:   {len(diagonal_pairs):,} pairs, {n_diagonal_tokens:,} tokens\")\n",
    "print(f\"  Total:      {n_lattice_tokens:,} unique tokens in lattice structure ({100 * n_lattice_tokens / n_dedup:.2f}% of deduplicated)\")\n",
    "print()\n",
    "\n",
    "if len(orthogonal_pairs) > 0:\n",
    "    # Dimension distribution for orthogonal neighbors\n",
    "    dim_counts = Counter([dim for _, _, dim in orthogonal_pairs])\n",
    "    print(f\"Orthogonal pairs by dimension (top 10):\")\n",
    "    for dim, count in dim_counts.most_common(10):\n",
    "        print(f\"  Dimension {dim:4d}: {count:4d} pairs\")\n",
    "    print()\n",
    "\n",
    "if len(diagonal_pairs) > 0:\n",
    "    # Dimensionality distribution for diagonal neighbors\n",
    "    diag_dim_counts = Counter([n_diff for _, _, _, n_diff in diagonal_pairs])\n",
    "    print(f\"Diagonal pairs by dimensionality:\")\n",
    "    for n_diff in sorted(diag_dim_counts.keys()):\n",
    "        count = diag_dim_counts[n_diff]\n",
    "        print(f\"  {n_diff}D diagonal: {count:4d} pairs\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
