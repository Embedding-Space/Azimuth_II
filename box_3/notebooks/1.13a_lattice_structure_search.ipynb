{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.13a: Lattice Structure Search\n",
    "\n",
    "**Goal:** Find black holes and orthogonally-adjacent lattice neighbors.\n",
    "\n",
    "## Method\n",
    "\n",
    "### Stage 1: Black Hole Detection\n",
    "Use `torch.unique()` to find duplicate vectors.\n",
    "\n",
    "### Stage 2: Deduplication\n",
    "Keep one representative per black hole centroid to reduce search space.\n",
    "\n",
    "### Stage 3: Isolated Token Exclusion (ε-sphere filter)\n",
    "For each token, find neighbors within ε = ULP × √D (worst-case diagonal distance).\n",
    "Exclude tokens with no neighbors.\n",
    "\n",
    "### Stage 4: Orthogonal Neighbor Detection\n",
    "For candidate pairs within ε:\n",
    "1. **Geometric filter**: L∞ = L1 (exactly one dimension differs)\n",
    "2. **ULP distance check**: Verify distance ≈ 1 ULP\n",
    "3. **Bit-level verification**: Same sign, same exponent, mantissa differs by 1\n",
    "\n",
    "## Design Goals\n",
    "\n",
    "- **Memory-efficient**: No massive allocations, batch processing\n",
    "- **Hardware accelerated**: Explicit device management for GPU/MPS\n",
    "- **Scalable**: Works for Qwen (151k tokens × 2560D)\n",
    "- **Correct**: Finds nothing in random Gaussian, detects spongecrystal in Qwen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor to analyze\n",
    "TENSOR_FILE = \"../tensors/Qwen3-4B-Instruct-2507/W.safetensors\"\n",
    "TENSOR_KEY = \"W\"\n",
    "TENSOR_INDEX = None  # None = load full tensor\n",
    "\n",
    "# Stage 3 parameters\n",
    "BATCH_SIZE = 100  # For distance computations (100 × 150k × 4 bytes ≈ 60 MB per batch)\n",
    "\n",
    "# Stage 4 parameters\n",
    "ULP_TOLERANCE = 0.01  # Allow 1% tolerance when checking ULP distances\n",
    "MAX_EXAMPLES = 10  # How many example tokens to show in output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import ml_dtypes\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Detect available device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded W from W.safetensors\n",
      "  Shape: torch.Size([151936, 2560])\n",
      "  Dtype: torch.bfloat16\n",
      "  Device: mps:0\n",
      "  Memory: ~0.72 GB\n",
      "\n",
      "Analyzing 151,936 vectors in 2,560 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Load tensor (safetensors loads to CPU by default)\n",
    "data = load_file(TENSOR_FILE)\n",
    "W = data[TENSOR_KEY]\n",
    "\n",
    "# Apply indexing if specified\n",
    "if TENSOR_INDEX is not None:\n",
    "    W = W[TENSOR_INDEX]\n",
    "\n",
    "# Move to device for hardware acceleration\n",
    "W = W.to(device)\n",
    "\n",
    "n_vectors, n_dims = W.shape\n",
    "\n",
    "print(f\"✓ Loaded W from {Path(TENSOR_FILE).name}\")\n",
    "print(f\"  Shape: {W.shape}\")\n",
    "print(f\"  Dtype: {W.dtype}\")\n",
    "print(f\"  Device: {W.device}\")\n",
    "print(f\"  Memory: ~{W.element_size() * W.numel() / 1024**3:.2f} GB\")\n",
    "print()\n",
    "print(f\"Analyzing {n_vectors:,} vectors in {n_dims:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def get_max_exponent(vector_bf16):\n",
    "    \"\"\"\n",
    "    Get the maximum exponent across all dimensions of a bfloat16 vector.\n",
    "    \n",
    "    Args:\n",
    "        vector_bf16: (D,) tensor of bfloat16 values (on any device)\n",
    "    \n",
    "    Returns:\n",
    "        int: maximum exponent value\n",
    "    \"\"\"\n",
    "    values_uint16 = vector_bf16.view(torch.int16).to(torch.int64) & 0xFFFF\n",
    "    exponents = (values_uint16 >> 7) & 0xFF\n",
    "    return exponents.max().item()\n",
    "\n",
    "def decode_bfloat16_bits(value_bf16):\n",
    "    \"\"\"\n",
    "    Decode a single bfloat16 value into its bit components.\n",
    "    \n",
    "    Args:\n",
    "        value_bf16: scalar torch.bfloat16 value (on any device)\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'sign', 'exponent', 'mantissa' as integers\n",
    "    \"\"\"\n",
    "    # Convert to bytes and interpret as uint16\n",
    "    bits_uint16 = value_bf16.view(torch.int16).cpu().numpy().astype(np.uint16).item()\n",
    "    bits_binary = format(bits_uint16, '016b')\n",
    "    \n",
    "    sign_bit = bits_binary[0]\n",
    "    exponent_bits = bits_binary[1:9]\n",
    "    mantissa_bits = bits_binary[9:16]\n",
    "    \n",
    "    sign = int(sign_bit)\n",
    "    exponent = int(exponent_bits, 2)\n",
    "    mantissa = int(mantissa_bits, 2)\n",
    "    \n",
    "    return {\n",
    "        'bits_uint16': bits_uint16,\n",
    "        'bits_binary': bits_binary,\n",
    "        'sign': sign,\n",
    "        'exponent': exponent,\n",
    "        'mantissa': mantissa,\n",
    "        'sign_bit': sign_bit,\n",
    "        'exponent_bits': exponent_bits,\n",
    "        'mantissa_bits': mantissa_bits\n",
    "    }\n",
    "\n",
    "def compute_ulp_at_exponent(exponent):\n",
    "    \"\"\"\n",
    "    Compute ULP (unit in last place) for bfloat16 at given exponent.\n",
    "    \n",
    "    bfloat16 has 7 mantissa bits, so ULP = 2^(exponent - 127 - 7) = 2^(exponent - 134)\n",
    "    \"\"\"\n",
    "    return 2.0 ** (exponent - 134)\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Black Hole Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 1: BLACK HOLE DETECTION\n",
      "================================================================================\n",
      "\n",
      "Finding unique vectors...\n",
      "  ✓ Found 149,849 unique vectors\n",
      "  ✓ 2,087 vectors are duplicates\n",
      "\n",
      "Found 13 black hole centroids\n",
      "Counting tokens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 13/13 [00:00<00:00, 5757.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Black hole tokens: 2,100 (1.38%)\n",
      "  Organized into 13 centroids\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STAGE 1: BLACK HOLE DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# torch.unique not implemented on MPS in Torch 2.8, use CPU\n",
    "print(\"Finding unique vectors...\")\n",
    "W_cpu = W.cpu()\n",
    "W_unique, inverse_indices, counts = torch.unique(W_cpu, dim=0, return_inverse=True, return_counts=True)\n",
    "\n",
    "n_unique = len(W_unique)\n",
    "n_duplicates = n_vectors - n_unique\n",
    "\n",
    "print(f\"  ✓ Found {n_unique:,} unique vectors\")\n",
    "print(f\"  ✓ {n_duplicates:,} vectors are duplicates\")\n",
    "print()\n",
    "\n",
    "# Count tokens participating in black holes\n",
    "duplicate_mask = counts > 1\n",
    "n_black_hole_centroids = duplicate_mask.sum().item()\n",
    "\n",
    "black_hole_tokens = []\n",
    "if n_black_hole_centroids > 0:\n",
    "    print(f\"Found {n_black_hole_centroids} black hole centroids\")\n",
    "    print(\"Counting tokens...\")\n",
    "    \n",
    "    black_hole_unique_ids = duplicate_mask.nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    for unique_id in tqdm(black_hole_unique_ids, desc=\"Processing\"):\n",
    "        # Find all tokens that map to this unique vector\n",
    "        tokens = (inverse_indices == unique_id).nonzero(as_tuple=True)[0].tolist()\n",
    "        black_hole_tokens.extend(tokens)\n",
    "    \n",
    "    print()\n",
    "\n",
    "n_black_hole_tokens = len(black_hole_tokens)\n",
    "\n",
    "print(f\"Black hole tokens: {n_black_hole_tokens:,} ({100 * n_black_hole_tokens / n_vectors:.2f}%)\")\n",
    "if n_black_hole_tokens > 0:\n",
    "    print(f\"  Organized into {n_black_hole_centroids} centroids\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 2: DEDUPLICATION\n",
      "================================================================================\n",
      "\n",
      "Creating deduplicated token set...\n",
      "  ✓ Deduplicated: 151,936 → 149,849 tokens\n",
      "  ✓ Reduced search space by 2,087 tokens (1.37%)\n",
      "  ✓ W_dedup on device: mps:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 2: DEDUPLICATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Keep one representative per unique vector\n",
    "print(\"Creating deduplicated token set...\")\n",
    "representative_tokens = []\n",
    "for unique_id in range(n_unique):\n",
    "    # Get first token that maps to this unique vector\n",
    "    token_id = (inverse_indices == unique_id).nonzero(as_tuple=True)[0][0].item()\n",
    "    representative_tokens.append(token_id)\n",
    "\n",
    "# Index W (which is on device) and keep on device\n",
    "W_dedup = W[representative_tokens]\n",
    "n_dedup = len(representative_tokens)\n",
    "\n",
    "print(f\"  ✓ Deduplicated: {n_vectors:,} → {n_dedup:,} tokens\")\n",
    "print(f\"  ✓ Reduced search space by {n_vectors - n_dedup:,} tokens ({100 * (n_vectors - n_dedup) / n_vectors:.2f}%)\")\n",
    "print(f\"  ✓ W_dedup on device: {W_dedup.device}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Isolated Token Exclusion (ε-sphere filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 3: ISOLATED TOKEN EXCLUSION (ε-sphere filter)\n",
      "================================================================================\n",
      "\n",
      "Using ε = 51 × ULP (worst-case 2560D diagonal)\n",
      "\n",
      "Memory estimate: 0.06 GB per batch (batch size = 100)\n",
      "\n",
      "Finding candidate pairs within ε...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1499/1499 [04:01<00:00,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ✓ Found 11,212 candidate pairs within ε\n",
      "  ✓ Excluded 149,698 isolated tokens (99.90%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 3: ISOLATED TOKEN EXCLUSION (ε-sphere filter)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Compute epsilon multiplier (worst-case diagonal distance)\n",
    "EPSILON_MULTIPLIER = math.ceil(math.sqrt(n_dims))\n",
    "print(f\"Using ε = {EPSILON_MULTIPLIER} × ULP (worst-case {n_dims}D diagonal)\")\n",
    "print()\n",
    "\n",
    "# Estimate memory usage\n",
    "batch_memory_gb = (BATCH_SIZE * n_dedup * 4) / 1024**3  # float32\n",
    "print(f\"Memory estimate: {batch_memory_gb:.2f} GB per batch (batch size = {BATCH_SIZE})\")\n",
    "\n",
    "if batch_memory_gb > 0.5:\n",
    "    print(f\"  ⚠️  Large memory usage! Consider reducing BATCH_SIZE if this crashes.\")\n",
    "\n",
    "print()\n",
    "print(\"Finding candidate pairs within ε...\")\n",
    "\n",
    "candidate_pairs = []\n",
    "isolated_tokens = set(range(n_dedup))\n",
    "\n",
    "# Pre-convert W_dedup to float32 once (stays on device)\n",
    "W_dedup_float = W_dedup.float()\n",
    "\n",
    "for i in tqdm(range(0, n_dedup, BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_end = min(i + BATCH_SIZE, n_dedup)\n",
    "    batch = W_dedup_float[i:batch_end]  # Already float32, on device\n",
    "    \n",
    "    # Compute distances to all deduplicated tokens (stays on device)\n",
    "    distances = torch.cdist(batch, W_dedup_float)  # (B, N) on device\n",
    "    \n",
    "    for b in range(batch.shape[0]):\n",
    "        token_i = i + b\n",
    "        \n",
    "        # Compute epsilon for this token\n",
    "        max_exp = get_max_exponent(W_dedup[token_i])\n",
    "        ulp = compute_ulp_at_exponent(max_exp)\n",
    "        epsilon = ulp * EPSILON_MULTIPLIER\n",
    "        \n",
    "        # Find neighbors within epsilon\n",
    "        neighbors = (distances[b] < epsilon).nonzero(as_tuple=True)[0]\n",
    "        neighbors = neighbors[neighbors != token_i]  # Exclude self\n",
    "        \n",
    "        if len(neighbors) > 0:\n",
    "            # This token has neighbors - not isolated\n",
    "            isolated_tokens.discard(token_i)\n",
    "            \n",
    "            # Add pairs (only j > i to avoid duplicates)\n",
    "            for j in neighbors.tolist():\n",
    "                if j > token_i:\n",
    "                    candidate_pairs.append((token_i, j))\n",
    "\n",
    "n_isolated = len(isolated_tokens)\n",
    "n_candidates = len(candidate_pairs)\n",
    "\n",
    "print()\n",
    "print(f\"  ✓ Found {n_candidates:,} candidate pairs within ε\")\n",
    "print(f\"  ✓ Excluded {n_isolated:,} isolated tokens ({100 * n_isolated / n_dedup:.2f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Orthogonal Neighbor Detection\n",
    "\n",
    "Three-step verification for candidate pairs:\n",
    "1. Geometric filter (L∞ = L1)\n",
    "2. ULP distance check\n",
    "3. Bit-level verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 4: ORTHOGONAL NEIGHBOR DETECTION\n",
      "================================================================================\n",
      "\n",
      "Step 4a: Geometric filter (L∞ = L1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking: 100%|██████████| 11212/11212 [00:03<00:00, 3442.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 240 pairs pass L∞ = L1 test\n",
      "\n",
      "Step 4b: ULP distance check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking: 100%|██████████| 240/240 [00:00<00:00, 2500.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 159 pairs pass ULP distance test\n",
      "\n",
      "Step 4c: Bit-level verification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Checking: 100%|██████████| 159/159 [00:00<00:00, 4337.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ 159 pairs verified as orthogonal neighbors\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 4: ORTHOGONAL NEIGHBOR DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "if n_candidates == 0:\n",
    "    print(\"No candidate pairs to check. Skipping Stage 4.\\n\")\n",
    "    orthogonal_pairs = []\n",
    "else:\n",
    "    # Step 4a: Geometric filter (L∞ = L1)\n",
    "    print(\"Step 4a: Geometric filter (L∞ = L1)...\")\n",
    "    orthogonal_candidates = []\n",
    "    \n",
    "    for i, j in tqdm(candidate_pairs, desc=\"  Checking\"):\n",
    "        # Operations on device\n",
    "        diff = (W_dedup_float[i] - W_dedup_float[j]).abs()\n",
    "        l_inf = diff.max().item()\n",
    "        l_1 = diff.sum().item()\n",
    "        \n",
    "        if abs(l_inf - l_1) < 1e-7:  # Exactly one dimension differs\n",
    "            orthogonal_candidates.append((i, j))\n",
    "    \n",
    "    print(f\"  ✓ {len(orthogonal_candidates):,} pairs pass L∞ = L1 test\")\n",
    "    print()\n",
    "    \n",
    "    # Step 4b: ULP distance check\n",
    "    if len(orthogonal_candidates) == 0:\n",
    "        print(\"No candidates passed geometric filter. Skipping steps 4b-4c.\\n\")\n",
    "        orthogonal_pairs = []\n",
    "    else:\n",
    "        print(\"Step 4b: ULP distance check...\")\n",
    "        ulp_candidates = []\n",
    "        \n",
    "        for i, j in tqdm(orthogonal_candidates, desc=\"  Checking\"):\n",
    "            diff = (W_dedup_float[i] - W_dedup_float[j]).abs()\n",
    "            dim = diff.argmax().item()  # The one dimension that differs\n",
    "            actual_dist = diff[dim].item()\n",
    "            \n",
    "            # Decode exponent at this dimension\n",
    "            val_i = W_dedup[i, dim]\n",
    "            decoded = decode_bfloat16_bits(val_i)\n",
    "            ulp = compute_ulp_at_exponent(decoded['exponent'])\n",
    "            \n",
    "            if abs(actual_dist - ulp) < ulp * ULP_TOLERANCE:\n",
    "                ulp_candidates.append((i, j, dim))\n",
    "        \n",
    "        print(f\"  ✓ {len(ulp_candidates):,} pairs pass ULP distance test\")\n",
    "        print()\n",
    "        \n",
    "        # Step 4c: Bit-level verification\n",
    "        if len(ulp_candidates) == 0:\n",
    "            print(\"No candidates passed ULP check. Skipping step 4c.\\n\")\n",
    "            orthogonal_pairs = []\n",
    "        else:\n",
    "            print(\"Step 4c: Bit-level verification...\")\n",
    "            orthogonal_pairs = []\n",
    "            \n",
    "            for i, j, dim in tqdm(ulp_candidates, desc=\"  Checking\"):\n",
    "                val_i = W_dedup[i, dim]\n",
    "                val_j = W_dedup[j, dim]\n",
    "                \n",
    "                decoded_i = decode_bfloat16_bits(val_i)\n",
    "                decoded_j = decode_bfloat16_bits(val_j)\n",
    "                \n",
    "                same_sign = decoded_i['sign'] == decoded_j['sign']\n",
    "                same_exp = decoded_i['exponent'] == decoded_j['exponent']\n",
    "                mant_diff = abs(decoded_i['mantissa'] - decoded_j['mantissa'])\n",
    "                \n",
    "                if same_sign and same_exp and mant_diff == 1:\n",
    "                    orthogonal_pairs.append((i, j, dim))\n",
    "            \n",
    "            print(f\"  ✓ {len(orthogonal_pairs):,} pairs verified as orthogonal neighbors\")\n",
    "            print()\n",
    "\n",
    "# Extract unique tokens that participate in orthogonal structure\n",
    "orthogonal_tokens = set()\n",
    "for i, j, dim in orthogonal_pairs:\n",
    "    orthogonal_tokens.add(i)\n",
    "    orthogonal_tokens.add(j)\n",
    "\n",
    "n_orthogonal_tokens = len(orthogonal_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Input: 151,936 vectors × 2,560 dimensions\n",
      "\n",
      "Black holes:\n",
      "  2,100 tokens (1.38%)\n",
      "  13 centroids\n",
      "\n",
      "Orthogonal neighbors:\n",
      "  159 pairs found\n",
      "  39 unique tokens participate (0.03% of deduplicated)\n",
      "\n",
      "Most common dimensions (top 10):\n",
      "  Dimension 1564:   67 pairs\n",
      "  Dimension 1435:   27 pairs\n",
      "  Dimension 1718:   21 pairs\n",
      "  Dimension  216:   13 pairs\n",
      "  Dimension 1362:   13 pairs\n",
      "  Dimension 1008:   10 pairs\n",
      "  Dimension 1382:    8 pairs\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(f\"Input: {n_vectors:,} vectors × {n_dims:,} dimensions\")\n",
    "print()\n",
    "\n",
    "print(f\"Black holes:\")\n",
    "print(f\"  {n_black_hole_tokens:,} tokens ({100 * n_black_hole_tokens / n_vectors:.2f}%)\")\n",
    "if n_black_hole_tokens > 0:\n",
    "    print(f\"  {n_black_hole_centroids} centroids\")\n",
    "print()\n",
    "\n",
    "print(f\"Orthogonal neighbors:\")\n",
    "print(f\"  {len(orthogonal_pairs):,} pairs found\")\n",
    "print(f\"  {n_orthogonal_tokens:,} unique tokens participate ({100 * n_orthogonal_tokens / n_dedup:.2f}% of deduplicated)\")\n",
    "print()\n",
    "\n",
    "if len(orthogonal_pairs) > 0:\n",
    "    # Dimension distribution\n",
    "    dim_counts = Counter([dim for _, _, dim in orthogonal_pairs])\n",
    "    print(f\"Most common dimensions (top 10):\")\n",
    "    for dim, count in dim_counts.most_common(10):\n",
    "        print(f\"  Dimension {dim:4d}: {count:4d} pairs\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
