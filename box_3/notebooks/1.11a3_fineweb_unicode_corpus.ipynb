{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.11a3: FineWeb Unicode Corpus Preparation\n",
    "\n",
    "**Goal:** Download a small sample of FineWeb, prepare it for training Wordybird, and identify which GPT-2 tokens appear in the corpus.\n",
    "\n",
    "## What is Wordybird?\n",
    "\n",
    "Wordybird is our dimensional crowding experiment. We're testing whether spongecrystal formation requires cramming many untrained tokens into a limited number of dimensions.\n",
    "\n",
    "**Hypothesis:** In 64D space with 50,257 tokens (GPT-2 vocab), thousands of untrained tokens compete for the same degrees of freedom, potentially causing:\n",
    "- Forced collisions\n",
    "- Quantization clustering (shared bfloat16 lattice)\n",
    "- Gradient averaging (equally-wrong tokens moving together)\n",
    "\n",
    "## This Notebook\n",
    "\n",
    "1. Download ~2 MB of FineWeb (enough for 100-1000 training steps)\n",
    "2. Keep as Unicode (unlike Gatsby's ASCII-only approach)\n",
    "3. Tokenize with GPT-2 tokenizer\n",
    "4. Identify trained vs untrained tokens\n",
    "5. Save corpus and token masks for Wordybird training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus parameters\n",
    "TARGET_SIZE_MB = 2.0  # Download ~2 MB of FineWeb\n",
    "DATASET_NAME = \"HuggingFaceFW/fineweb\"\n",
    "DATASET_CONFIG = \"sample-10BT\"  # 10B token sample\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Output paths\n",
    "CORPUS_OUTPUT = \"../data/fineweb_2mb_unicode.txt\"\n",
    "MASK_OUTPUT = \"../tensors/Wordybird/fineweb_token_masks.safetensors\"\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "import random\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Detect available device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT-2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 tokenizer...\n",
      "\n",
      "✓ Loaded GPT-2 tokenizer\n",
      "  Vocabulary size: 50,257 tokens\n",
      "  Vocab size (attribute): 50,257\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading GPT-2 tokenizer...\\n\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(f\"✓ Loaded GPT-2 tokenizer\")\n",
    "print(f\"  Vocabulary size: {len(tokenizer):,} tokens\")\n",
    "print(f\"  Vocab size (attribute): {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download FineWeb Sample\n",
    "\n",
    "Stream from HuggingFace until we hit our target size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ~2.0 MB from FineWeb...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136d41f03c004df58b2c28dc44439d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Downloaded corpus\n",
      "  Documents: 668\n",
      "  Characters: 2,089,201\n",
      "  Bytes (UTF-8): 2,098,654 (2.00 MB)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Downloading ~{TARGET_SIZE_MB} MB from FineWeb...\\n\")\n",
    "\n",
    "# Load dataset in streaming mode\n",
    "dataset = load_dataset(\n",
    "    DATASET_NAME,\n",
    "    name=DATASET_CONFIG,\n",
    "    split=DATASET_SPLIT,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Collect text until we hit target size\n",
    "target_bytes = int(TARGET_SIZE_MB * 1024 * 1024)\n",
    "texts = []\n",
    "total_bytes = 0\n",
    "\n",
    "for example in dataset:\n",
    "    text = example['text']\n",
    "    text_bytes = len(text.encode('utf-8'))\n",
    "    \n",
    "    texts.append(text)\n",
    "    total_bytes += text_bytes\n",
    "    \n",
    "    if total_bytes >= target_bytes:\n",
    "        break\n",
    "\n",
    "# Combine all texts\n",
    "corpus_text = '\\n\\n'.join(texts)\n",
    "actual_bytes = len(corpus_text.encode('utf-8'))\n",
    "actual_mb = actual_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"✓ Downloaded corpus\")\n",
    "print(f\"  Documents: {len(texts):,}\")\n",
    "print(f\"  Characters: {len(corpus_text):,}\")\n",
    "print(f\"  Bytes (UTF-8): {actual_bytes:,} ({actual_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing corpus...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (475160 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 475,160\n",
      "Unique tokens in corpus: 30,590\n",
      "Token usage: 60.87% of vocabulary\n",
      "\n",
      "Trained tokens: 30,590\n",
      "Untrained tokens: 19,667\n",
      "\n",
      "Unique characters: 252\n",
      "\n",
      "Sample of untrained tokens (first 20):\n",
      "     90: {\n",
      "     92: }\n",
      "    106: �\n",
      "    107: �\n",
      "    114: �\n",
      "    121: �\n",
      "    124: �\n",
      "    125: �\n",
      "    128: �\n",
      "    130: �\n",
      "    131: �\n",
      "    132: �\n",
      "    133: �\n",
      "    135: �\n",
      "    136: �\n",
      "    137: �\n",
      "    142: �\n",
      "    143: �\n",
      "    144: �\n",
      "    145: �\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAnalyzing corpus...\\n\")\n",
    "\n",
    "# Tokenize the entire corpus\n",
    "tokens = tokenizer.encode(corpus_text)\n",
    "print(f\"Total tokens: {len(tokens):,}\")\n",
    "\n",
    "# Find unique tokens that appear\n",
    "unique_tokens = sorted(set(tokens))\n",
    "print(f\"Unique tokens in corpus: {len(unique_tokens):,}\")\n",
    "print(f\"Token usage: {100 * len(unique_tokens) / len(tokenizer):.2f}% of vocabulary\")\n",
    "print()\n",
    "\n",
    "# Identify trained vs untrained\n",
    "all_token_ids = set(range(len(tokenizer)))\n",
    "trained_token_ids = set(unique_tokens)\n",
    "untrained_token_ids = all_token_ids - trained_token_ids\n",
    "\n",
    "print(f\"Trained tokens: {len(trained_token_ids):,}\")\n",
    "print(f\"Untrained tokens: {len(untrained_token_ids):,}\")\n",
    "print()\n",
    "\n",
    "# Character set analysis\n",
    "unique_chars = set(corpus_text)\n",
    "print(f\"Unique characters: {len(unique_chars):,}\")\n",
    "\n",
    "# Show some untrained token examples\n",
    "print(f\"\\nSample of untrained tokens (first 20):\")\n",
    "untrained_sample = sorted(untrained_token_ids)[:20]\n",
    "for token_id in untrained_sample:\n",
    "    token_str = tokenizer.decode([token_id])\n",
    "    # Escape special chars for display\n",
    "    display_str = repr(token_str)[1:-1]  # Remove outer quotes from repr\n",
    "    print(f\"  {token_id:5d}: {display_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving corpus to ../data/fineweb_2mb_unicode.txt...\n",
      "\n",
      "✓ Saved corpus\n",
      "  Path: ../data/fineweb_2mb_unicode.txt\n",
      "  Size: 2.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving corpus to {CORPUS_OUTPUT}...\\n\")\n",
    "\n",
    "# Ensure directory exists\n",
    "Path(CORPUS_OUTPUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as UTF-8\n",
    "with open(CORPUS_OUTPUT, 'w', encoding='utf-8') as f:\n",
    "    f.write(corpus_text)\n",
    "\n",
    "print(f\"✓ Saved corpus\")\n",
    "print(f\"  Path: {CORPUS_OUTPUT}\")\n",
    "print(f\"  Size: {actual_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Token Masks\n",
    "\n",
    "Save boolean masks indicating which tokens are trained/untrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating token masks...\n",
      "\n",
      "✓ Created masks\n",
      "  Trained mask: 30,590 True values\n",
      "  Untrained mask: 19,667 True values\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCreating token masks...\\n\")\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "# Create masks\n",
    "trained_mask = torch.zeros(vocab_size, dtype=torch.bool)\n",
    "trained_mask[list(trained_token_ids)] = True\n",
    "\n",
    "untrained_mask = ~trained_mask\n",
    "\n",
    "# Also save the actual token IDs for convenience\n",
    "trained_indices = torch.tensor(sorted(trained_token_ids), dtype=torch.long)\n",
    "untrained_indices = torch.tensor(sorted(untrained_token_ids), dtype=torch.long)\n",
    "\n",
    "print(f\"✓ Created masks\")\n",
    "print(f\"  Trained mask: {trained_mask.sum().item():,} True values\")\n",
    "print(f\"  Untrained mask: {untrained_mask.sum().item():,} True values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Token Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving token masks to ../tensors/Wordybird/fineweb_token_masks.safetensors...\n",
      "\n",
      "✓ Saved token masks\n",
      "  Path: ../tensors/Wordybird/fineweb_token_masks.safetensors\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving token masks to {MASK_OUTPUT}...\\n\")\n",
    "\n",
    "# Ensure directory exists\n",
    "Path(MASK_OUTPUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save to safetensors\n",
    "save_file(\n",
    "    {\n",
    "        'trained_mask': trained_mask,\n",
    "        'untrained_mask': untrained_mask,\n",
    "        'trained_indices': trained_indices,\n",
    "        'untrained_indices': untrained_indices,\n",
    "    },\n",
    "    str(MASK_OUTPUT)\n",
    ")\n",
    "\n",
    "print(f\"✓ Saved token masks\")\n",
    "print(f\"  Path: {MASK_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CORPUS PREPARATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Corpus:\n",
      "  Path: ../data/fineweb_2mb_unicode.txt\n",
      "  Size: 2.00 MB\n",
      "  Tokens: 475,160\n",
      "\n",
      "Tokenizer: GPT-2\n",
      "  Vocabulary: 50,257 tokens\n",
      "  Trained: 30,590 (60.9%)\n",
      "  Untrained: 19,667 (39.1%)\n",
      "\n",
      "Token masks saved to:\n",
      "  ../tensors/Wordybird/fineweb_token_masks.safetensors\n",
      "\n",
      "Ready for Wordybird training!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"CORPUS PREPARATION COMPLETE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Corpus:\")\n",
    "print(f\"  Path: {CORPUS_OUTPUT}\")\n",
    "print(f\"  Size: {actual_mb:.2f} MB\")\n",
    "print(f\"  Tokens: {len(tokens):,}\")\n",
    "print()\n",
    "\n",
    "print(f\"Tokenizer: GPT-2\")\n",
    "print(f\"  Vocabulary: {vocab_size:,} tokens\")\n",
    "print(f\"  Trained: {len(trained_token_ids):,} ({100*len(trained_token_ids)/vocab_size:.1f}%)\")\n",
    "print(f\"  Untrained: {len(untrained_token_ids):,} ({100*len(untrained_token_ids)/vocab_size:.1f}%)\")\n",
    "print()\n",
    "\n",
    "print(f\"Token masks saved to:\")\n",
    "print(f\"  {MASK_OUTPUT}\")\n",
    "print()\n",
    "\n",
    "print(f\"Ready for Wordybird training!\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
