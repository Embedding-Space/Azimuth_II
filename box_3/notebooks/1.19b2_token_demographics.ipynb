{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.19b2: Flannel Tokenizer Demographics\n",
    "\n",
    "**Goal:** Analyze the composition of our 10,000-token Flannel tokenizer.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "We trained our tokenizer on 80% English + 20% Thai, and we'll train our model on 100% English. This means **all Thai tokens should be dead** (never appear during model training).\n",
    "\n",
    "By counting exact token demographics now, we can validate our experiment later:\n",
    "- If we find **N pure Thai tokens** in the vocabulary\n",
    "- And exactly **N tokens never get trained** during English-only training\n",
    "- That's perfect validation of our experimental design!\n",
    "\n",
    "## Token Categories\n",
    "\n",
    "We'll classify every token as:\n",
    "1. **Pure English:** All characters are ASCII letters/common punctuation\n",
    "2. **Pure Thai:** All characters are Thai script (U+0E00–U+0E7F)\n",
    "3. **Mixed:** Contains both English and Thai characters\n",
    "4. **Numeric:** Numbers and math symbols\n",
    "5. **Whitespace:** Spaces, tabs, newlines\n",
    "6. **Punctuation:** Pure punctuation (no letters)\n",
    "7. **Special:** Special tokens like `<|endoftext|>`\n",
    "8. **Other:** Everything else (emoji, other scripts, etc.)\n",
    "\n",
    "## This Notebook\n",
    "\n",
    "1. Load the Flannel tokenizer\n",
    "2. Classify every token\n",
    "3. Show statistics and examples\n",
    "4. Save demographics data for later validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parameters set\n"
     ]
    }
   ],
   "source": [
    "# Input tokenizer (from 1.19b)\n",
    "TOKENIZER_PATH = \"../data/flannel_tokenizer_chars.json\"\n",
    "\n",
    "# Output demographics\n",
    "OUTPUT_PATH = \"../data/flannel_token_demographics.json\"\n",
    "\n",
    "# Special tokens\n",
    "SPECIAL_TOKENS = [\"<|endoftext|>\"]\n",
    "\n",
    "print(\"✓ Parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from ../data/flannel_tokenizer_chars.json...\n",
      "\n",
      "✓ Loaded tokenizer\n",
      "  Vocabulary size: 10,000 tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = Path(TOKENIZER_PATH)\n",
    "\n",
    "if not tokenizer_path.exists():\n",
    "    raise FileNotFoundError(f\"Tokenizer not found at {TOKENIZER_PATH}. Run 1.19b first.\")\n",
    "\n",
    "print(f\"Loading tokenizer from {TOKENIZER_PATH}...\\n\")\n",
    "tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"✓ Loaded tokenizer\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Classification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Classification function defined\n"
     ]
    }
   ],
   "source": [
    "def classify_token(token):\n",
    "    \"\"\"\n",
    "    Classify a token into one of several categories.\n",
    "    \n",
    "    Returns: (category, subcategory)\n",
    "    \"\"\"\n",
    "    # Special tokens\n",
    "    if token in SPECIAL_TOKENS:\n",
    "        return ('special', 'special')\n",
    "    \n",
    "    # Empty token (shouldn't happen, but check)\n",
    "    if not token:\n",
    "        return ('other', 'empty')\n",
    "    \n",
    "    # Analyze character composition\n",
    "    has_thai = False\n",
    "    has_ascii_letter = False\n",
    "    has_digit = False\n",
    "    has_whitespace = False\n",
    "    has_punctuation = False\n",
    "    has_other = False\n",
    "    \n",
    "    for char in token:\n",
    "        code = ord(char)\n",
    "        \n",
    "        # Thai script: U+0E00 to U+0E7F\n",
    "        if 0x0E00 <= code <= 0x0E7F:\n",
    "            has_thai = True\n",
    "        \n",
    "        # ASCII letters: a-z, A-Z\n",
    "        elif (65 <= code <= 90) or (97 <= code <= 122):\n",
    "            has_ascii_letter = True\n",
    "        \n",
    "        # Digits: 0-9\n",
    "        elif 48 <= code <= 57:\n",
    "            has_digit = True\n",
    "        \n",
    "        # Whitespace\n",
    "        elif char.isspace():\n",
    "            has_whitespace = True\n",
    "        \n",
    "        # Common ASCII punctuation\n",
    "        elif code < 128 and not char.isalnum():\n",
    "            has_punctuation = True\n",
    "        \n",
    "        # Everything else (emoji, other scripts, etc.)\n",
    "        else:\n",
    "            has_other = True\n",
    "    \n",
    "    # Classify based on composition\n",
    "    # Pure categories first\n",
    "    if has_thai and not (has_ascii_letter or has_digit or has_other):\n",
    "        # Pure Thai (whitespace/punctuation allowed)\n",
    "        return ('thai', 'pure')\n",
    "    \n",
    "    if has_ascii_letter and not (has_thai or has_other):\n",
    "        # Pure English (whitespace/punctuation/digits allowed)\n",
    "        return ('english', 'pure')\n",
    "    \n",
    "    if has_whitespace and not (has_ascii_letter or has_thai or has_digit or has_other):\n",
    "        # Pure whitespace (with possible punctuation)\n",
    "        return ('whitespace', 'pure')\n",
    "    \n",
    "    if has_punctuation and not (has_ascii_letter or has_thai or has_digit or has_other or has_whitespace):\n",
    "        # Pure punctuation\n",
    "        return ('punctuation', 'pure')\n",
    "    \n",
    "    if has_digit and not (has_ascii_letter or has_thai or has_other):\n",
    "        # Numeric (with possible punctuation/whitespace)\n",
    "        return ('numeric', 'pure')\n",
    "    \n",
    "    # Mixed categories\n",
    "    if has_thai and has_ascii_letter:\n",
    "        return ('mixed', 'thai_english')\n",
    "    \n",
    "    if has_thai and has_other:\n",
    "        return ('mixed', 'thai_other')\n",
    "    \n",
    "    if has_ascii_letter and has_other:\n",
    "        return ('mixed', 'english_other')\n",
    "    \n",
    "    # Catch-all\n",
    "    return ('other', 'unknown')\n",
    "\n",
    "print(\"✓ Classification function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify All Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying 10,000 tokens...\n",
      "\n",
      "✓ Classification complete\n",
      "  Total tokens classified: 10,000\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nClassifying {vocab_size:,} tokens...\\n\")\n",
    "\n",
    "# Classify every token\n",
    "token_classifications = {}  # token_id -> (category, subcategory)\n",
    "tokens_by_category = defaultdict(list)  # category -> [(token, id), ...]\n",
    "\n",
    "for token_str, token_id in vocab.items():\n",
    "    category, subcategory = classify_token(token_str)\n",
    "    token_classifications[token_id] = (category, subcategory)\n",
    "    tokens_by_category[category].append((token_str, token_id))\n",
    "\n",
    "# Count by category\n",
    "category_counts = Counter(cat for cat, _ in token_classifications.values())\n",
    "\n",
    "print(f\"✓ Classification complete\")\n",
    "print(f\"  Total tokens classified: {len(token_classifications):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TOKEN DEMOGRAPHICS\n",
      "======================================================================\n",
      "\n",
      "Total vocabulary: 10,000 tokens\n",
      "\n",
      "Breakdown by category:\n",
      "  English     : 5,897 tokens (58.97%)\n",
      "  Thai        : 1,272 tokens (12.72%)\n",
      "  Mixed       :     0 tokens ( 0.00%)\n",
      "  Numeric     :   132 tokens ( 1.32%)\n",
      "  Whitespace  :     3 tokens ( 0.03%)\n",
      "  Punctuation :    68 tokens ( 0.68%)\n",
      "  Special     :     1 tokens ( 0.01%)\n",
      "  Other       : 2,627 tokens (26.27%)\n",
      "\n",
      "Key findings:\n",
      "  Pure English tokens: 5,897\n",
      "  Pure Thai tokens: 1,272\n",
      "  Mixed tokens: 0\n",
      "\n",
      "Expected dead tokens (Thai + mixed with Thai):\n",
      "  Estimated: 1,272 tokens\n",
      "  (These should never appear during English-only training)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TOKEN DEMOGRAPHICS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Total vocabulary: {vocab_size:,} tokens\\n\")\n",
    "\n",
    "print(f\"Breakdown by category:\")\n",
    "for category in ['english', 'thai', 'mixed', 'numeric', 'whitespace', 'punctuation', 'special', 'other']:\n",
    "    count = category_counts[category]\n",
    "    pct = 100 * count / vocab_size\n",
    "    print(f\"  {category.capitalize():12s}: {count:5,} tokens ({pct:5.2f}%)\")\n",
    "\n",
    "print()\n",
    "print(f\"Key findings:\")\n",
    "print(f\"  Pure English tokens: {category_counts['english']:,}\")\n",
    "print(f\"  Pure Thai tokens: {category_counts['thai']:,}\")\n",
    "print(f\"  Mixed tokens: {category_counts['mixed']:,}\")\n",
    "print()\n",
    "print(f\"Expected dead tokens (Thai + mixed with Thai):\")\n",
    "mixed_with_thai = sum(1 for cat, subcat in token_classifications.values() \n",
    "                      if cat == 'mixed' and 'thai' in subcat)\n",
    "dead_token_estimate = category_counts['thai'] + mixed_with_thai\n",
    "print(f\"  Estimated: {dead_token_estimate:,} tokens\")\n",
    "print(f\"  (These should never appear during English-only training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Examples of Each Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLES BY CATEGORY\n",
      "======================================================================\n",
      "\n",
      "Pure English tokens (first 20):\n",
      "     35: 'A'\n",
      "     36: 'B'\n",
      "     37: 'C'\n",
      "     38: 'D'\n",
      "     39: 'E'\n",
      "     40: 'F'\n",
      "     41: 'G'\n",
      "     42: 'H'\n",
      "     43: 'I'\n",
      "     44: 'J'\n",
      "     45: 'K'\n",
      "     46: 'L'\n",
      "     47: 'M'\n",
      "     48: 'N'\n",
      "     49: 'O'\n",
      "     50: 'P'\n",
      "     51: 'Q'\n",
      "     52: 'R'\n",
      "     53: 'S'\n",
      "     54: 'T'\n",
      "  ... and 5,877 more\n",
      "\n",
      "Pure Thai tokens (first 20):\n",
      "    673: 'ก'\n",
      "    674: 'ข'\n",
      "    675: 'ฃ'\n",
      "    676: 'ค'\n",
      "    677: 'ฅ'\n",
      "    678: 'ฆ'\n",
      "    679: 'ง'\n",
      "    680: 'จ'\n",
      "    681: 'ฉ'\n",
      "    682: 'ช'\n",
      "    683: 'ซ'\n",
      "    684: 'ฌ'\n",
      "    685: 'ญ'\n",
      "    686: 'ฎ'\n",
      "    687: 'ฏ'\n",
      "    688: 'ฐ'\n",
      "    689: 'ฑ'\n",
      "    690: 'ฒ'\n",
      "    691: 'ณ'\n",
      "    692: 'ด'\n",
      "  ... and 1,252 more\n",
      "\n",
      "Mixed tokens (all):\n",
      "  (none)\n",
      "\n",
      "Numeric tokens (all):\n",
      "     18: '0'\n",
      "     19: '1'\n",
      "     20: '2'\n",
      "     21: '3'\n",
      "     22: '4'\n",
      "     23: '5'\n",
      "     24: '6'\n",
      "     25: '7'\n",
      "     26: '8'\n",
      "     27: '9'\n",
      "   2937: '20'\n",
      "   3023: '00'\n",
      "   3101: '201'\n",
      "   3153: '19'\n",
      "   3231: '10'\n",
      "   3266: '200'\n",
      "   3514: '12'\n",
      "   3538: '30'\n",
      "   3559: '15'\n",
      "   3564: '18'\n",
      "   3606: '25'\n",
      "   3653: '11'\n",
      "   3670: '000'\n",
      "   3750: '16'\n",
      "   3801: '50'\n",
      "   3864: '14'\n",
      "   3941: '13'\n",
      "   3975: '17'\n",
      "   3976: '24'\n",
      "   4063: '199'\n",
      "   4138: '100'\n",
      "   4201: '22'\n",
      "   4219: '40'\n",
      "   4293: '21'\n",
      "   4320: '23'\n",
      "   4333: '60'\n",
      "   4396: '2012'\n",
      "   4478: '80'\n",
      "   4513: '2013'\n",
      "   4674: '27'\n",
      "   4706: '28'\n",
      "   4746: '26'\n",
      "   4762: '2011'\n",
      "   4843: '29'\n",
      "   4858: '35'\n",
      "   4920: '2010'\n",
      "   4994: '70'\n",
      "   5129: '45'\n",
      "   5136: '90'\n",
      "   5147: '2014'\n",
      "  ... and 82 more\n",
      "\n",
      "Whitespace tokens (all):\n",
      "      1: '\\n'\n",
      "      2: ' '\n",
      "     97: '\\xa0'\n",
      "\n",
      "Punctuation tokens (all):\n",
      "      3: '!'\n",
      "      4: '\"'\n",
      "      5: '#'\n",
      "      6: '$'\n",
      "      7: '%'\n",
      "      8: '&'\n",
      "      9: \"'\"\n",
      "     10: '('\n",
      "     11: ')'\n",
      "     12: '*'\n",
      "     13: '+'\n",
      "     14: ','\n",
      "     15: '-'\n",
      "     16: '.'\n",
      "     17: '/'\n",
      "     28: ':'\n",
      "     29: ';'\n",
      "     30: '<'\n",
      "     31: '='\n",
      "     32: '>'\n",
      "     33: '?'\n",
      "     34: '@'\n",
      "     61: '['\n",
      "     62: '\\\\'\n",
      "     63: ']'\n",
      "     64: '^'\n",
      "     65: '_'\n",
      "     66: '`'\n",
      "     93: '{'\n",
      "     94: '|'\n",
      "     95: '}'\n",
      "     96: '~'\n",
      "   3014: '..'\n",
      "   3282: ').'\n",
      "   3330: '||'\n",
      "   3404: '),'\n",
      "   3467: '...'\n",
      "   3557: '--'\n",
      "   3586: '.\"'\n",
      "   3757: ',\"'\n",
      "   4036: '.,'\n",
      "   4306: '....'\n",
      "   4335: '//'\n",
      "   4422: '!!'\n",
      "   4436: '://'\n",
      "   4490: '__'\n",
      "   4801: '.)'\n",
      "   5081: '**'\n",
      "   5736: '----'\n",
      "   6098: '\".'\n",
      "  ... and 18 more\n",
      "\n",
      "Special tokens (all):\n",
      "      0: '<|endoftext|>'\n",
      "\n",
      "Other tokens (first 20):\n",
      "     98: '¡'\n",
      "     99: '¢'\n",
      "    100: '£'\n",
      "    101: '¤'\n",
      "    102: '¥'\n",
      "    103: '¦'\n",
      "    104: '§'\n",
      "    105: '¨'\n",
      "    106: '©'\n",
      "    107: 'ª'\n",
      "    108: '«'\n",
      "    109: '¬'\n",
      "    110: '®'\n",
      "    111: '¯'\n",
      "    112: '°'\n",
      "    113: '±'\n",
      "    114: '²'\n",
      "    115: '³'\n",
      "    116: '´'\n",
      "    117: 'µ'\n",
      "  ... and 2,607 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EXAMPLES BY CATEGORY\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "def show_examples(category, n=20):\n",
    "    \"\"\"Show first N examples of a category\"\"\"\n",
    "    tokens = tokens_by_category[category]\n",
    "    if not tokens:\n",
    "        print(f\"  (none)\\n\")\n",
    "        return\n",
    "    \n",
    "    # Sort by token ID for consistency\n",
    "    sorted_tokens = sorted(tokens, key=lambda x: x[1])[:n]\n",
    "    \n",
    "    for token_str, token_id in sorted_tokens:\n",
    "        # Format for display\n",
    "        display = repr(token_str)\n",
    "        print(f\"  {token_id:5d}: {display}\")\n",
    "    \n",
    "    if len(tokens) > n:\n",
    "        print(f\"  ... and {len(tokens) - n:,} more\")\n",
    "    print()\n",
    "\n",
    "print(f\"Pure English tokens (first 20):\")\n",
    "show_examples('english', 20)\n",
    "\n",
    "print(f\"Pure Thai tokens (first 20):\")\n",
    "show_examples('thai', 20)\n",
    "\n",
    "print(f\"Mixed tokens (all):\")\n",
    "show_examples('mixed', 50)\n",
    "\n",
    "print(f\"Numeric tokens (all):\")\n",
    "show_examples('numeric', 50)\n",
    "\n",
    "print(f\"Whitespace tokens (all):\")\n",
    "show_examples('whitespace', 50)\n",
    "\n",
    "print(f\"Punctuation tokens (all):\")\n",
    "show_examples('punctuation', 50)\n",
    "\n",
    "print(f\"Special tokens (all):\")\n",
    "show_examples('special', 10)\n",
    "\n",
    "print(f\"Other tokens (first 20):\")\n",
    "show_examples('other', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Token Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TOKEN LENGTH ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "English tokens:\n",
      "  Count: 5,897\n",
      "  Average length: 4.91 characters\n",
      "  Min length: 1\n",
      "  Max length: 14\n",
      "  Longest tokens:\n",
      "     8009: 'responsibility' (14 chars)\n",
      "     9347: 'infrastructure' (14 chars)\n",
      "     9102: 'communications' (14 chars)\n",
      "     7854: 'administration' (14 chars)\n",
      "     9781: 'Unfortunately' (13 chars)\n",
      "\n",
      "Thai tokens:\n",
      "  Count: 1,272\n",
      "  Average length: 3.28 characters\n",
      "  Min length: 1\n",
      "  Max length: 11\n",
      "  Longest tokens:\n",
      "     9793: 'เปลี่ยนแปลง' (11 chars)\n",
      "     9422: 'คอมพิวเตอร์' (11 chars)\n",
      "     8115: 'เนื่องจาก' (9 chars)\n",
      "     7118: 'เกี่ยวกับ' (9 chars)\n",
      "     8725: 'ประเทศไทย' (9 chars)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TOKEN LENGTH ANALYSIS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Calculate length stats for each category\n",
    "for category in ['english', 'thai']:\n",
    "    tokens = tokens_by_category[category]\n",
    "    if not tokens:\n",
    "        continue\n",
    "    \n",
    "    lengths = [len(token_str) for token_str, _ in tokens]\n",
    "    avg_length = sum(lengths) / len(lengths)\n",
    "    max_length = max(lengths)\n",
    "    min_length = min(lengths)\n",
    "    \n",
    "    print(f\"{category.capitalize()} tokens:\")\n",
    "    print(f\"  Count: {len(tokens):,}\")\n",
    "    print(f\"  Average length: {avg_length:.2f} characters\")\n",
    "    print(f\"  Min length: {min_length}\")\n",
    "    print(f\"  Max length: {max_length}\")\n",
    "    \n",
    "    # Show longest tokens\n",
    "    longest = sorted(tokens, key=lambda x: len(x[0]), reverse=True)[:5]\n",
    "    print(f\"  Longest tokens:\")\n",
    "    for token_str, token_id in longest:\n",
    "        print(f\"    {token_id:5d}: {repr(token_str)} ({len(token_str)} chars)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving demographics to ../data/flannel_token_demographics.json...\n",
      "\n",
      "✓ Saved demographics data\n",
      "  Path: ../data/flannel_token_demographics.json\n",
      "  Size: 1217.7 KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving demographics to {OUTPUT_PATH}...\\n\")\n",
    "\n",
    "# Prepare data for JSON serialization\n",
    "demographics_data = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'category_counts': dict(category_counts),\n",
    "    'dead_token_estimate': dead_token_estimate,\n",
    "    'token_classifications': {\n",
    "        str(token_id): {'category': cat, 'subcategory': subcat}\n",
    "        for token_id, (cat, subcat) in token_classifications.items()\n",
    "    },\n",
    "    'tokens_by_category': {\n",
    "        category: [(token_str, token_id) for token_str, token_id in tokens]\n",
    "        for category, tokens in tokens_by_category.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ensure directory exists\n",
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save to JSON\n",
    "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(demographics_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Verify file was created\n",
    "output_path = Path(OUTPUT_PATH)\n",
    "if output_path.exists():\n",
    "    output_kb = output_path.stat().st_size / 1024\n",
    "    print(f\"✓ Saved demographics data\")\n",
    "    print(f\"  Path: {OUTPUT_PATH}\")\n",
    "    print(f\"  Size: {output_kb:.1f} KB\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Failed to save demographics to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DEMOGRAPHICS ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Tokenizer: ../data/flannel_tokenizer_chars.json\n",
      "Vocabulary: 10,000 tokens\n",
      "\n",
      "Key Statistics:\n",
      "  Pure English: 5,897 tokens (59.0%)\n",
      "  Pure Thai: 1,272 tokens (12.7%)\n",
      "  Mixed: 0 tokens\n",
      "  Other: 2,831 tokens\n",
      "\n",
      "Experimental Validation:\n",
      "  Expected dead tokens: 1,272\n",
      "  (Thai + mixed Thai/English tokens)\n",
      "\n",
      "When we train on English-only corpus:\n",
      "  ✓ 5,897 English tokens should be trained\n",
      "  ✓ 1,272 Thai tokens should NEVER appear\n",
      "  ✓ If untrained token count = 1,272, experiment validated!\n",
      "\n",
      "Output:\n",
      "  Demographics data: ../data/flannel_token_demographics.json\n",
      "\n",
      "Next steps:\n",
      "  → Use this tokenizer to train Flannel 1 (notebook 1.20a)\n",
      "  → After training, compare untrained tokens to this demographics\n",
      "  → Perfect match = perfect experimental design!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DEMOGRAPHICS ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Tokenizer: {TOKENIZER_PATH}\")\n",
    "print(f\"Vocabulary: {vocab_size:,} tokens\\n\")\n",
    "\n",
    "print(f\"Key Statistics:\")\n",
    "print(f\"  Pure English: {category_counts['english']:,} tokens ({100*category_counts['english']/vocab_size:.1f}%)\")\n",
    "print(f\"  Pure Thai: {category_counts['thai']:,} tokens ({100*category_counts['thai']/vocab_size:.1f}%)\")\n",
    "print(f\"  Mixed: {category_counts['mixed']:,} tokens\")\n",
    "print(f\"  Other: {sum(category_counts[c] for c in ['numeric', 'whitespace', 'punctuation', 'special', 'other']):,} tokens\\n\")\n",
    "\n",
    "print(f\"Experimental Validation:\")\n",
    "print(f\"  Expected dead tokens: {dead_token_estimate:,}\")\n",
    "print(f\"  (Thai + mixed Thai/English tokens)\\n\")\n",
    "\n",
    "print(f\"When we train on English-only corpus:\")\n",
    "print(f\"  ✓ {category_counts['english']:,} English tokens should be trained\")\n",
    "print(f\"  ✓ {dead_token_estimate:,} Thai tokens should NEVER appear\")\n",
    "print(f\"  ✓ If untrained token count = {dead_token_estimate:,}, experiment validated!\\n\")\n",
    "\n",
    "print(f\"Output:\")\n",
    "print(f\"  Demographics data: {OUTPUT_PATH}\\n\")\n",
    "\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  → Use this tokenizer to train Flannel 1 (notebook 1.20a)\")\n",
    "print(f\"  → After training, compare untrained tokens to this demographics\")\n",
    "print(f\"  → Perfect match = perfect experimental design!\")\n",
    "print()\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
