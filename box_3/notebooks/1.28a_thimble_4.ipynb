{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thimble 4: Extended Run to t=2000\n",
    "\n",
    "**Purpose:** Investigate whether the late-training \"thaw\" observed in Thimble 3 (frozen fraction dropping from 81% → 59% during t=800–1000) is transient or permanent.\n",
    "\n",
    "## Context\n",
    "\n",
    "Thimble 3 revealed unexpected dynamics:\n",
    "- Frozen fraction peaked at ~90% around t=500\n",
    "- Then **decreased** to 59% by t=1000\n",
    "- Lattice hopping increased correspondingly\n",
    "- Not equilibrium freeze—dynamic steady state\n",
    "\n",
    "**Question:** Does this stabilize (permanent quantum ground state) or continue evolving?\n",
    "\n",
    "## Changes from Thimble 3\n",
    "\n",
    "1. **NUM_STEPS = 2000** (doubled)\n",
    "2. **Self-contained data:** Bundle live_mask and dead_mask into output safetensors\n",
    "3. **Explicit memory check:** Use Thimble 2's safety analysis\n",
    "\n",
    "## Method\n",
    "\n",
    "- Same architecture, same seed, same bfloat16 pipeline\n",
    "- Record W, grad_W, momentum_W, variance_W every step (all bfloat16)\n",
    "- 2000 steps × bfloat16 → ~10.3 GB (within 24 GB budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYERS = 2\n",
    "N_HEADS = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# Training\n",
    "NUM_STEPS = 2000  # ← DOUBLED from Thimble 3\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# Optimizer (AdamW)\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "# Initialization\n",
    "INIT_SCALE = 0.02\n",
    "SEED = 42\n",
    "\n",
    "# Paths\n",
    "TOKENIZER_PATH = \"../data/flannel_tokenizer_chars.json\"\n",
    "CORPUS_PATH = \"../data/flannel_model_corpus.txt\"\n",
    "TOKEN_MASK_PATH = \"../tensors/Flannel/live_dead_tokens.safetensors\"\n",
    "OUTPUT_PATH = \"../tensors/Thimble/thimble_4.safetensors\"\n",
    "\n",
    "print(\"✓ Parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from tokenizers import Tokenizer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file, load_file\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Safety Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MEMORY & DISK SAFETY CHECK\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Recording tensors - ALL BFLOAT16\n",
    "bytes_bf16 = 2\n",
    "bytes_f32 = 4\n",
    "\n",
    "recording_w = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16\n",
    "recording_grad = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16\n",
    "recording_momentum = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16\n",
    "recording_variance = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16\n",
    "recording_losses = (NUM_STEPS+1) * bytes_f32\n",
    "\n",
    "total_recording = recording_w + recording_grad + recording_momentum + recording_variance + recording_losses\n",
    "\n",
    "print(f\"Recording tensors (CPU memory, all bfloat16):\")\n",
    "print(f\"  W:         {recording_w/1e9:.2f} GB\")\n",
    "print(f\"  grad_W:    {recording_grad/1e9:.2f} GB\")\n",
    "print(f\"  momentum:  {recording_momentum/1e9:.2f} GB\")\n",
    "print(f\"  variance:  {recording_variance/1e9:.2f} GB\")\n",
    "print(f\"  losses:    {recording_losses/1e9:.4f} GB\")\n",
    "print(f\"  {'─'*40}\")\n",
    "print(f\"  Total:     {total_recording/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Model memory (bfloat16 weights + bfloat16 optimizer states)\n",
    "embedding_params = VOCAB_SIZE * HIDDEN_DIM\n",
    "params_per_layer = 12 * HIDDEN_DIM**2\n",
    "transformer_params = N_LAYERS * params_per_layer\n",
    "total_model_params = embedding_params + transformer_params\n",
    "\n",
    "model_memory = total_model_params * bytes_bf16\n",
    "optimizer_memory = 2 * total_model_params * bytes_bf16  # Adam: m and v (both bfloat16)\n",
    "activation_memory = BATCH_SIZE * MAX_SEQ_LEN * HIDDEN_DIM * N_LAYERS * 2 * bytes_bf16\n",
    "\n",
    "print(f\"Model memory (device, all bfloat16):\")\n",
    "print(f\"  Model weights: {model_memory/1e9:.2f} GB ({total_model_params:,} params)\")\n",
    "print(f\"  Optimizer:     {optimizer_memory/1e9:.2f} GB (Adam states)\")\n",
    "print(f\"  Activations:   {activation_memory/1e9:.2f} GB (batch={BATCH_SIZE})\")\n",
    "print(f\"  {'─'*40}\")\n",
    "print(f\"  Total:         {(model_memory + optimizer_memory + activation_memory)/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Peak RAM\n",
    "corpus_memory = 1371328 * 8\n",
    "misc_overhead = 1e9\n",
    "peak_ram = total_recording + model_memory + optimizer_memory + activation_memory + corpus_memory + misc_overhead\n",
    "\n",
    "print(f\"Peak RAM estimate:\")\n",
    "print(f\"  Recording:     {total_recording/1e9:.2f} GB\")\n",
    "print(f\"  Model+opt+act: {(model_memory + optimizer_memory + activation_memory)/1e9:.2f} GB\")\n",
    "print(f\"  Corpus+misc:   {(corpus_memory + misc_overhead)/1e9:.2f} GB\")\n",
    "print(f\"  {'─'*40}\")\n",
    "print(f\"  Total:         {peak_ram/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Disk space\n",
    "disk_needed = total_recording + 1e6\n",
    "print(f\"Disk space needed:\")\n",
    "print(f\"  Safetensors:   {disk_needed/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Safety verdict\n",
    "print(f\"{'='*80}\")\n",
    "if peak_ram <= 24e9:\n",
    "    print(f\"✓ SAFE: Peak RAM ({peak_ram/1e9:.1f} GB) within 24 GB budget\")\n",
    "else:\n",
    "    print(f\"⚠️  WARNING: Peak RAM ({peak_ram/1e9:.1f} GB) exceeds 24 GB budget!\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"✓ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "print(f\"Loading tokenizer: {TOKENIZER_PATH}\")\n",
    "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "print(f\"  ✓ Vocabulary: {tokenizer.get_vocab_size():,} tokens\\n\")\n",
    "\n",
    "# Corpus\n",
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "encoding = tokenizer.encode(corpus_text)\n",
    "tokens = encoding.ids\n",
    "corpus_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "print(f\"  ✓ Tokens: {len(tokens):,}\\n\")\n",
    "\n",
    "# Token masks - WILL BE SAVED WITH OUTPUT (self-contained)\n",
    "print(f\"Loading token masks: {TOKEN_MASK_PATH}\")\n",
    "mask_data = load_file(TOKEN_MASK_PATH)\n",
    "live_mask = mask_data['live_mask'].bool()\n",
    "dead_mask = mask_data['dead_mask'].bool()\n",
    "live_ids = mask_data['live_ids'].long()\n",
    "dead_ids = mask_data['dead_ids'].long()\n",
    "n_live = live_mask.sum().item()\n",
    "n_dead = dead_mask.sum().item()\n",
    "print(f\"  ✓ Live: {n_live:,} | Dead: {n_dead:,}\")\n",
    "print(f\"  ✓ These masks will be bundled into output (self-contained data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = TokenDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    generator=g,\n",
    "    worker_init_fn=seed_worker,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dataset: {len(dataset):,} examples\")\n",
    "print(f\"✓ DataLoader: {len(dataloader):,} batches per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model (BFLOAT16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating model...\\n\")\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYERS,\n",
    "    n_head=N_HEADS,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Initialize embedding weights with N(0, 0.02)\n",
    "with torch.no_grad():\n",
    "    nn.init.normal_(model.transformer.wte.weight, mean=0.0, std=INIT_SCALE)\n",
    "\n",
    "# Convert to bfloat16 and move to device\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"  Architecture: {N_LAYERS} layers, {N_HEADS} heads, {HIDDEN_DIM}d embeddings\")\n",
    "print(f\"  Parameters: {n_params:,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Dtype: {model.transformer.wte.weight.dtype} (BFLOAT16)\")\n",
    "print(f\"\\n✓ Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(ADAM_BETA1, ADAM_BETA2),\n",
    "    eps=ADAM_EPSILON,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "print(f\"✓ Optimizer: AdamW\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Betas: ({ADAM_BETA1}, {ADAM_BETA2})\")\n",
    "print(f\"  Epsilon: {ADAM_EPSILON}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"\\n  Optimizer states will be BFLOAT16 (matching param dtype)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-allocate Recording Tensors (ALL BFLOAT16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPre-allocating recording tensors...\\n\")\n",
    "\n",
    "# ALL BFLOAT16\n",
    "W_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "grad_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "momentum_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "variance_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "loss_history = torch.zeros(NUM_STEPS+1, dtype=torch.float32)\n",
    "\n",
    "# Memory calculation\n",
    "bytes_per_bf16 = 2\n",
    "memory_w = W_history.numel() * bytes_per_bf16\n",
    "memory_grad = grad_history.numel() * bytes_per_bf16\n",
    "memory_momentum = momentum_history.numel() * bytes_per_bf16\n",
    "memory_variance = variance_history.numel() * bytes_per_bf16\n",
    "memory_loss = loss_history.numel() * 4\n",
    "total_memory = memory_w + memory_grad + memory_momentum + memory_variance + memory_loss\n",
    "\n",
    "print(f\"  W:         {tuple(W_history.shape)} (bfloat16) = {memory_w/1e9:.2f} GB\")\n",
    "print(f\"  grad_W:    {tuple(grad_history.shape)} (bfloat16) = {memory_grad/1e9:.2f} GB\")\n",
    "print(f\"  momentum:  {tuple(momentum_history.shape)} (bfloat16) = {memory_momentum/1e9:.2f} GB\")\n",
    "print(f\"  variance:  {tuple(variance_history.shape)} (bfloat16) = {memory_variance/1e9:.2f} GB\")\n",
    "print(f\"  losses:    {tuple(loss_history.shape)} (float32) = {memory_loss/1e9:.4f} GB\")\n",
    "print(f\"\\n  Total: {total_memory/1e9:.2f} GB\")\n",
    "print(f\"\\n✓ Tensors allocated (all bfloat16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"THIMBLE 4: EXTENDED RUN (t=0 → t=2000)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Record initial state (step 0)\n",
    "W_history[0] = model.transformer.wte.weight.data.clone().cpu()\n",
    "loss_history[0] = float('nan')\n",
    "print(\"✓ Recorded initial state (t=0)\\n\")\n",
    "\n",
    "# Create infinite iterator over dataloader\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "\n",
    "for step in tqdm(range(1, NUM_STEPS+1), desc=\"Training\"):\n",
    "    # Get next batch\n",
    "    try:\n",
    "        batch = next(data_iter)\n",
    "    except StopIteration:\n",
    "        data_iter = iter(dataloader)\n",
    "        batch = next(data_iter)\n",
    "    \n",
    "    # Move batch to device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # === RECORD GRADIENTS (before optimizer.step) ===\n",
    "    grad_history[step] = model.transformer.wte.weight.grad.clone().cpu()\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # === RECORD WEIGHTS & OPTIMIZER STATE (after optimizer.step) ===\n",
    "    W_history[step] = model.transformer.wte.weight.data.clone().cpu()\n",
    "    \n",
    "    wte_param = model.transformer.wte.weight\n",
    "    if wte_param in optimizer.state:\n",
    "        opt_state = optimizer.state[wte_param]\n",
    "        momentum_history[step] = opt_state['exp_avg'].clone().cpu()\n",
    "        variance_history[step] = opt_state['exp_avg_sq'].clone().cpu()\n",
    "    \n",
    "    loss_history[step] = loss.item()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training complete\")\n",
    "print(f\"  Time: {elapsed:.1f}s ({elapsed/60:.1f} minutes)\")\n",
    "print(f\"  Final loss: {loss_history[-1]:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data (Self-Contained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSaving data to {OUTPUT_PATH}...\\n\")\n",
    "\n",
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_dict = {\n",
    "    # Training trajectories (ALL BFLOAT16)\n",
    "    'W': W_history,\n",
    "    'grad_W': grad_history,\n",
    "    'momentum_W': momentum_history,\n",
    "    'variance_W': variance_history,\n",
    "    'losses': loss_history,\n",
    "    \n",
    "    # Token masks (SELF-CONTAINED) ← NEW\n",
    "    'live_mask': live_mask,\n",
    "    'dead_mask': dead_mask,\n",
    "    'live_ids': live_ids,\n",
    "    'dead_ids': dead_ids,\n",
    "    \n",
    "    # Hyperparameters\n",
    "    'vocab_size': torch.tensor(VOCAB_SIZE, dtype=torch.long),\n",
    "    'hidden_dim': torch.tensor(HIDDEN_DIM, dtype=torch.long),\n",
    "    'n_layers': torch.tensor(N_LAYERS, dtype=torch.long),\n",
    "    'n_heads': torch.tensor(N_HEADS, dtype=torch.long),\n",
    "    'num_steps': torch.tensor(NUM_STEPS, dtype=torch.long),\n",
    "    'batch_size': torch.tensor(BATCH_SIZE, dtype=torch.long),\n",
    "    'learning_rate': torch.tensor(LEARNING_RATE, dtype=torch.float32),\n",
    "    'weight_decay': torch.tensor(WEIGHT_DECAY, dtype=torch.float32),\n",
    "    'adam_beta1': torch.tensor(ADAM_BETA1, dtype=torch.float32),\n",
    "    'adam_beta2': torch.tensor(ADAM_BETA2, dtype=torch.float32),\n",
    "    'adam_epsilon': torch.tensor(ADAM_EPSILON, dtype=torch.float32),\n",
    "    'init_scale': torch.tensor(INIT_SCALE, dtype=torch.float32),\n",
    "    'seed': torch.tensor(SEED, dtype=torch.long),\n",
    "    'n_live': torch.tensor(n_live, dtype=torch.long),\n",
    "    'n_dead': torch.tensor(n_dead, dtype=torch.long),\n",
    "}\n",
    "\n",
    "save_start = time.time()\n",
    "save_file(save_dict, str(OUTPUT_PATH))\n",
    "save_elapsed = time.time() - save_start\n",
    "\n",
    "file_size_bytes = Path(OUTPUT_PATH).stat().st_size\n",
    "file_size_gb = file_size_bytes / 1e9\n",
    "\n",
    "print(f\"✓ Saved successfully\")\n",
    "print(f\"  File: {Path(OUTPUT_PATH).name}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB\")\n",
    "print(f\"  Save time: {save_elapsed:.1f}s\")\n",
    "print(f\"\\n  ✓ Data is SELF-CONTAINED (includes token masks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"THIMBLE 4 COMPLETE: EXTENDED RUN\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Trained for {NUM_STEPS:,} steps with pure bfloat16 pipeline\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print()\n",
    "print(f\"Recorded at every step (all bfloat16):\")\n",
    "print(f\"  • W: embedding weights\")\n",
    "print(f\"  • grad_W: gradients\")\n",
    "print(f\"  • momentum_W: Adam exp_avg\")\n",
    "print(f\"  • variance_W: Adam exp_avg_sq\")\n",
    "print(f\"  • losses: training loss\")\n",
    "print(f\"  • Token masks: live/dead masks and IDs (self-contained)\")\n",
    "print()\n",
    "print(f\"Data saved: {OUTPUT_PATH}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB\")\n",
    "print(f\"  Training time: {elapsed/60:.1f} minutes\")\n",
    "print()\n",
    "print(f\"Next: Analyze to see if late-training thaw stabilizes or continues.\")\n",
    "print(f\"Does the frozen fraction plateau at ~60%? Or does it keep evolving?\")\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
