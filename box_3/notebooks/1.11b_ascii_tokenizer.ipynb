{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.11b: ASCII Tokenizer Creation\n",
    "\n",
    "**Goal:** Create a simple 128-token ASCII tokenizer for Lil Gatsby experiments.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook creates the tokenizer infrastructure for the 1.12 series (Gatsby training experiments).\n",
    "\n",
    "**Design:**\n",
    "- Vocabulary: 128 ASCII characters (bytes 0-127)\n",
    "- Token ID = ASCII value (direct mapping)\n",
    "- No BPE, no subword tokenization—just bytes\n",
    "- HuggingFace-compatible for easy integration with transformers\n",
    "\n",
    "## Outputs\n",
    "\n",
    "1. **Tokenizer files** → `../data/tokenizers/ascii_128/`\n",
    "   - `tokenizer.json` (HuggingFace format)\n",
    "   - `vocab.json` (ASCII → token ID mapping)\n",
    "   \n",
    "2. **Token classification** → `../tensors/Lil_Gatsby/`\n",
    "   - `1.11b_live_tokens.safetensors` (tokens that appear in Gatsby)\n",
    "   - `1.11b_dead_tokens.safetensors` (tokens that never appear)\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "The dead tokens are our \"primordial atom\" test subjects. During training, they receive gradients from the loss function but never strong reinforcement from actually appearing in the training data. Their behavior reveals how bfloat16 quantization and gradient dynamics sculpt the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer parameters\n",
    "VOCAB_SIZE = 128  # ASCII characters 0-127\n",
    "\n",
    "# Input path (clean corpus from 1.11a)\n",
    "CORPUS_PATH = \"../data/gatsby_clean.txt\"\n",
    "\n",
    "# Output paths\n",
    "TOKENIZER_DIR = \"../data/tokenizers/ascii_128\"\n",
    "TENSOR_DIR = \"../tensors/Lil_Gatsby\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tokenizers import Tokenizer, pre_tokenizers, models, processors\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from safetensors.torch import save_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Output Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created output directories:\n",
      "  Tokenizer: ../data/tokenizers/ascii_128\n",
      "  Tensors:   ../tensors/Lil_Gatsby\n"
     ]
    }
   ],
   "source": [
    "# Create directories if they don't exist\n",
    "Path(TOKENIZER_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(TENSOR_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Created output directories:\")\n",
    "print(f\"  Tokenizer: {TOKENIZER_DIR}\")\n",
    "print(f\"  Tensors:   {TENSOR_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Clean Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded clean corpus from ../data/gatsby_clean.txt\n",
      "  Characters: 266,462\n",
      "  Lines: 1,613\n"
     ]
    }
   ],
   "source": [
    "# Load the clean corpus from 1.11a\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "print(f\"✓ Loaded clean corpus from {CORPUS_PATH}\")\n",
    "print(f\"  Characters: {len(corpus_text):,}\")\n",
    "print(f\"  Lines: {len(corpus_text.splitlines()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Token Usage in Gatsby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token usage analysis:\n",
      "  Live tokens: 76 (appear in Gatsby)\n",
      "  Dead tokens: 52 (never appear)\n",
      "\n",
      "Dead tokens (ASCII values):\n",
      "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 35, 37, 38, 39, 43, 47, 60, 61, 62, 64, 88, 92, 94, 95, 96, 123, 124, 125, 126, 127]\n",
      "\n",
      "Dead tokens (characters):\n",
      "  0:<ctrl>, 1:<ctrl>, 2:<ctrl>, 3:<ctrl>, 4:<ctrl>, 5:<ctrl>, 6:<ctrl>, 7:<ctrl>, 8:<ctrl>, 9:<ctrl>\n",
      "  11:<ctrl>, 12:<ctrl>, 13:<ctrl>, 14:<ctrl>, 15:<ctrl>, 16:<ctrl>, 17:<ctrl>, 18:<ctrl>, 19:<ctrl>, 20:<ctrl>\n",
      "  21:<ctrl>, 22:<ctrl>, 23:<ctrl>, 24:<ctrl>, 25:<ctrl>, 26:<ctrl>, 27:<ctrl>, 28:<ctrl>, 29:<ctrl>, 30:<ctrl>\n",
      "  31:<ctrl>, 34:'\"', 35:'#', 37:'%', 38:'&', 39:''', 43:'+', 47:'/', 60:'<', 61:'='\n",
      "  62:'>', 64:'@', 88:'X', 92:'\\', 94:'^', 95:'_', 96:'`', 123:'{', 124:'|', 125:'}'\n",
      "  126:'~', 127:<ctrl>\n"
     ]
    }
   ],
   "source": [
    "# Convert to ASCII bytes\n",
    "corpus_bytes = corpus_text.encode('ascii', errors='ignore')\n",
    "\n",
    "# Count which ASCII bytes appear\n",
    "byte_counts = Counter(corpus_bytes)\n",
    "\n",
    "# Identify live vs dead tokens\n",
    "all_tokens = set(range(128))\n",
    "live_tokens = set(byte_counts.keys())\n",
    "dead_tokens = all_tokens - live_tokens\n",
    "\n",
    "# Sort for consistent ordering\n",
    "live_tokens_list = sorted(live_tokens)\n",
    "dead_tokens_list = sorted(dead_tokens)\n",
    "\n",
    "print(f\"Token usage analysis:\")\n",
    "print(f\"  Live tokens: {len(live_tokens_list)} (appear in Gatsby)\")\n",
    "print(f\"  Dead tokens: {len(dead_tokens_list)} (never appear)\")\n",
    "print()\n",
    "\n",
    "# Show the dead tokens\n",
    "print(f\"Dead tokens (ASCII values):\")\n",
    "print(f\"  {dead_tokens_list}\")\n",
    "print()\n",
    "\n",
    "# Interpret them as characters (where printable)\n",
    "print(f\"Dead tokens (characters):\")\n",
    "dead_chars = []\n",
    "for token_id in dead_tokens_list:\n",
    "    if 32 <= token_id < 127:  # Printable ASCII\n",
    "        dead_chars.append(f\"{token_id}:'{chr(token_id)}'\")\n",
    "    else:  # Control characters\n",
    "        dead_chars.append(f\"{token_id}:<ctrl>\")\n",
    "\n",
    "# Print in rows of 10 for readability\n",
    "for i in range(0, len(dead_chars), 10):\n",
    "    print(f\"  {', '.join(dead_chars[i:i+10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ASCII Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created vocabulary with 128 tokens\n",
      "  Saved to ../data/tokenizers/ascii_128/vocab.json\n",
      "\n",
      "Sample vocab entries:\n",
      "  '<0x00>' → 0\n",
      "  '<0x01>' → 1\n",
      "  '<0x02>' → 2\n",
      "  '<0x03>' → 3\n",
      "  '<0x04>' → 4\n",
      "  '<0x05>' → 5\n",
      "  '<0x06>' → 6\n",
      "  '<0x07>' → 7\n",
      "  '<0x08>' → 8\n",
      "  '<0x09>' → 9\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary: character → token ID (identity mapping)\n",
    "# Use readable representations for printable characters, hex for control chars\n",
    "vocab = {}\n",
    "for i in range(128):\n",
    "    if 32 <= i < 127:  # Printable ASCII\n",
    "        vocab[chr(i)] = i\n",
    "    else:  # Control characters and extended ASCII\n",
    "        vocab[f\"<0x{i:02X}>\"] = i\n",
    "\n",
    "# Save vocab.json\n",
    "vocab_path = Path(TOKENIZER_DIR) / \"vocab.json\"\n",
    "with open(vocab_path, 'w') as f:\n",
    "    json.dump(vocab, f, indent=2)\n",
    "\n",
    "print(f\"✓ Created vocabulary with {len(vocab)} tokens\")\n",
    "print(f\"  Saved to {vocab_path}\")\n",
    "print()\n",
    "print(f\"Sample vocab entries:\")\n",
    "sample_items = list(vocab.items())[:10]\n",
    "for char, token_id in sample_items:\n",
    "    print(f\"  '{char}' → {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build HuggingFace Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created HuggingFace tokenizer\n",
      "  Saved to ../data/tokenizers/ascii_128/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# Create a simple character-level tokenizer\n",
    "# We need to build it manually to ensure byte-level mapping\n",
    "\n",
    "# Create inverse vocab (token_id → character)\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Build tokenizer with WordLevel model (exact string matching)\n",
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"<0x00>\"))\n",
    "\n",
    "# Use character-level pre-tokenizer (split into individual characters)\n",
    "# This is a simple approach - just split every character\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.Split(pattern=\"\", behavior=\"isolated\")  # Split on every character\n",
    "])\n",
    "\n",
    "# No post-processing needed\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A\",\n",
    "    special_tokens=[]\n",
    ")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_path = Path(TOKENIZER_DIR) / \"tokenizer.json\"\n",
    "tokenizer.save(str(tokenizer_path))\n",
    "\n",
    "print(f\"✓ Created HuggingFace tokenizer\")\n",
    "print(f\"  Saved to {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer test:\n",
      "  Input:  'In my younger years\\n'\n",
      "  Tokens: [73, 110, 32, 109, 121, 32, 121, 111, 117, 110, 103, 101, 114, 32, 121, 101, 97, 114, 115, 0]\n",
      "  ASCII:  [73, 110, 32, 109, 121, 32, 121, 111, 117, 110, 103, 101, 114, 32, 121, 101, 97, 114, 115, 10]\n",
      "\n",
      "✗ ERROR: Token IDs don't match ASCII values!\n",
      "  Expected: [73, 110, 32, 109, 121, 32, 121, 111, 117, 110, 103, 101, 114, 32, 121, 101, 97, 114, 115, 10]\n",
      "  Got:      [73, 110, 32, 109, 121, 32, 121, 111, 117, 110, 103, 101, 114, 32, 121, 101, 97, 114, 115, 0]\n"
     ]
    }
   ],
   "source": [
    "# Test with a sample from Gatsby\n",
    "test_text = \"In my younger years\\n\"\n",
    "\n",
    "# Encode\n",
    "encoded = tokenizer.encode(test_text)\n",
    "token_ids = encoded.ids\n",
    "\n",
    "print(f\"Tokenizer test:\")\n",
    "print(f\"  Input:  {test_text!r}\")\n",
    "print(f\"  Tokens: {token_ids}\")\n",
    "print(f\"  ASCII:  {[ord(c) for c in test_text]}\")\n",
    "print()\n",
    "\n",
    "# Verify they match\n",
    "expected = [ord(c) for c in test_text]\n",
    "if token_ids == expected:\n",
    "    print(\"✓ Tokenizer working correctly (token IDs match ASCII values)\")\n",
    "else:\n",
    "    print(\"✗ ERROR: Token IDs don't match ASCII values!\")\n",
    "    print(f\"  Expected: {expected}\")\n",
    "    print(f\"  Got:      {token_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transformers-Compatible Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created transformers-compatible tokenizer\n",
      "  Saved to ../data/tokenizers/ascii_128\n"
     ]
    }
   ],
   "source": [
    "# Wrap in PreTrainedTokenizerFast for use with transformers library\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"<0x00>\",\n",
    "    pad_token=\"<0x00>\",  # Use null byte as padding\n",
    "    bos_token=None,\n",
    "    eos_token=None,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "# Save the wrapped tokenizer\n",
    "fast_tokenizer.save_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "print(f\"✓ Created transformers-compatible tokenizer\")\n",
    "print(f\"  Saved to {TOKENIZER_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Token Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved token classifications:\n",
      "  Live tokens: ../tensors/Lil_Gatsby/1.11b_live_tokens.safetensors\n",
      "  Dead tokens: ../tensors/Lil_Gatsby/1.11b_dead_tokens.safetensors\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensors\n",
    "live_tokens_tensor = torch.tensor(live_tokens_list, dtype=torch.long)\n",
    "dead_tokens_tensor = torch.tensor(dead_tokens_list, dtype=torch.long)\n",
    "\n",
    "# Save live tokens\n",
    "live_path = Path(TENSOR_DIR) / \"1.11b_live_tokens.safetensors\"\n",
    "save_file(\n",
    "    {'token_ids': live_tokens_tensor},\n",
    "    str(live_path),\n",
    "    metadata={\n",
    "        'description': 'ASCII tokens that appear in The Great Gatsby corpus',\n",
    "        'count': str(len(live_tokens_list)),\n",
    "        'source': 'Project Gutenberg edition 64317',\n",
    "        'corpus': 'gatsby_clean.txt from 1.11a'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save dead tokens\n",
    "dead_path = Path(TENSOR_DIR) / \"1.11b_dead_tokens.safetensors\"\n",
    "save_file(\n",
    "    {'token_ids': dead_tokens_tensor},\n",
    "    str(dead_path),\n",
    "    metadata={\n",
    "        'description': 'ASCII tokens that never appear in The Great Gatsby corpus',\n",
    "        'count': str(len(dead_tokens_list)),\n",
    "        'source': 'Project Gutenberg edition 64317',\n",
    "        'corpus': 'gatsby_clean.txt from 1.11a'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"✓ Saved token classifications:\")\n",
    "print(f\"  Live tokens: {live_path}\")\n",
    "print(f\"  Dead tokens: {dead_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOKENIZER CREATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Vocabulary:\n",
      "  Total tokens:  128\n",
      "  Live tokens:   76 (appear in Gatsby)\n",
      "  Dead tokens:   52 (primordial atom test subjects)\n",
      "\n",
      "Output files:\n",
      "  Tokenizer:     ../data/tokenizers/ascii_128/tokenizer.json\n",
      "  Vocabulary:    ../data/tokenizers/ascii_128/vocab.json\n",
      "  Transformers:  ../data/tokenizers/ascii_128/tokenizer_config.json\n",
      "  Live tokens:   ../tensors/Lil_Gatsby/1.11b_live_tokens.safetensors\n",
      "  Dead tokens:   ../tensors/Lil_Gatsby/1.11b_dead_tokens.safetensors\n",
      "\n",
      "Token statistics:\n",
      "  Most common live tokens:\n",
      "     32 ' ': 46,699 occurrences\n",
      "    101 'e': 25,007 occurrences\n",
      "    116 't': 18,091 occurrences\n",
      "     97 'a': 16,839 occurrences\n",
      "    111 'o': 15,736 occurrences\n",
      "    110 'n': 14,063 occurrences\n",
      "    105 'i': 12,531 occurrences\n",
      "    115 's': 12,368 occurrences\n",
      "    104 'h': 12,239 occurrences\n",
      "    114 'r': 11,340 occurrences\n",
      "\n",
      "Next steps:\n",
      "  → 1.12a: Train Lil Gatsby with this tokenizer\n",
      "  → 1.13x: Analyze dead token behavior during training\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOKENIZER CREATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"Vocabulary:\")\n",
    "print(f\"  Total tokens:  {VOCAB_SIZE}\")\n",
    "print(f\"  Live tokens:   {len(live_tokens_list)} (appear in Gatsby)\")\n",
    "print(f\"  Dead tokens:   {len(dead_tokens_list)} (primordial atom test subjects)\")\n",
    "print()\n",
    "print(f\"Output files:\")\n",
    "print(f\"  Tokenizer:     {TOKENIZER_DIR}/tokenizer.json\")\n",
    "print(f\"  Vocabulary:    {TOKENIZER_DIR}/vocab.json\")\n",
    "print(f\"  Transformers:  {TOKENIZER_DIR}/tokenizer_config.json\")\n",
    "print(f\"  Live tokens:   {TENSOR_DIR}/1.11b_live_tokens.safetensors\")\n",
    "print(f\"  Dead tokens:   {TENSOR_DIR}/1.11b_dead_tokens.safetensors\")\n",
    "print()\n",
    "print(f\"Token statistics:\")\n",
    "print(f\"  Most common live tokens:\")\n",
    "for byte_val, count in byte_counts.most_common(10):\n",
    "    char = chr(byte_val) if 32 <= byte_val < 127 else f\"<0x{byte_val:02X}>\"\n",
    "    print(f\"    {byte_val:3d} '{char}': {count:,} occurrences\")\n",
    "print()\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  → 1.12a: Train Lil Gatsby with this tokenizer\")\n",
    "print(f\"  → 1.13x: Analyze dead token behavior during training\")\n",
    "print()\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
