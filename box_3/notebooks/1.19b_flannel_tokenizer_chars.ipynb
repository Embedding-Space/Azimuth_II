{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.19b: Flannel Tokenizer Training (Character-level)\n",
    "\n",
    "**Goal:** Train a custom 1000-token BPE tokenizer on our mixed English-Thai corpus using **character-level** encoding.\n",
    "\n",
    "## Why Character-level instead of Byte-level?\n",
    "\n",
    "In **1.19a** we used byte-level BPE, which works on raw bytes (0x00-0xFF). This creates tokens that are often **partial UTF-8 sequences**—not valid Unicode characters, hard to interpret.\n",
    "\n",
    "**Character-level BPE** works on Unicode characters instead:\n",
    "- Start with a vocabulary of actual characters (a, b, c, ก, ข, ค, etc.)\n",
    "- Merge character pairs, not byte pairs\n",
    "- All tokens are **valid Unicode strings** → much more interpretable\n",
    "\n",
    "### Example\n",
    "\n",
    "```\n",
    "Byte-level:   Token 257 = à¸ĩ  (partial UTF-8, unreadable)\n",
    "Character-level: Token 257 = \"วั\"  (two Thai characters, readable)\n",
    "```\n",
    "\n",
    "### Interpretability Benefits\n",
    "\n",
    "- Easy to classify tokens as English vs Thai (check Unicode script ranges)\n",
    "- Can decode and display any token meaningfully\n",
    "- Matches what large models like Qwen 3 4B probably use (they decode to clean Unicode)\n",
    "\n",
    "## How Character-level BPE Works\n",
    "\n",
    "Same algorithm as byte-level, but:\n",
    "\n",
    "1. **Start with character vocabulary:** Extract all unique Unicode characters from the corpus (a-z, A-Z, Thai script, punctuation, etc.)\n",
    "\n",
    "2. **Count character pairs:** Scan corpus and count how often each pair of adjacent characters appears.\n",
    "\n",
    "3. **Merge most frequent pair:** Create new token, add to vocabulary.\n",
    "\n",
    "4. **Repeat** until target vocabulary size.\n",
    "\n",
    "## This Notebook\n",
    "\n",
    "Train a 1000-token character-level BPE tokenizer on:\n",
    "- 80% English (from FineWeb)\n",
    "- 20% Thai (from FineWeb-2)\n",
    "\n",
    "Expected result: ~800 English tokens, ~200 Thai tokens, all human-readable.\n",
    "\n",
    "## Output\n",
    "\n",
    "- `../data/flannel_tokenizer_chars.json` - Trained tokenizer in HuggingFace format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input corpus (from 1.18a)\n",
    "CORPUS_PATH = \"../data/flannel_tokenizer_corpus.txt\"\n",
    "\n",
    "# Output tokenizer\n",
    "TOKENIZER_OUTPUT = \"../data/flannel_tokenizer_chars.json\"\n",
    "\n",
    "# Tokenizer parameters\n",
    "VOCAB_SIZE = 10000\n",
    "MIN_FREQUENCY = 2  # Ignore pairs that appear less than this\n",
    "\n",
    "# Special tokens\n",
    "SPECIAL_TOKENS = [\"<|endoftext|>\"]\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Corpus Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found corpus at ../data/flannel_tokenizer_corpus.txt\n",
      "  Size: 104,920,936 bytes (100.06 MB)\n"
     ]
    }
   ],
   "source": [
    "corpus_path = Path(CORPUS_PATH)\n",
    "\n",
    "if not corpus_path.exists():\n",
    "    raise FileNotFoundError(f\"Corpus not found at {CORPUS_PATH}. Run 1.18a first.\")\n",
    "\n",
    "# Check corpus size\n",
    "corpus_bytes = corpus_path.stat().st_size\n",
    "corpus_mb = corpus_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"✓ Found corpus at {CORPUS_PATH}\")\n",
    "print(f\"  Size: {corpus_bytes:,} bytes ({corpus_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Corpus Character Set\n",
    "\n",
    "Before training, let's see what characters appear in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing corpus character set...\n",
      "\n",
      "✓ Found 2,800 unique characters in corpus\n",
      "  ASCII characters: 96\n",
      "  Thai characters: 86\n",
      "  Other characters: 2,618\n",
      "\n",
      "Sample Thai characters (first 20):\n",
      "  ก (U+0E01)\n",
      "  ข (U+0E02)\n",
      "  ฃ (U+0E03)\n",
      "  ค (U+0E04)\n",
      "  ฅ (U+0E05)\n",
      "  ฆ (U+0E06)\n",
      "  ง (U+0E07)\n",
      "  จ (U+0E08)\n",
      "  ฉ (U+0E09)\n",
      "  ช (U+0E0A)\n",
      "  ซ (U+0E0B)\n",
      "  ฌ (U+0E0C)\n",
      "  ญ (U+0E0D)\n",
      "  ฎ (U+0E0E)\n",
      "  ฏ (U+0E0F)\n",
      "  ฐ (U+0E10)\n",
      "  ฑ (U+0E11)\n",
      "  ฒ (U+0E12)\n",
      "  ณ (U+0E13)\n",
      "  ด (U+0E14)\n",
      "\n",
      "This character set will be the base vocabulary for BPE.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Analyzing corpus character set...\\n\")\n",
    "\n",
    "# Read corpus\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "# Find unique characters\n",
    "unique_chars = set(corpus_text)\n",
    "\n",
    "print(f\"✓ Found {len(unique_chars):,} unique characters in corpus\")\n",
    "\n",
    "# Classify by Unicode range\n",
    "ascii_chars = [c for c in unique_chars if ord(c) < 128]\n",
    "thai_chars = [c for c in unique_chars if 0x0E00 <= ord(c) <= 0x0E7F]  # Thai script range\n",
    "other_chars = [c for c in unique_chars if c not in ascii_chars and c not in thai_chars]\n",
    "\n",
    "print(f\"  ASCII characters: {len(ascii_chars):,}\")\n",
    "print(f\"  Thai characters: {len(thai_chars):,}\")\n",
    "print(f\"  Other characters: {len(other_chars):,}\")\n",
    "print()\n",
    "\n",
    "# Show some Thai characters\n",
    "if thai_chars:\n",
    "    print(f\"Sample Thai characters (first 20):\")\n",
    "    for i, char in enumerate(sorted(thai_chars)[:20]):\n",
    "        print(f\"  {char} (U+{ord(char):04X})\")\n",
    "    print()\n",
    "\n",
    "print(f\"This character set will be the base vocabulary for BPE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tokenizer\n",
    "\n",
    "We'll use character-level BPE with whitespace pre-tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating tokenizer...\n",
      "\n",
      "✓ Created character-level BPE tokenizer\n",
      "  Base vocabulary: 2,800 unique characters from corpus\n",
      "  Target vocabulary: 10,000 tokens\n",
      "  Merges to perform: 7,200\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCreating tokenizer...\\n\")\n",
    "\n",
    "# Create a character-level BPE tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Use whitespace pre-tokenizer (splits on spaces, preserves Unicode)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "print(f\"✓ Created character-level BPE tokenizer\")\n",
    "print(f\"  Base vocabulary: {len(unique_chars):,} unique characters from corpus\")\n",
    "print(f\"  Target vocabulary: {VOCAB_SIZE:,} tokens\")\n",
    "print(f\"  Merges to perform: {VOCAB_SIZE - len(unique_chars):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Tokenizer\n",
    "\n",
    "This is where the BPE algorithm runs. It will:\n",
    "1. Scan the entire corpus\n",
    "2. Count all character pairs\n",
    "3. Iteratively merge the most frequent pairs until we reach 1000 tokens\n",
    "\n",
    "Expected time: 1-3 minutes on your M4 Pro for ~100 MB of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on ../data/flannel_tokenizer_corpus.txt...\n",
      "\n",
      "This will take 1-3 minutes. The tokenizer is:\n",
      "  1. Scanning 100 MB of text\n",
      "  2. Counting all character pairs\n",
      "  3. Performing 7,200 merge operations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "✓ Training complete in 6.1 seconds (0.10 minutes)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training tokenizer on {CORPUS_PATH}...\\n\")\n",
    "print(f\"This will take 1-3 minutes. The tokenizer is:\")\n",
    "print(f\"  1. Scanning {corpus_mb:.0f} MB of text\")\n",
    "print(f\"  2. Counting all character pairs\")\n",
    "print(f\"  3. Performing {max(0, VOCAB_SIZE - len(unique_chars)):,} merge operations\\n\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=MIN_FREQUENCY,\n",
    "    special_tokens=SPECIAL_TOKENS,\n",
    "    show_progress=True,\n",
    "    initial_alphabet=list(unique_chars)  # Start with corpus character set\n",
    ")\n",
    "\n",
    "# Train (this is the slow part)\n",
    "start_time = time.time()\n",
    "tokenizer.train(files=[str(corpus_path)], trainer=trainer)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Training complete in {elapsed:.1f} seconds ({elapsed/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Tokenizer\n",
    "\n",
    "Let's see what the tokenizer learned. All tokens should be valid Unicode strings now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer statistics:\n",
      "\n",
      "  Vocabulary size: 10,000 tokens\n",
      "  Special tokens: 1\n",
      "  Base characters: 2,800\n",
      "  Learned merges: 7,200\n",
      "\n",
      "Example merged tokens (first 30):\n",
      "  2801: 'th'\n",
      "  2802: 'in'\n",
      "  2803: 'er'\n",
      "  2804: 'an'\n",
      "  2805: 'on'\n",
      "  2806: 'the'\n",
      "  2807: 're'\n",
      "  2808: 'at'\n",
      "  2809: 'en'\n",
      "  2810: 'or'\n",
      "  2811: 'ou'\n",
      "  2812: 'es'\n",
      "  2813: 'al'\n",
      "  2814: 'is'\n",
      "  2815: 'to'\n",
      "  2816: 'ing'\n",
      "  2817: 'ed'\n",
      "  2818: 'and'\n",
      "  2819: 'ar'\n",
      "  2820: 'it'\n",
      "  2821: 'as'\n",
      "  2822: 'of'\n",
      "  2823: 'ic'\n",
      "  2824: 'le'\n",
      "  2825: 'st'\n",
      "  2826: 'ion'\n",
      "  2827: 'om'\n",
      "  2828: 'il'\n",
      "  2829: 'ent'\n",
      "  2830: 'he'\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTokenizer statistics:\\n\")\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"  Vocabulary size: {vocab_size:,} tokens\")\n",
    "print(f\"  Special tokens: {len(SPECIAL_TOKENS)}\")\n",
    "print(f\"  Base characters: {len(unique_chars):,}\")\n",
    "print(f\"  Learned merges: {vocab_size - len(unique_chars):,}\")\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Show some example merged tokens (skip base characters)\n",
    "print(f\"\\nExample merged tokens (first 30):\")\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
    "\n",
    "# Skip to merged tokens (past base character set)\n",
    "merged_tokens = [(token, idx) for token, idx in sorted_vocab if len(token) > 1 and token not in SPECIAL_TOKENS]\n",
    "\n",
    "for token, idx in merged_tokens[:30]:\n",
    "    # All tokens should be valid Unicode now\n",
    "    print(f\"  {idx:4d}: {repr(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Token Languages\n",
    "\n",
    "Since all tokens are valid Unicode, we can easily classify them by script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying tokens by language...\n",
      "\n",
      "Token classification:\n",
      "  English tokens: 6,108 (61.1%)\n",
      "  Thai tokens: 1,272 (12.7%)\n",
      "  Mixed tokens: 0 (0.0%)\n",
      "  Special tokens: 1\n",
      "  Other tokens: 2,619\n",
      "\n",
      "Example English tokens:\n",
      "  2801: 'th'\n",
      "  2802: 'in'\n",
      "  2803: 'er'\n",
      "  2804: 'an'\n",
      "  2805: 'on'\n",
      "  2806: 'the'\n",
      "  2807: 're'\n",
      "  2808: 'at'\n",
      "  2809: 'en'\n",
      "  2810: 'or'\n",
      "\n",
      "Example Thai tokens:\n",
      "  2901: 'าร'\n",
      "  2903: 'อง'\n",
      "  2912: 'ี่'\n",
      "  2914: '่า'\n",
      "  2934: 'ที่'\n",
      "  2943: 'การ'\n",
      "  2956: '้า'\n",
      "  2975: 'ระ'\n",
      "  2980: 'าม'\n",
      "  2985: '่อ'\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nClassifying tokens by language...\\n\")\n",
    "\n",
    "def classify_token(token):\n",
    "    \"\"\"Classify a token as English, Thai, Mixed, or Special\"\"\"\n",
    "    if token in SPECIAL_TOKENS:\n",
    "        return 'special'\n",
    "    \n",
    "    # Check character composition\n",
    "    has_ascii = any(ord(c) < 128 for c in token)\n",
    "    has_thai = any(0x0E00 <= ord(c) <= 0x0E7F for c in token)\n",
    "    \n",
    "    if has_thai and has_ascii:\n",
    "        return 'mixed'\n",
    "    elif has_thai:\n",
    "        return 'thai'\n",
    "    elif has_ascii:\n",
    "        return 'english'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Classify all tokens\n",
    "token_classes = {}\n",
    "for token, idx in vocab.items():\n",
    "    token_classes[idx] = classify_token(token)\n",
    "\n",
    "# Count by category\n",
    "from collections import Counter\n",
    "class_counts = Counter(token_classes.values())\n",
    "\n",
    "print(f\"Token classification:\")\n",
    "print(f\"  English tokens: {class_counts['english']:,} ({100*class_counts['english']/vocab_size:.1f}%)\")\n",
    "print(f\"  Thai tokens: {class_counts['thai']:,} ({100*class_counts['thai']/vocab_size:.1f}%)\")\n",
    "print(f\"  Mixed tokens: {class_counts['mixed']:,} ({100*class_counts['mixed']/vocab_size:.1f}%)\")\n",
    "print(f\"  Special tokens: {class_counts['special']:,}\")\n",
    "print(f\"  Other tokens: {class_counts['other']:,}\")\n",
    "print()\n",
    "\n",
    "# Show some examples of each type\n",
    "print(f\"Example English tokens:\")\n",
    "english_tokens = [(t, i) for t, i in vocab.items() if token_classes[i] == 'english' and len(t) > 1]\n",
    "for token, idx in sorted(english_tokens, key=lambda x: x[1])[:10]:\n",
    "    print(f\"  {idx:4d}: {repr(token)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Example Thai tokens:\")\n",
    "thai_tokens = [(t, i) for t, i in vocab.items() if token_classes[i] == 'thai' and len(t) > 1]\n",
    "for token, idx in sorted(thai_tokens, key=lambda x: x[1])[:10]:\n",
    "    print(f\"  {idx:4d}: {repr(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tokenizer\n",
    "\n",
    "Let's encode some test strings to verify the tokenizer works and produces readable tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing tokenizer...\n",
      "\n",
      "Input:  'Hello, world!'\n",
      "Tokens: ['Hel', 'lo', ',', 'world', '!']\n",
      "IDs:    [5976, 2840, 14, 3501, 3]\n",
      "Count:  5 tokens\n",
      "Types:  ['english', 'english', 'english', 'english', 'english']\n",
      "\n",
      "Input:  'The quick brown fox jumps over the lazy dog.'\n",
      "Tokens: ['The', 'quick', 'brown', 'fo', 'x', 'jum', 'ps', 'over', 'the', 'l', 'azy', 'dog', '.']\n",
      "IDs:    [2870, 4469, 8657, 3760, 90, 9440, 3432, 3093, 2806, 78, 7951, 5013, 16]\n",
      "Count:  13 tokens\n",
      "Types:  ['english', 'english', 'english', 'english', 'english', 'english', 'english', 'english', 'english', 'english', 'english', 'english', 'english']\n",
      "\n",
      "Input:  'สวัสดีครับ'\n",
      "Tokens: ['ส', 'วั', 'ส', 'ดี', 'ครับ']\n",
      "IDs:    [714, 4985, 714, 3631, 4681]\n",
      "Count:  5 tokens\n",
      "Types:  ['thai', 'thai', 'thai', 'thai', 'thai']\n",
      "\n",
      "Input:  'ภาษาไทย'\n",
      "Tokens: ['ภาษา', 'ไทย']\n",
      "IDs:    [7397, 4240]\n",
      "Count:  2 tokens\n",
      "Types:  ['thai', 'thai']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTesting tokenizer...\\n\")\n",
    "\n",
    "test_strings = [\n",
    "    \"Hello, world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"สวัสดีครับ\",  # Thai: \"Hello\" (polite, male)\n",
    "    \"ภาษาไทย\"      # Thai: \"Thai language\"\n",
    "]\n",
    "\n",
    "for test_str in test_strings:\n",
    "    encoding = tokenizer.encode(test_str)\n",
    "    tokens = encoding.tokens\n",
    "    ids = encoding.ids\n",
    "    \n",
    "    print(f\"Input:  {repr(test_str)}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"IDs:    {ids}\")\n",
    "    print(f\"Count:  {len(ids)} tokens\")\n",
    "    \n",
    "    # Classify tokens in this encoding\n",
    "    classes = [token_classes[tid] for tid in ids]\n",
    "    print(f\"Types:  {classes}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenizer to ../data/flannel_tokenizer_chars.json...\n",
      "\n",
      "✓ Saved tokenizer\n",
      "  Path: ../data/flannel_tokenizer_chars.json\n",
      "  Size: 526.1 KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving tokenizer to {TOKENIZER_OUTPUT}...\\n\")\n",
    "\n",
    "# Ensure directory exists\n",
    "Path(TOKENIZER_OUTPUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save in HuggingFace format (JSON)\n",
    "tokenizer.save(str(TOKENIZER_OUTPUT))\n",
    "\n",
    "# Verify file was created\n",
    "output_path = Path(TOKENIZER_OUTPUT)\n",
    "if output_path.exists():\n",
    "    output_kb = output_path.stat().st_size / 1024\n",
    "    print(f\"✓ Saved tokenizer\")\n",
    "    print(f\"  Path: {TOKENIZER_OUTPUT}\")\n",
    "    print(f\"  Size: {output_kb:.1f} KB\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Failed to save tokenizer to {TOKENIZER_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TOKENIZER TRAINING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Training corpus:\n",
      "  Path: ../data/flannel_tokenizer_corpus.txt\n",
      "  Size: 100.06 MB\n",
      "  Composition: ~80% English, ~20% Thai\n",
      "  Unique characters: 2,800\n",
      "\n",
      "Tokenizer:\n",
      "  Type: Character-level BPE\n",
      "  Vocabulary size: 10,000 tokens\n",
      "  Base characters: 2,800\n",
      "  Learned merges: 7,200\n",
      "  Training time: 6.1 seconds\n",
      "\n",
      "Token composition:\n",
      "  English: 6,108 tokens (61.1%)\n",
      "  Thai: 1,272 tokens (12.7%)\n",
      "  Mixed: 0 tokens\n",
      "  Other: 2,619 tokens\n",
      "\n",
      "Output:\n",
      "  Path: ../data/flannel_tokenizer_chars.json\n",
      "\n",
      "Benefits of character-level encoding:\n",
      "  ✓ All tokens are valid Unicode strings (interpretable)\n",
      "  ✓ Easy to classify tokens by language/script\n",
      "  ✓ Matches approach used by models like Qwen 3 4B\n",
      "  ✓ Better for tracking token movement during training\n",
      "\n",
      "Next steps:\n",
      "  → Use this tokenizer to train Flannel models (notebook 1.20a+)\n",
      "  → ~1272 Thai tokens will be dead (never appear in English training)\n",
      "  → Watch what happens to those dead tokens during training\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TOKENIZER TRAINING COMPLETE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Training corpus:\")\n",
    "print(f\"  Path: {CORPUS_PATH}\")\n",
    "print(f\"  Size: {corpus_mb:.2f} MB\")\n",
    "print(f\"  Composition: ~80% English, ~20% Thai\")\n",
    "print(f\"  Unique characters: {len(unique_chars):,}\")\n",
    "print()\n",
    "\n",
    "print(f\"Tokenizer:\")\n",
    "print(f\"  Type: Character-level BPE\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,} tokens\")\n",
    "print(f\"  Base characters: {len(unique_chars):,}\")\n",
    "print(f\"  Learned merges: {vocab_size - len(unique_chars):,}\")\n",
    "print(f\"  Training time: {elapsed:.1f} seconds\")\n",
    "print()\n",
    "\n",
    "print(f\"Token composition:\")\n",
    "print(f\"  English: {class_counts['english']:,} tokens ({100*class_counts['english']/vocab_size:.1f}%)\")\n",
    "print(f\"  Thai: {class_counts['thai']:,} tokens ({100*class_counts['thai']/vocab_size:.1f}%)\")\n",
    "print(f\"  Mixed: {class_counts['mixed']:,} tokens\")\n",
    "print(f\"  Other: {class_counts['other']:,} tokens\")\n",
    "print()\n",
    "\n",
    "print(f\"Output:\")\n",
    "print(f\"  Path: {TOKENIZER_OUTPUT}\")\n",
    "print()\n",
    "\n",
    "print(f\"Benefits of character-level encoding:\")\n",
    "print(f\"  ✓ All tokens are valid Unicode strings (interpretable)\")\n",
    "print(f\"  ✓ Easy to classify tokens by language/script\")\n",
    "print(f\"  ✓ Matches approach used by models like Qwen 3 4B\")\n",
    "print(f\"  ✓ Better for tracking token movement during training\")\n",
    "print()\n",
    "\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  → Use this tokenizer to train Flannel models (notebook 1.20a+)\")\n",
    "print(f\"  → ~{class_counts['thai']} Thai tokens will be dead (never appear in English training)\")\n",
    "print(f\"  → Watch what happens to those dead tokens during training\")\n",
    "print()\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
