{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thimble 3: The All-Bfloat16 Revue ðŸŽ­\n",
    "\n",
    "**Starring:** Quantization at every step!\n",
    "\n",
    "**Objective:** Train in bfloat16 and record **everything** in bfloat16. No dtype conversions. What PyTorch does, we do.\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "**Thimble 1:** Trained in bfloat16, but saved optimizer states as float32 (converting during recording)\n",
    "\n",
    "**Thimble 2:** Trained in float32, proved the AdamW formula works (max_diff < 3e-08)\n",
    "\n",
    "**The Problem:** Thimble 1's saved states are MORE PRECISE than what PyTorch actually used during training. We converted bf16â†’f32 when saving, giving us higher precision states than the actual training.\n",
    "\n",
    "**The Solution:** Record states in bfloat16. Match PyTorch's actual precision, quantization and all.\n",
    "\n",
    "## The All-Bfloat16 Pipeline\n",
    "\n",
    "PyTorch does this (when param is bfloat16):\n",
    "```python\n",
    "# All in bfloat16\n",
    "m[t] = (0.9 * m[t-1] + 0.1 * grad[t]).bfloat16()\n",
    "v[t] = (0.999 * v[t-1] + 0.001 * grad[t]Â²).bfloat16()\n",
    "m_hat = (m[t] / bias_corr1).bfloat16()\n",
    "v_hat = (v[t] / bias_corr2).bfloat16()\n",
    "dW = (-lr * m_hat / sqrt(v_hat)).bfloat16()\n",
    "W[t] = (W[t-1] + dW).bfloat16()\n",
    "```\n",
    "\n",
    "Quantization at every intermediate step. This is what we'll simulate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Parameters set\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYERS = 2\n",
    "N_HEADS = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# Training\n",
    "NUM_STEPS = 1000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# Optimizer (AdamW)\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "# Initialization\n",
    "INIT_SCALE = 0.02\n",
    "SEED = 42\n",
    "\n",
    "# Paths\n",
    "TOKENIZER_PATH = \"../data/flannel_tokenizer_chars.json\"\n",
    "CORPUS_PATH = \"../data/flannel_model_corpus.txt\"\n",
    "TOKEN_MASK_PATH = \"../tensors/Flannel/live_dead_tokens.safetensors\"\n",
    "OUTPUT_PATH = \"../tensors/Thimble/thimble_3.safetensors\"\n",
    "\n",
    "print(\"âœ“ Parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from tokenizers import Tokenizer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file, load_file\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "print(\"âœ“ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Safety Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MEMORY & DISK SAFETY CHECK\n",
      "================================================================================\n",
      "\n",
      "Recording tensors (ALL BFLOAT16):\n",
      "  W:         1.28 GB (bfloat16)\n",
      "  grad_W:    1.28 GB (bfloat16)\n",
      "  momentum:  1.28 GB (bfloat16) â† THE KEY CHANGE\n",
      "  variance:  1.28 GB (bfloat16) â† THE KEY CHANGE\n",
      "  losses:    0.0000 GB (float32)\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Total:     5.13 GB\n",
      "\n",
      "This is EXACTLY what PyTorch uses internally.\n",
      "No dtype conversions. Pure bfloat16 pipeline.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MEMORY & DISK SAFETY CHECK\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Recording tensors - ALL BFLOAT16\n",
    "bytes_bf16 = 2\n",
    "bytes_f32 = 4\n",
    "\n",
    "recording_w = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16\n",
    "recording_grad = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16\n",
    "recording_momentum = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16  # bfloat16!\n",
    "recording_variance = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16  # bfloat16!\n",
    "recording_losses = (NUM_STEPS+1) * bytes_f32\n",
    "\n",
    "total_recording = recording_w + recording_grad + recording_momentum + recording_variance + recording_losses\n",
    "\n",
    "print(f\"Recording tensors (ALL BFLOAT16):\")\n",
    "print(f\"  W:         {recording_w/1e9:.2f} GB (bfloat16)\")\n",
    "print(f\"  grad_W:    {recording_grad/1e9:.2f} GB (bfloat16)\")\n",
    "print(f\"  momentum:  {recording_momentum/1e9:.2f} GB (bfloat16) â† THE KEY CHANGE\")\n",
    "print(f\"  variance:  {recording_variance/1e9:.2f} GB (bfloat16) â† THE KEY CHANGE\")\n",
    "print(f\"  losses:    {recording_losses/1e9:.4f} GB (float32)\")\n",
    "print(f\"  {'â”€'*40}\")\n",
    "print(f\"  Total:     {total_recording/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "print(f\"This is EXACTLY what PyTorch uses internally.\")\n",
    "print(f\"No dtype conversions. Pure bfloat16 pipeline.\")\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"âœ“ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: ../data/flannel_tokenizer_chars.json\n",
      "  âœ“ Vocabulary: 10,000 tokens\n",
      "\n",
      "Loading corpus: ../data/flannel_model_corpus.txt\n",
      "  âœ“ Tokens: 1,371,328\n",
      "\n",
      "Loading token masks: ../tensors/Flannel/live_dead_tokens.safetensors\n",
      "  âœ“ Live: 6,301 | Dead: 3,699\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "print(f\"Loading tokenizer: {TOKENIZER_PATH}\")\n",
    "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "print(f\"  âœ“ Vocabulary: {tokenizer.get_vocab_size():,} tokens\\n\")\n",
    "\n",
    "# Corpus\n",
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "encoding = tokenizer.encode(corpus_text)\n",
    "tokens = encoding.ids\n",
    "corpus_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "print(f\"  âœ“ Tokens: {len(tokens):,}\\n\")\n",
    "\n",
    "# Token masks (for analysis later)\n",
    "print(f\"Loading token masks: {TOKEN_MASK_PATH}\")\n",
    "mask_data = load_file(TOKEN_MASK_PATH)\n",
    "live_mask = mask_data['live_mask'].bool()\n",
    "dead_mask = mask_data['dead_mask'].bool()\n",
    "n_live = live_mask.sum().item()\n",
    "n_dead = dead_mask.sum().item()\n",
    "print(f\"  âœ“ Live: {n_live:,} | Dead: {n_dead:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Dataset: 1,371,200 examples\n",
      "âœ“ DataLoader: 10,713 batches per epoch\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = TokenDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    generator=g,\n",
    "    worker_init_fn=seed_worker,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Dataset: {len(dataset):,} examples\")\n",
    "print(f\"âœ“ DataLoader: {len(dataloader):,} batches per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model (BFLOAT16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "\n",
      "  Architecture: 2 layers, 2 heads, 64d embeddings\n",
      "  Parameters: 748,288\n",
      "  Device: mps\n",
      "  Dtype: torch.bfloat16 (BFLOAT16)\n",
      "\n",
      "âœ“ Model created\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating model...\\n\")\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYERS,\n",
    "    n_head=N_HEADS,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Initialize embedding weights\n",
    "with torch.no_grad():\n",
    "    nn.init.normal_(model.transformer.wte.weight, mean=0.0, std=INIT_SCALE)\n",
    "\n",
    "# Convert to bfloat16 and move to device\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"  Architecture: {N_LAYERS} layers, {N_HEADS} heads, {HIDDEN_DIM}d embeddings\")\n",
    "print(f\"  Parameters: {n_params:,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Dtype: {model.transformer.wte.weight.dtype} (BFLOAT16)\")\n",
    "print(f\"\\nâœ“ Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Optimizer: AdamW\n",
      "  Learning rate: 0.001\n",
      "  Betas: (0.9, 0.999)\n",
      "  Epsilon: 1e-08\n",
      "  Weight decay: 0.0\n",
      "\n",
      "  Optimizer states will be BFLOAT16 (matching param dtype)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(ADAM_BETA1, ADAM_BETA2),\n",
    "    eps=ADAM_EPSILON,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Optimizer: AdamW\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Betas: ({ADAM_BETA1}, {ADAM_BETA2})\")\n",
    "print(f\"  Epsilon: {ADAM_EPSILON}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"\\n  Optimizer states will be BFLOAT16 (matching param dtype)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-allocate Recording Tensors (ALL BFLOAT16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-allocating recording tensors...\n",
      "\n",
      "  W:         (1001, 10000, 64) (bfloat16) = 1.28 GB\n",
      "  grad_W:    (1001, 10000, 64) (bfloat16) = 1.28 GB\n",
      "  momentum:  (1001, 10000, 64) (bfloat16) = 1.28 GB\n",
      "  variance:  (1001, 10000, 64) (bfloat16) = 1.28 GB\n",
      "  losses:    (1001,) (float32) = 0.0000 GB\n",
      "\n",
      "  Total: 5.13 GB\n",
      "\n",
      "âœ“ Tensors allocated (ALL BFLOAT16 - matching PyTorch's internal precision)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPre-allocating recording tensors...\\n\")\n",
    "\n",
    "# ALL BFLOAT16 - exactly what PyTorch uses\n",
    "W_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "grad_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "momentum_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "variance_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "loss_history = torch.zeros(NUM_STEPS+1, dtype=torch.float32)\n",
    "\n",
    "# Memory calculation\n",
    "bytes_per_bf16 = 2\n",
    "memory_w = W_history.numel() * bytes_per_bf16\n",
    "memory_grad = grad_history.numel() * bytes_per_bf16\n",
    "memory_momentum = momentum_history.numel() * bytes_per_bf16\n",
    "memory_variance = variance_history.numel() * bytes_per_bf16\n",
    "memory_loss = loss_history.numel() * 4\n",
    "total_memory = memory_w + memory_grad + memory_momentum + memory_variance + memory_loss\n",
    "\n",
    "print(f\"  W:         {tuple(W_history.shape)} (bfloat16) = {memory_w/1e9:.2f} GB\")\n",
    "print(f\"  grad_W:    {tuple(grad_history.shape)} (bfloat16) = {memory_grad/1e9:.2f} GB\")\n",
    "print(f\"  momentum:  {tuple(momentum_history.shape)} (bfloat16) = {memory_momentum/1e9:.2f} GB\")\n",
    "print(f\"  variance:  {tuple(variance_history.shape)} (bfloat16) = {memory_variance/1e9:.2f} GB\")\n",
    "print(f\"  losses:    {tuple(loss_history.shape)} (float32) = {memory_loss/1e9:.4f} GB\")\n",
    "print(f\"\\n  Total: {total_memory/1e9:.2f} GB\")\n",
    "print(f\"\\nâœ“ Tensors allocated (ALL BFLOAT16 - matching PyTorch's internal precision)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "THIMBLE 3: THE ALL-BFLOAT16 REVUE ðŸŽ­\n",
      "================================================================================\n",
      "\n",
      "âœ“ Recorded initial state (t=0)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d445f301daa94dfeaa85dc67412c14be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ“ Training complete\n",
      "  Time: 75.7s (1.3 minutes)\n",
      "  Final loss: 6.5621\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"THIMBLE 3: THE ALL-BFLOAT16 REVUE ðŸŽ­\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Record initial state (step 0)\n",
    "W_history[0] = model.transformer.wte.weight.data.clone().cpu()\n",
    "loss_history[0] = float('nan')\n",
    "print(\"âœ“ Recorded initial state (t=0)\\n\")\n",
    "\n",
    "# Create infinite iterator over dataloader\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "\n",
    "for step in tqdm(range(1, NUM_STEPS+1), desc=\"Training\"):\n",
    "    # Get next batch\n",
    "    try:\n",
    "        batch = next(data_iter)\n",
    "    except StopIteration:\n",
    "        data_iter = iter(dataloader)\n",
    "        batch = next(data_iter)\n",
    "    \n",
    "    # Move batch to device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # === RECORD GRADIENTS (before optimizer.step) ===\n",
    "    grad_history[step] = model.transformer.wte.weight.grad.clone().cpu()\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # === RECORD WEIGHTS & OPTIMIZER STATE (after optimizer.step) ===\n",
    "    # NO DTYPE CONVERSION - record exactly what PyTorch has\n",
    "    W_history[step] = model.transformer.wte.weight.data.clone().cpu()\n",
    "    \n",
    "    wte_param = model.transformer.wte.weight\n",
    "    if wte_param in optimizer.state:\n",
    "        opt_state = optimizer.state[wte_param]\n",
    "        # Record as-is (bfloat16)\n",
    "        momentum_history[step] = opt_state['exp_avg'].clone().cpu()\n",
    "        variance_history[step] = opt_state['exp_avg_sq'].clone().cpu()\n",
    "    \n",
    "    loss_history[step] = loss.item()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ“ Training complete\")\n",
    "print(f\"  Time: {elapsed:.1f}s ({elapsed/60:.1f} minutes)\")\n",
    "print(f\"  Final loss: {loss_history[-1]:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation: Predict W[t] Using Bfloat16 Pipeline\n",
    "\n",
    "Now we simulate EXACTLY what PyTorch does, quantizing at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating bfloat16 accounting...\n",
      "\n",
      "t=  1: exact_match=False, max_diff=2.44e-04, ratio=0.999207\n",
      "t= 10: exact_match=False, max_diff=4.88e-04, ratio=1.000738\n",
      "t= 50: exact_match=False, max_diff=4.88e-04, ratio=0.998727\n",
      "t=100: exact_match=False, max_diff=4.88e-04, ratio=0.999764\n",
      "t=200: exact_match=False, max_diff=9.77e-04, ratio=1.000013\n",
      "t=500: exact_match=False, max_diff=1.95e-03, ratio=0.986503\n",
      "t=800: exact_match=False, max_diff=9.77e-04, ratio=0.999655\n",
      "\n",
      "If exact_match=True (or max_diff << bfloat16 precision), we've matched PyTorch!\n",
      "\n",
      "âœ“ Validation complete\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nValidating bfloat16 accounting...\\n\")\n",
    "\n",
    "test_steps = [1, 10, 50, 100, 200, 500, 800]\n",
    "\n",
    "for t in test_steps:\n",
    "    if t > NUM_STEPS:\n",
    "        continue\n",
    "    \n",
    "    # Get bfloat16 optimizer states (as recorded - no conversion)\n",
    "    m_t = momentum_history[t]  # bfloat16\n",
    "    v_t = variance_history[t]  # bfloat16\n",
    "    W_prev = W_history[t-1]    # bfloat16\n",
    "    \n",
    "    # Simulate PyTorch's bfloat16 pipeline\n",
    "    # (We could do this in float32 then quantize, or just use bfloat16 throughout)\n",
    "    bias_correction1 = 1 - ADAM_BETA1**t\n",
    "    bias_correction2 = 1 - ADAM_BETA2**t\n",
    "    \n",
    "    # All in bfloat16\n",
    "    m_hat = (m_t.float() / bias_correction1).bfloat16()\n",
    "    v_hat = (v_t.float() / bias_correction2).bfloat16()\n",
    "    dW = (-LEARNING_RATE * m_hat.float() / (torch.sqrt(v_hat.float()) + ADAM_EPSILON)).bfloat16()\n",
    "    predicted_W = (W_prev.float() + dW.float()).bfloat16()\n",
    "    \n",
    "    # Observed W[t]\n",
    "    observed_W = W_history[t]\n",
    "    \n",
    "    # Test for exact equality (or very close)\n",
    "    exactly_equal = torch.equal(predicted_W, observed_W)\n",
    "    max_abs_diff = torch.max(torch.abs(predicted_W.float() - observed_W.float())).item()\n",
    "    \n",
    "    # Approximate metrics\n",
    "    norm_pred = torch.norm((predicted_W - W_prev).float())\n",
    "    norm_obs = torch.norm((observed_W - W_prev).float())\n",
    "    ratio = norm_pred / (norm_obs + 1e-10)\n",
    "    \n",
    "    print(f\"t={t:3d}: exact_match={exactly_equal}, max_diff={max_abs_diff:.2e}, ratio={ratio:.6f}\")\n",
    "\n",
    "print(\"\\nIf exact_match=True (or max_diff << bfloat16 precision), we've matched PyTorch!\")\n",
    "print(\"\\nâœ“ Validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data to ../tensors/Thimble/thimble_3.safetensors...\n",
      "\n",
      "âœ“ Saved successfully\n",
      "  File: thimble_3.safetensors\n",
      "  Size: 5.13 GB\n",
      "  Save time: 2.3s\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving data to {OUTPUT_PATH}...\\n\")\n",
    "\n",
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_dict = {\n",
    "    # Training trajectories (ALL BFLOAT16)\n",
    "    'W': W_history,\n",
    "    'grad_W': grad_history,\n",
    "    'momentum_W': momentum_history,\n",
    "    'variance_W': variance_history,\n",
    "    'losses': loss_history,\n",
    "    \n",
    "    # Hyperparameters\n",
    "    'vocab_size': torch.tensor(VOCAB_SIZE, dtype=torch.long),\n",
    "    'hidden_dim': torch.tensor(HIDDEN_DIM, dtype=torch.long),\n",
    "    'n_layers': torch.tensor(N_LAYERS, dtype=torch.long),\n",
    "    'n_heads': torch.tensor(N_HEADS, dtype=torch.long),\n",
    "    'num_steps': torch.tensor(NUM_STEPS, dtype=torch.long),\n",
    "    'batch_size': torch.tensor(BATCH_SIZE, dtype=torch.long),\n",
    "    'learning_rate': torch.tensor(LEARNING_RATE, dtype=torch.float32),\n",
    "    'weight_decay': torch.tensor(WEIGHT_DECAY, dtype=torch.float32),\n",
    "    'adam_beta1': torch.tensor(ADAM_BETA1, dtype=torch.float32),\n",
    "    'adam_beta2': torch.tensor(ADAM_BETA2, dtype=torch.float32),\n",
    "    'adam_epsilon': torch.tensor(ADAM_EPSILON, dtype=torch.float32),\n",
    "    'init_scale': torch.tensor(INIT_SCALE, dtype=torch.float32),\n",
    "    'seed': torch.tensor(SEED, dtype=torch.long),\n",
    "    'n_live': torch.tensor(n_live, dtype=torch.long),\n",
    "    'n_dead': torch.tensor(n_dead, dtype=torch.long),\n",
    "}\n",
    "\n",
    "save_start = time.time()\n",
    "save_file(save_dict, str(OUTPUT_PATH))\n",
    "save_elapsed = time.time() - save_start\n",
    "\n",
    "file_size_bytes = Path(OUTPUT_PATH).stat().st_size\n",
    "file_size_gb = file_size_bytes / 1e9\n",
    "\n",
    "print(f\"âœ“ Saved successfully\")\n",
    "print(f\"  File: {Path(OUTPUT_PATH).name}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB\")\n",
    "print(f\"  Save time: {save_elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "THIMBLE 3 COMPLETE: THE ALL-BFLOAT16 REVUE ðŸŽ­\n",
      "================================================================================\n",
      "\n",
      "Trained for 1,000 steps with PURE BFLOAT16 pipeline\n",
      "  Model dtype: bfloat16\n",
      "  Optimizer states: bfloat16 (matching PyTorch internal precision)\n",
      "  No dtype conversions during recording\n",
      "\n",
      "Recorded at every step (all bfloat16):\n",
      "  â€¢ W: embedding weights\n",
      "  â€¢ grad_W: gradients\n",
      "  â€¢ momentum_W: Adam exp_avg\n",
      "  â€¢ variance_W: Adam exp_avg_sq\n",
      "  â€¢ losses: training loss\n",
      "\n",
      "Data saved: ../tensors/Thimble/thimble_3.safetensors\n",
      "  Size: 5.13 GB\n",
      "  Training time: 1.3 minutes\n",
      "\n",
      "This is EXACTLY what PyTorch does. Pure bfloat16. Quantization at every step.\n",
      "If the validation showed exact matches, we've cracked it. ðŸŽ‰\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"THIMBLE 3 COMPLETE: THE ALL-BFLOAT16 REVUE ðŸŽ­\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Trained for {NUM_STEPS:,} steps with PURE BFLOAT16 pipeline\")\n",
    "print(f\"  Model dtype: bfloat16\")\n",
    "print(f\"  Optimizer states: bfloat16 (matching PyTorch internal precision)\")\n",
    "print(f\"  No dtype conversions during recording\")\n",
    "print()\n",
    "print(f\"Recorded at every step (all bfloat16):\")\n",
    "print(f\"  â€¢ W: embedding weights\")\n",
    "print(f\"  â€¢ grad_W: gradients\")\n",
    "print(f\"  â€¢ momentum_W: Adam exp_avg\")\n",
    "print(f\"  â€¢ variance_W: Adam exp_avg_sq\")\n",
    "print(f\"  â€¢ losses: training loss\")\n",
    "print()\n",
    "print(f\"Data saved: {OUTPUT_PATH}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB\")\n",
    "print(f\"  Training time: {elapsed/60:.1f} minutes\")\n",
    "print()\n",
    "print(f\"This is EXACTLY what PyTorch does. Pure bfloat16. Quantization at every step.\")\n",
    "print(f\"If the validation showed exact matches, we've cracked it. ðŸŽ‰\")\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
