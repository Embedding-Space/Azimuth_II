{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 1.13b: Lattice Topology Analysis\n",
    "\n",
    "**Goal:** Analyze the graph structure of any lattice discovered by 1.13a.\n",
    "\n",
    "## Method\n",
    "\n",
    "If 1.13a found lattice structure, build an undirected graph to characterize its topology:\n",
    "- **Nodes**: Unique vectors participating in the lattice\n",
    "  - Black hole centroids (degenerate vectors with multiple identical tokens)\n",
    "  - Singletons (unique vectors with lattice neighbors)\n",
    "- **Edges**: All lattice neighbor relationships\n",
    "  - Orthogonal: differ by ±1 mantissa in exactly 1 dimension\n",
    "  - Diagonal: differ by ±1 mantissa in multiple dimensions\n",
    "\n",
    "Analyze:\n",
    "1. **Composition**: How many singletons vs black holes?\n",
    "2. **Connected components**: One structure or multiple disconnected clusters?\n",
    "3. **Graph properties**: Diameter, density, clustering coefficient\n",
    "4. **Degree distribution**: Which nodes are hubs? What's the connectivity pattern?\n",
    "\n",
    "## Use Case\n",
    "\n",
    "This notebook provides sanity-check analysis of lattice structures found by 1.13a:\n",
    "- If no structure found (0 nodes), nothing to analyze\n",
    "- If structure found, characterize its topology to understand what it is\n",
    "- Compare across different W matrices to identify patterns or anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to analyze (must match 1.13a)\n",
    "# MODEL_NAME = \"Qwen3-4B-Instruct-2507\"\n",
    "# MODEL_NAME = \"Qwen2.5-3B-Instruct\"\n",
    "MODEL_NAME = \"Lil_Gatsby\"\n",
    "\n",
    "# Input from 1.13a\n",
    "STRUCTURE_FILE = f\"../tensors/{MODEL_NAME}/1.13a_lattice_structure.safetensors\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded structure from 1.13a_lattice_structure.safetensors\n",
      "\n",
      "1.13a results:\n",
      "  Black holes: 53 tokens → 1 centroids\n",
      "  Lattice edges: 0 orthogonal + 0 diagonal = 0 total\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load structure results from 1.13a\n",
    "data = load_file(STRUCTURE_FILE)\n",
    "\n",
    "print(f\"✓ Loaded structure from {Path(STRUCTURE_FILE).name}\")\n",
    "print()\n",
    "\n",
    "# Extract data\n",
    "black_hole_token_ids = data['black_hole_token_ids'].tolist()\n",
    "orthogonal_edges = data['orthogonal_edges'].tolist()\n",
    "diagonal_edges = data['diagonal_edges'].tolist()\n",
    "representative_tokens = data['representative_tokens'].tolist()\n",
    "inverse_indices = data['inverse_indices']\n",
    "\n",
    "n_black_hole_tokens = data['n_black_hole_tokens'].item()\n",
    "n_black_hole_centroids = data['n_black_hole_centroids'].item()\n",
    "\n",
    "print(f\"1.13a results:\")\n",
    "print(f\"  Black holes: {n_black_hole_tokens:,} tokens → {n_black_hole_centroids} centroids\")\n",
    "print(f\"  Lattice edges: {len(orthogonal_edges):,} orthogonal + {len(diagonal_edges):,} diagonal = {len(orthogonal_edges) + len(diagonal_edges):,} total\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Check for Lattice Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NO LATTICE STRUCTURE FOUND\n",
      "================================================================================\n",
      "\n",
      "1.13a found no lattice neighbor relationships.\n",
      "This W matrix does not contain detectable lattice-scale structure.\n",
      "\n",
      "Possible interpretations:\n",
      "  • Embeddings are truly continuous (no bfloat16 quantization artifacts)\n",
      "  • Lattice structure exists but is too sparse to detect with current method\n",
      "  • Tokens were initialized to random positions without shared structure\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "No lattice structure to analyze. Stopping notebook execution.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m No lattice structure to analyze. Stopping notebook execution.\n"
     ]
    }
   ],
   "source": [
    "# Quick check: do we have anything to analyze?\n",
    "total_edges = len(orthogonal_edges) + len(diagonal_edges)\n",
    "\n",
    "if total_edges == 0:\n",
    "    print(\"=\"*80)\n",
    "    print(\"NO LATTICE STRUCTURE FOUND\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(\"1.13a found no lattice neighbor relationships.\")\n",
    "    print(\"This W matrix does not contain detectable lattice-scale structure.\")\n",
    "    print()\n",
    "    print(\"Possible interpretations:\")\n",
    "    print(\"  • Embeddings are truly continuous (no bfloat16 quantization artifacts)\")\n",
    "    print(\"  • Lattice structure exists but is too sparse to detect with current method\")\n",
    "    print(\"  • Tokens were initialized to random positions without shared structure\")\n",
    "    print()\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Stop here - nothing to analyze\n",
    "    raise SystemExit(\"No lattice structure to analyze. Stopping notebook execution.\")\n",
    "else:\n",
    "    print(f\"✓ Lattice structure detected: {total_edges:,} edges found\")\n",
    "    print(f\"  Proceeding with topology analysis...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Map Tokens to Unique Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MAPPING TOKENS TO UNIQUE VECTORS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Build mapping: token_id -> unique_vector_id\n",
    "token_to_unique = {}\n",
    "for unique_id, token_id in enumerate(representative_tokens):\n",
    "    token_to_unique[token_id] = unique_id\n",
    "\n",
    "print(f\"Total unique vectors in W: {len(representative_tokens):,}\")\n",
    "print()\n",
    "\n",
    "# Collect all unique vector IDs that participate in the lattice\n",
    "lattice_unique_ids = set()\n",
    "\n",
    "for token_i, token_j in orthogonal_edges + diagonal_edges:\n",
    "    lattice_unique_ids.add(token_to_unique[token_i])\n",
    "    lattice_unique_ids.add(token_to_unique[token_j])\n",
    "\n",
    "# Identify which are black holes vs singletons\n",
    "black_hole_unique_ids = set()\n",
    "for bh_token in black_hole_token_ids:\n",
    "    if bh_token in token_to_unique:  # Should always be true\n",
    "        black_hole_unique_ids.add(token_to_unique[bh_token])\n",
    "\n",
    "singleton_unique_ids = lattice_unique_ids - black_hole_unique_ids\n",
    "\n",
    "n_singletons = len(singleton_unique_ids)\n",
    "n_bh_centroids_in_lattice = len(black_hole_unique_ids)\n",
    "n_total_lattice = len(lattice_unique_ids)\n",
    "\n",
    "pct_singletons = 100 * n_singletons / n_total_lattice if n_total_lattice > 0 else 0\n",
    "pct_bh = 100 * n_bh_centroids_in_lattice / n_total_lattice if n_total_lattice > 0 else 0\n",
    "\n",
    "print(f\"Lattice composition:\")\n",
    "print(f\"  Total: {n_total_lattice} unique vectors\")\n",
    "print(f\"  Singletons: {n_singletons} ({pct_singletons:.1f}%)\")\n",
    "print(f\"  Black hole centroids: {n_bh_centroids_in_lattice} ({pct_bh:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Map edges from token space to unique vector space\n",
    "unique_edges = set()\n",
    "for token_i, token_j in orthogonal_edges + diagonal_edges:\n",
    "    unique_i = token_to_unique[token_i]\n",
    "    unique_j = token_to_unique[token_j]\n",
    "    edge = tuple(sorted([unique_i, unique_j]))\n",
    "    unique_edges.add(edge)\n",
    "\n",
    "print(f\"Unique edges (after deduplication): {len(unique_edges):,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Build Lattice Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BUILDING LATTICE GRAPH\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Create graph over unique vector IDs\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(lattice_unique_ids)\n",
    "G.add_edges_from(unique_edges)\n",
    "\n",
    "print(f\"✓ Graph constructed\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()} unique vectors\")\n",
    "print(f\"  Edges: {G.number_of_edges()} lattice neighbor relationships\")\n",
    "print(f\"  Density: {nx.density(G):.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Connected Components Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CONNECTED COMPONENTS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "components = list(nx.connected_components(G))\n",
    "n_components = len(components)\n",
    "components_sorted = sorted(components, key=len, reverse=True)\n",
    "\n",
    "print(f\"Number of connected components: {n_components}\")\n",
    "print()\n",
    "\n",
    "# Show component sizes\n",
    "if n_components <= 20:\n",
    "    print(f\"Component sizes:\")\n",
    "    for i, component in enumerate(components_sorted):\n",
    "        size = len(component)\n",
    "        pct = 100 * size / G.number_of_nodes()\n",
    "        print(f\"  Component {i+1}: {size:4d} nodes ({pct:5.1f}%)\")\n",
    "        \n",
    "        # Show members if small enough\n",
    "        if size <= 10:\n",
    "            members = sorted(component)\n",
    "            member_tokens = [representative_tokens[uid] for uid in members]\n",
    "            print(f\"    Representative token IDs: {member_tokens}\")\n",
    "else:\n",
    "    print(f\"Component size distribution (showing top 20):\")\n",
    "    for i in range(min(20, n_components)):\n",
    "        component = components_sorted[i]\n",
    "        size = len(component)\n",
    "        pct = 100 * size / G.number_of_nodes()\n",
    "        print(f\"  Component {i+1}: {size:4d} nodes ({pct:5.1f}%)\")\n",
    "    if n_components > 20:\n",
    "        print(f\"  ... and {n_components - 20} more components\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Graph Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GRAPH PROPERTIES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "largest_component = max(components, key=len)\n",
    "G_largest = G.subgraph(largest_component).copy()\n",
    "\n",
    "print(f\"Largest component:\")\n",
    "print(f\"  Size: {len(largest_component)} nodes ({100*len(largest_component)/G.number_of_nodes():.1f}%)\")\n",
    "print(f\"  Edges: {G_largest.number_of_edges()}\")\n",
    "print(f\"  Density: {nx.density(G_largest):.4f}\")\n",
    "\n",
    "# Diameter and path length (if connected)\n",
    "if nx.is_connected(G_largest):\n",
    "    diameter = nx.diameter(G_largest)\n",
    "    avg_path = nx.average_shortest_path_length(G_largest)\n",
    "    print(f\"  Diameter: {diameter} hops\")\n",
    "    print(f\"  Average path length: {avg_path:.2f} hops\")\n",
    "else:\n",
    "    diameter = None\n",
    "    avg_path = None\n",
    "    print(f\"  Diameter: N/A (not fully connected)\")\n",
    "\n",
    "clustering = nx.average_clustering(G_largest)\n",
    "print(f\"  Average clustering coefficient: {clustering:.4f}\")\n",
    "print()\n",
    "\n",
    "# Degree statistics\n",
    "degrees = [G.degree(n) for n in G.nodes()]\n",
    "print(f\"Degree statistics (all nodes):\")\n",
    "print(f\"  Min: {min(degrees)}\")\n",
    "print(f\"  Max: {max(degrees)}\")\n",
    "print(f\"  Mean: {np.mean(degrees):.2f}\")\n",
    "print(f\"  Median: {np.median(degrees):.2f}\")\n",
    "print()\n",
    "\n",
    "# Hub nodes\n",
    "degree_dict = dict(G.degree())\n",
    "hubs = sorted(degree_dict.items(), key=lambda x: -x[1])[:10]\n",
    "print(f\"Top 10 hub nodes (by degree):\")\n",
    "for unique_id, degree in hubs:\n",
    "    token_id = representative_tokens[unique_id]\n",
    "    node_type = \"BH\" if unique_id in black_hole_unique_ids else \"singleton\"\n",
    "    print(f\"  Unique ID {unique_id:5d} (token {token_id:6d}, {node_type:9s}): {degree:3d} neighbors\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Distance Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if nx.is_connected(G_largest) and len(G_largest) <= 500:\n",
    "    print(\"=\"*80)\n",
    "    print(\"DISTANCE DISTRIBUTION\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    print(\"Computing all pairwise shortest paths...\")\n",
    "    all_paths = dict(nx.all_pairs_shortest_path_length(G_largest))\n",
    "    \n",
    "    distances = []\n",
    "    for source in all_paths:\n",
    "        for target, distance in all_paths[source].items():\n",
    "            if source < target:\n",
    "                distances.append(distance)\n",
    "    \n",
    "    distance_counts = Counter(distances)\n",
    "    print(f\"\\nDistance distribution:\")\n",
    "    for dist in sorted(distance_counts.keys()):\n",
    "        count = distance_counts[dist]\n",
    "        pct = 100 * count / len(distances)\n",
    "        print(f\"  Distance {dist}: {count:6,} pairs ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTotal pairs: {len(distances):,}\")\n",
    "    print()\n",
    "else:\n",
    "    distance_counts = None\n",
    "    print(\"\\n(Skipping distance distribution: component too large or not connected)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12), dpi=100)\n",
    "\n",
    "# Plot 1: Component sizes\n",
    "ax = axes[0, 0]\n",
    "component_sizes = [len(c) for c in components_sorted]\n",
    "if len(component_sizes) > 1:\n",
    "    sizes_to_plot = component_sizes[:min(30, len(component_sizes))]\n",
    "    ax.bar(range(1, len(sizes_to_plot)+1), sizes_to_plot, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    ax.set_xlabel('Component rank', fontsize=11)\n",
    "    ax.set_ylabel('Component size (nodes)', fontsize=11)\n",
    "    ax.set_title(f'Connected Component Sizes\\n({n_components} total)', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    info_text = f'Single Connected Component\\n\\n{G.number_of_nodes()} nodes\\n{G.number_of_edges()} edges'\n",
    "    ax.text(0.5, 0.5, info_text, ha='center', va='center', fontsize=14,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Component Structure', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 2: Degree distribution\n",
    "ax = axes[0, 1]\n",
    "ax.hist(degrees, bins=range(0, max(degrees)+2), edgecolor='black', alpha=0.7, color='coral')\n",
    "ax.set_xlabel('Degree (number of neighbors)', fontsize=11)\n",
    "ax.set_ylabel('Number of nodes', fontsize=11)\n",
    "ax.set_title('Degree Distribution', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "stats_text = f\"Mean: {np.mean(degrees):.1f}\\nMedian: {np.median(degrees):.1f}\\nMax: {max(degrees)}\"\n",
    "ax.text(0.97, 0.97, stats_text, transform=ax.transAxes,\n",
    "        fontsize=9, verticalalignment='top', horizontalalignment='right',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Plot 3: Distance distribution\n",
    "ax = axes[1, 0]\n",
    "if distance_counts is not None:\n",
    "    ax.bar(sorted(distance_counts.keys()), \n",
    "           [distance_counts[k] for k in sorted(distance_counts.keys())], \n",
    "           edgecolor='black', alpha=0.7, color='mediumseagreen')\n",
    "    ax.set_xlabel('Shortest path distance (hops)', fontsize=11)\n",
    "    ax.set_ylabel('Number of node pairs', fontsize=11)\n",
    "    ax.set_title('Pairwise Distance Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'Distance distribution\\nskipped\\n(too large or not connected)', \n",
    "            ha='center', va='center', fontsize=11)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Pairwise Distance Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 4: Network visualization\n",
    "ax = axes[1, 1]\n",
    "if len(G_largest) <= 200:\n",
    "    pos = nx.spring_layout(G_largest, k=0.5, iterations=50, seed=42)\n",
    "    \n",
    "    # Color nodes by type\n",
    "    node_colors = ['red' if uid in black_hole_unique_ids else 'lightblue' \n",
    "                   for uid in G_largest.nodes()]\n",
    "    \n",
    "    nx.draw_networkx_nodes(G_largest, pos, node_size=100, node_color=node_colors, \n",
    "                           edgecolors='black', linewidths=0.5, ax=ax)\n",
    "    nx.draw_networkx_edges(G_largest, pos, alpha=0.3, width=0.5, ax=ax)\n",
    "    \n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='red', edgecolor='black', label='Black holes'),\n",
    "        Patch(facecolor='lightblue', edgecolor='black', label='Singletons')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    ax.set_title(f'Network Layout\\n(Largest Component: {len(largest_component)} nodes)', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, f'Network visualization\\nskipped\\n({len(G_largest)} nodes too large)', \n",
    "            ha='center', va='center', fontsize=11)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Network Layout', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(f\"Lattice structure detected:\")\n",
    "print(f\"  Unique vectors: {G.number_of_nodes()}\")\n",
    "print(f\"    • {n_singletons} singletons ({pct_singletons:.1f}%)\")\n",
    "print(f\"    • {n_bh_centroids_in_lattice} black hole centroids ({pct_bh:.1f}%)\")\n",
    "print(f\"  Lattice edges: {G.number_of_edges()}\")\n",
    "print(f\"  Graph density: {nx.density(G):.4f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Connectivity:\")\n",
    "print(f\"  Connected components: {n_components}\")\n",
    "print(f\"  Largest component: {len(largest_component)} nodes ({100*len(largest_component)/G.number_of_nodes():.1f}%)\")\n",
    "if diameter is not None:\n",
    "    print(f\"  Diameter: {diameter} hops\")\n",
    "    print(f\"  Average path length: {avg_path:.2f} hops\")\n",
    "print()\n",
    "\n",
    "# Interpretation\n",
    "print(\"Interpretation:\")\n",
    "if n_components == 1:\n",
    "    print(f\"  ✓ FULLY CONNECTED: All {G.number_of_nodes()} nodes form a single connected structure\")\n",
    "    if diameter is not None:\n",
    "        print(f\"  • Max distance: {diameter} lattice steps\")\n",
    "        print(f\"  • Avg distance: {avg_path:.2f} lattice steps\")\n",
    "    print(f\"  • Every node reachable from every other via single-mantissa jumps\")\n",
    "elif len(largest_component) > G.number_of_nodes() * 0.9:\n",
    "    print(f\"  ⚠️  MOSTLY CONNECTED: Giant component ({len(largest_component)} nodes) + {n_components-1} small clusters\")\n",
    "    print(f\"  • Most structure is connected, with few outliers\")\n",
    "else:\n",
    "    print(f\"  ⚠️  FRAGMENTED: {n_components} disconnected components\")\n",
    "    print(f\"  • Largest: {len(largest_component)} nodes ({100*len(largest_component)/G.number_of_nodes():.1f}%)\")\n",
    "    print(f\"  • Structure is split into separate regions\")\n",
    "\n",
    "print()\n",
    "\n",
    "if n_bh_centroids_in_lattice > 0 and n_singletons > 0:\n",
    "    print(\"Composition note:\")\n",
    "    print(f\"  • Lattice contains both black holes ({n_bh_centroids_in_lattice}) and singletons ({n_singletons})\")\n",
    "    print(f\"  • Ratio: {n_singletons/n_bh_centroids_in_lattice:.1f} singletons per black hole centroid\")\n",
    "elif n_bh_centroids_in_lattice > 0:\n",
    "    print(\"Composition note:\")\n",
    "    print(f\"  • Lattice contains ONLY black hole centroids (no singletons with neighbors)\")\n",
    "elif n_singletons > 0:\n",
    "    print(\"Composition note:\")\n",
    "    print(f\"  • Lattice contains ONLY singletons (no black holes detected)\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
