{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.11a2: FineWeb Corpus Preparation\n",
    "\n",
    "**Goal:** Download a 1MB sample from FineWeb and convert it to pure ASCII for training Lil Gatsby.\n",
    "\n",
    "## Why FineWeb?\n",
    "\n",
    "FineWeb is HuggingFace's newest web corpus (15 trillion tokens) containing:\n",
    "- Web pages from Common Crawl (2013-2024)\n",
    "- Extensive deduplication and filtering\n",
    "- High-quality English text\n",
    "- Modern, clean dataset using Parquet format\n",
    "\n",
    "Unlike Gatsby (single book, ~270k tokens), FineWeb gives us:\n",
    "- **Diversity:** Many writing styles, topics, domains\n",
    "- **Scale:** Can grab 1MB, 10MB, or more\n",
    "- **Non-repetition:** Fresh data throughout training (no memorization)\n",
    "\n",
    "## The Corpus-as-Heat-Reservoir Hypothesis\n",
    "\n",
    "**Hypothesis:** Larger, more diverse corpus keeps gradients large longer → tokens stay \"warm\" (mobile) longer → more time for structure formation before freezing.\n",
    "\n",
    "**Experiment:**\n",
    "1. Train on Gatsby (270k, repeats after 5 epochs) → freezes by step ~8k, k=2 clusters\n",
    "2. Train on FineWeb (1MB, ~10 epochs) → freezes later? More clusters?\n",
    "3. If yes → corpus size is the missing ingredient!\n",
    "\n",
    "## Processing Steps\n",
    "\n",
    "1. Load FineWeb via HuggingFace `datasets` (streaming mode)\n",
    "2. Grab enough text to reach ~1MB\n",
    "3. Convert to ASCII using `unidecode` (handles UTF-8 → ASCII gracefully)\n",
    "4. Filter to valid ASCII bytes (0-127)\n",
    "5. Save as `fineweb_1mb_ascii.txt`\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "- File: `../data/fineweb_1mb_ascii.txt`\n",
    "- Size: ~1,000,000 bytes\n",
    "- All characters in range [0, 127] (pure ASCII)\n",
    "- Used tokens: ~79 (letters, digits, punctuation)\n",
    "- Unused tokens: ~49 (control chars, special symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target corpus size\n",
    "TARGET_SIZE_BYTES = 2_000_000  # 2M\n",
    "\n",
    "# Output path\n",
    "OUTPUT_PATH = \"../data/fineweb_ascii.txt\"\n",
    "\n",
    "# FineWeb dataset\n",
    "DATASET_NAME = \"HuggingFaceFW/fineweb\"\n",
    "SPLIT = \"train\"\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from unidecode import unidecode\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FineWeb (Streaming)\n",
    "\n",
    "We use streaming mode to avoid downloading the entire 15T token dataset.\n",
    "\n",
    "**Note:** First time running this will download some index files (~few MB). Subsequent runs are fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FineWeb dataset (streaming mode)...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca17507d9304a17b60b723eab8215a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2e8c3187cc4af798c2f1d3f4a8452d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded\n",
      "  Name: HuggingFaceFW/fineweb\n",
      "  Split: train\n",
      "  Mode: streaming (on-demand download)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading FineWeb dataset (streaming mode)...\\n\")\n",
    "\n",
    "# Load in streaming mode (doesn't download everything)\n",
    "fineweb = load_dataset(DATASET_NAME, split=SPLIT, streaming=True)\n",
    "\n",
    "print(f\"✓ Dataset loaded\")\n",
    "print(f\"  Name: {DATASET_NAME}\")\n",
    "print(f\"  Split: {SPLIT}\")\n",
    "print(f\"  Mode: streaming (on-demand download)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample and Convert to ASCII\n",
    "\n",
    "We'll iterate through examples, convert each to ASCII via `unidecode`, and accumulate until we hit 1MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling 2,000,000 bytes from FineWeb...\n",
      "\n",
      "  Examples processed: 100 | Bytes collected: 252,659 / 2,000,000\n",
      "  Examples processed: 200 | Bytes collected: 640,529 / 2,000,000\n",
      "  Examples processed: 300 | Bytes collected: 957,297 / 2,000,000\n",
      "  Examples processed: 400 | Bytes collected: 1,201,090 / 2,000,000\n",
      "  Examples processed: 500 | Bytes collected: 1,469,396 / 2,000,000\n",
      "  Examples processed: 600 | Bytes collected: 1,773,172 / 2,000,000\n",
      "\n",
      "✓ Sampling complete\n",
      "  Examples used: 610\n",
      "  Total bytes: 2,001,727\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSampling {TARGET_SIZE_BYTES:,} bytes from FineWeb...\\n\")\n",
    "\n",
    "corpus_chunks = []\n",
    "total_bytes = 0\n",
    "num_examples = 0\n",
    "\n",
    "for example in fineweb:\n",
    "    # Extract text\n",
    "    text = example.get('text', '')\n",
    "    \n",
    "    if not text:\n",
    "        continue\n",
    "    \n",
    "    # Convert to ASCII (unidecode handles UTF-8 → ASCII gracefully)\n",
    "    ascii_text = unidecode(text)\n",
    "    \n",
    "    # Filter to valid ASCII bytes (0-127)\n",
    "    # We'll do this by encoding to ASCII with 'ignore' to drop non-ASCII\n",
    "    try:\n",
    "        ascii_bytes = ascii_text.encode('ascii', errors='ignore')\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to encode example {num_examples}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    if not ascii_bytes:\n",
    "        continue\n",
    "    \n",
    "    # Add to corpus\n",
    "    corpus_chunks.append(ascii_bytes)\n",
    "    total_bytes += len(ascii_bytes)\n",
    "    num_examples += 1\n",
    "    \n",
    "    # Progress indicator\n",
    "    if num_examples % 100 == 0:\n",
    "        print(f\"  Examples processed: {num_examples:,} | Bytes collected: {total_bytes:,} / {TARGET_SIZE_BYTES:,}\")\n",
    "    \n",
    "    # Stop when we hit target\n",
    "    if total_bytes >= TARGET_SIZE_BYTES:\n",
    "        break\n",
    "\n",
    "print(f\"\\n✓ Sampling complete\")\n",
    "print(f\"  Examples used: {num_examples:,}\")\n",
    "print(f\"  Total bytes: {total_bytes:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Trim to Exact Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining chunks...\n",
      "\n",
      "✓ Corpus ready\n",
      "  Size: 2,000,000 bytes (1953.1 KB)\n",
      "  Characters: 2,000,000\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCombining chunks...\\n\")\n",
    "\n",
    "# Combine all chunks\n",
    "corpus_bytes = b''.join(corpus_chunks)\n",
    "\n",
    "# Trim to exact target size (if we overshot)\n",
    "corpus_bytes = corpus_bytes[:TARGET_SIZE_BYTES]\n",
    "\n",
    "# Decode to string\n",
    "corpus_text = corpus_bytes.decode('ascii')\n",
    "\n",
    "print(f\"✓ Corpus ready\")\n",
    "print(f\"  Size: {len(corpus_bytes):,} bytes ({len(corpus_bytes) / 1024:.1f} KB)\")\n",
    "print(f\"  Characters: {len(corpus_text):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing corpus...\n",
      "\n",
      "✓ Corpus statistics\n",
      "  Unique bytes used: 94 / 128\n",
      "  Unused bytes: 34\n",
      "\n",
      "Top 10 most common bytes:\n",
      "   1. ' ' (byte  32):  332,309 (16.62%)\n",
      "   2. 'e' (byte 101):  186,940 ( 9.35%)\n",
      "   3. 't' (byte 116):  136,595 ( 6.83%)\n",
      "   4. 'a' (byte  97):  124,195 ( 6.21%)\n",
      "   5. 'o' (byte 111):  120,346 ( 6.02%)\n",
      "   6. 'i' (byte 105):  107,923 ( 5.40%)\n",
      "   7. 'n' (byte 110):  107,816 ( 5.39%)\n",
      "   8. 's' (byte 115):   98,548 ( 4.93%)\n",
      "   9. 'r' (byte 114):   94,539 ( 4.73%)\n",
      "  10. 'h' (byte 104):   72,259 ( 3.61%)\n",
      "\n",
      "Unused ASCII bytes (untrained tokens):\n",
      "  Count: 34\n",
      "  Bytes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]...\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAnalyzing corpus...\\n\")\n",
    "\n",
    "# Count unique bytes\n",
    "unique_bytes = set(corpus_bytes)\n",
    "byte_counts = {b: corpus_bytes.count(bytes([b])) for b in unique_bytes}\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_bytes = sorted(byte_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"✓ Corpus statistics\")\n",
    "print(f\"  Unique bytes used: {len(unique_bytes)} / 128\")\n",
    "print(f\"  Unused bytes: {128 - len(unique_bytes)}\")\n",
    "print()\n",
    "print(f\"Top 10 most common bytes:\")\n",
    "for i, (byte, count) in enumerate(sorted_bytes[:10], 1):\n",
    "    char = chr(byte) if 32 <= byte < 127 else f\"<{byte}>\"\n",
    "    freq = 100 * count / len(corpus_bytes)\n",
    "    print(f\"  {i:2d}. '{char}' (byte {byte:3d}): {count:8,} ({freq:5.2f}%)\")\n",
    "\n",
    "print()\n",
    "print(f\"Unused ASCII bytes (untrained tokens):\")\n",
    "unused_bytes = sorted(set(range(128)) - unique_bytes)\n",
    "print(f\"  Count: {len(unused_bytes)}\")\n",
    "print(f\"  Bytes: {unused_bytes[:20]}{'...' if len(unused_bytes) > 20 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving corpus to: ../data/fineweb_ascii.txt\n",
      "\n",
      "✓ Corpus saved\n",
      "  File: ../data/fineweb_ascii.txt\n",
      "  Size: 2,000,000 bytes\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving corpus to: {OUTPUT_PATH}\\n\")\n",
    "\n",
    "# Create directory if needed\n",
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as text file\n",
    "with open(OUTPUT_PATH, 'w', encoding='ascii') as f:\n",
    "    f.write(corpus_text)\n",
    "\n",
    "print(f\"✓ Corpus saved\")\n",
    "print(f\"  File: {OUTPUT_PATH}\")\n",
    "print(f\"  Size: {Path(OUTPUT_PATH).stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CORPUS PREVIEW (first 500 characters)\n",
      "================================================================================\n",
      "\n",
      "How AP reported in all formats from tornado-stricken regionsMarch 8, 2012\n",
      "When the first serious bout of tornadoes of 2012 blew through middle America in the middle of the night, they touched down in places hours from any AP bureau. Our closest video journalist was Chicago-based Robert Ray, who dropped his plans to travel to Georgia for Super Tuesday, booked several flights to the cities closest to the strikes and headed for the airport. He'd decide once there which flight to take.\n",
      "He never got \n",
      "\n",
      "[... 1,999,500 more characters ...]\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"CORPUS PREVIEW (first 500 characters)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(corpus_text[:500])\n",
    "print(f\"\\n[... {len(corpus_text) - 500:,} more characters ...]\")\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Source: FineWeb (HuggingFaceFW/fineweb)\n",
      "  Examples sampled: 610\n",
      "  Total bytes: 2,000,000\n",
      "\n",
      "Output: ../data/fineweb_ascii.txt\n",
      "  Unique bytes: 94 / 128\n",
      "  Trained tokens: 94 (bytes that appear in corpus)\n",
      "  Untrained tokens: 34 (bytes that never appear)\n",
      "\n",
      "Comparison to Gatsby:\n",
      "  Gatsby: 268,928 bytes, ~79 trained tokens\n",
      "  FineWeb (1MB): 2,000,000 bytes, ~94 trained tokens\n",
      "  Size ratio: 7.4x larger\n",
      "\n",
      "Next steps:\n",
      "  1. Run 1.12a with CORPUS_PATH = '../data/fineweb_ascii.txt'\n",
      "  2. Train for 10,000 steps (or more)\n",
      "  3. Run 1.13d to analyze bounding hypersphere dynamics\n",
      "  4. Compare to Gatsby results:\n",
      "     - Does freezing happen later?\n",
      "     - More fragmentation (k > 2)?\n",
      "     - Stronger contraction (R < 0.126)?\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Source: FineWeb (HuggingFaceFW/fineweb)\")\n",
    "print(f\"  Examples sampled: {num_examples:,}\")\n",
    "print(f\"  Total bytes: {len(corpus_bytes):,}\")\n",
    "print()\n",
    "print(f\"Output: {OUTPUT_PATH}\")\n",
    "print(f\"  Unique bytes: {len(unique_bytes)} / 128\")\n",
    "print(f\"  Trained tokens: {len(unique_bytes)} (bytes that appear in corpus)\")\n",
    "print(f\"  Untrained tokens: {len(unused_bytes)} (bytes that never appear)\")\n",
    "print()\n",
    "print(f\"Comparison to Gatsby:\")\n",
    "print(f\"  Gatsby: 268,928 bytes, ~79 trained tokens\")\n",
    "print(f\"  FineWeb (1MB): {len(corpus_bytes):,} bytes, ~{len(unique_bytes)} trained tokens\")\n",
    "print(f\"  Size ratio: {len(corpus_bytes) / 268928:.1f}x larger\")\n",
    "print()\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  1. Run 1.12a with CORPUS_PATH = '{OUTPUT_PATH}'\")\n",
    "print(f\"  2. Train for 10,000 steps (or more)\")\n",
    "print(f\"  3. Run 1.13d to analyze bounding hypersphere dynamics\")\n",
    "print(f\"  4. Compare to Gatsby results:\")\n",
    "print(f\"     - Does freezing happen later?\")\n",
    "print(f\"     - More fragmentation (k > 2)?\")\n",
    "print(f\"     - Stronger contraction (R < 0.126)?\")\n",
    "print()\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
