{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.12f: Wordybird 1b - Clean 200-Step Run (No Checkpoint Artifact)\n",
    "\n",
    "**Experiment:** Exact copy of Wordybird 1, but run for 200 steps straight through without checkpointing.\n",
    "\n",
    "## The Checkpoint Artifact Problem\n",
    "\n",
    "**Wordybird 1 (0-100):** Sharp freeze around step 42, nearly complete by step 70.\n",
    "\n",
    "**Wordybird 3 (100-200, resumed from checkpoint):** Massive spike at step 100 when training resumes, then rapid decay and freeze by ~step 125.\n",
    "\n",
    "**The Artifact:** WB1+WB3 stitched together shows an unnatural \"kick\" at the seam (step 100) that doesn't match continuous training dynamics.\n",
    "\n",
    "## Test\n",
    "\n",
    "Run Wordybird 1b: **same seed, same hyperparameters, same everything** as WB1, but:\n",
    "- Train for 200 steps continuously (no checkpointing)\n",
    "- Record every step throughout\n",
    "\n",
    "This will show us the TRUE dynamics of steps 0-200 without checkpoint artifacts.\n",
    "\n",
    "## Wordybird 1b Parameters\n",
    "\n",
    "**Identical to Wordybird 1 except:**\n",
    "- **Steps: 200** (was 100)\n",
    "- **Output: `1.12f_wordybird_1b.safetensors`**\n",
    "\n",
    "Everything else unchanged:\n",
    "- Same random seed (42)\n",
    "- Same model architecture (2 layers, 2 heads, 64D)\n",
    "- Same training config (batch=32, lr=0.001, Adam)\n",
    "- Same corpus (FineWeb 2MB)\n",
    "- Same initialization (N(0, 0.02) bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "VOCAB_SIZE = 50257  # GPT-2\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYER = 2\n",
    "N_HEAD = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "NUM_TRAIN_STEPS = 200  # ← CHANGED from 100\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# Optimizer: Adam\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "# Initialization (bfloat16 native)\n",
    "INIT_SCALE = 0.02  # N(0, 0.02)\n",
    "\n",
    "# Data\n",
    "CORPUS_PATH = \"../data/fineweb_2mb_unicode.txt\"\n",
    "TOKEN_MASK_PATH = \"../tensors/Wordybird/fineweb_token_masks.safetensors\"\n",
    "OUTPUT_DIR = \"../tensors/Wordybird\"\n",
    "OUTPUT_FILE = \"1.12f_wordybird_1b.safetensors\"  # ← CHANGED\n",
    "\n",
    "# Instrumentation\n",
    "RECORD_EVERY_N_STEPS = 1  # Record every step\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file, load_file\n",
    "import time\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 tokenizer...\n",
      "\n",
      "✓ Loaded GPT-2 tokenizer\n",
      "  Vocabulary size: 50,257 tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading GPT-2 tokenizer...\\n\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(f\"✓ Loaded GPT-2 tokenizer\")\n",
    "print(f\"  Vocabulary size: {len(tokenizer):,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading corpus: ../data/fineweb_2mb_unicode.txt\n",
      "\n",
      "✓ Loaded corpus\n",
      "  Size: 2.00 MB\n",
      "  Characters: 2,089,201\n",
      "\n",
      "Tokenizing corpus...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (475160 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenized\n",
      "  Tokens: 475,160\n",
      "\n",
      "✓ Corpus on device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading corpus: {CORPUS_PATH}\\n\")\n",
    "\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "corpus_bytes = len(corpus_text.encode('utf-8'))\n",
    "corpus_mb = corpus_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"✓ Loaded corpus\")\n",
    "print(f\"  Size: {corpus_mb:.2f} MB\")\n",
    "print(f\"  Characters: {len(corpus_text):,}\")\n",
    "print()\n",
    "\n",
    "# Tokenize\n",
    "print(\"Tokenizing corpus...\\n\")\n",
    "tokens = tokenizer.encode(corpus_text)\n",
    "\n",
    "print(f\"✓ Tokenized\")\n",
    "print(f\"  Tokens: {len(tokens):,}\")\n",
    "print()\n",
    "\n",
    "# Pre-load to device\n",
    "corpus_tensor = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "print(f\"✓ Corpus on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Token Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading token masks: ../tensors/Wordybird/fineweb_token_masks.safetensors\n",
      "\n",
      "✓ Loaded token masks\n",
      "  Trained tokens: 30,590 (60.9%)\n",
      "  Untrained tokens: 19,667 (39.1%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading token masks: {TOKEN_MASK_PATH}\\n\")\n",
    "\n",
    "mask_data = load_file(TOKEN_MASK_PATH)\n",
    "trained_mask = mask_data['trained_mask']\n",
    "untrained_mask = mask_data['untrained_mask']\n",
    "trained_indices = mask_data['trained_indices']\n",
    "untrained_indices = mask_data['untrained_indices']\n",
    "\n",
    "n_trained = trained_mask.sum().item()\n",
    "n_untrained = untrained_mask.sum().item()\n",
    "\n",
    "print(f\"✓ Loaded token masks\")\n",
    "print(f\"  Trained tokens: {n_trained:,} ({100*n_trained/VOCAB_SIZE:.1f}%)\")\n",
    "print(f\"  Untrained tokens: {n_untrained:,} ({100*n_untrained/VOCAB_SIZE:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset: 475,032 examples\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = TokenDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "print(f\"\\n✓ Dataset: {len(dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating model...\n",
      "\n",
      "✓ Model created\n",
      "  Total parameters: 3,324,736\n",
      "  Embedding parameters (E+W): 3,216,448\n",
      "  Other parameters: 108,288\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCreating model...\\n\")\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "embedding_params = model.transformer.wte.weight.numel()\n",
    "\n",
    "print(f\"✓ Model created\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Embedding parameters (E+W): {embedding_params:,}\")\n",
    "print(f\"  Other parameters: {total_params - embedding_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization (bfloat16 Native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INITIALIZING: N(0, 0.02) bfloat16-native\n",
      "================================================================================\n",
      "\n",
      "Initialization: N(0, 0.02)\n",
      "  Shape: torch.Size([50257, 64])\n",
      "  Each token initialized independently\n",
      "\n",
      "✓ Initialized embeddings (pure bfloat16)\n",
      "  Shape: torch.Size([50257, 64])\n",
      "  Dtype: torch.bfloat16\n",
      "\n",
      "Initial untrained token statistics (19,667 tokens):\n",
      "  Centroid norm: 0.001090\n",
      "  Mean radius from centroid: 0.159273\n",
      "  Max radius from centroid: 0.219594\n",
      "  Bounding hypersphere volume ∝ R^64 = 7.31e-43\n",
      "\n",
      "  ✓ Tokens distributed in hypersphere (standard init)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"INITIALIZING: N(0, {INIT_SCALE}) bfloat16-native\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Standard GPT-2 init: each embedding drawn independently from N(0, 0.02)\n",
    "# Generate in float32, immediately convert to bfloat16\n",
    "init_f32 = torch.randn(VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32, device=device) * INIT_SCALE\n",
    "init_bf16 = init_f32.to(torch.bfloat16)\n",
    "\n",
    "print(f\"Initialization: N(0, {INIT_SCALE})\")\n",
    "print(f\"  Shape: {init_bf16.shape}\")\n",
    "print(f\"  Each token initialized independently\")\n",
    "print()\n",
    "\n",
    "# Assign to model\n",
    "with torch.no_grad():\n",
    "    model.transformer.wte.weight[:] = init_bf16\n",
    "\n",
    "print(f\"✓ Initialized embeddings (pure bfloat16)\")\n",
    "print(f\"  Shape: {model.transformer.wte.weight.shape}\")\n",
    "print(f\"  Dtype: {model.transformer.wte.weight.dtype}\")\n",
    "print()\n",
    "\n",
    "# Verify initialization stats\n",
    "W_check = model.transformer.wte.weight.cpu().float()\n",
    "W_untrained = W_check[untrained_indices]\n",
    "\n",
    "centroid = W_untrained.mean(dim=0)\n",
    "centroid_norm = torch.norm(centroid).item()\n",
    "radii = torch.norm(W_untrained - centroid, dim=1)\n",
    "mean_radius = radii.mean().item()\n",
    "max_radius = radii.max().item()\n",
    "\n",
    "print(f\"Initial untrained token statistics ({n_untrained:,} tokens):\")\n",
    "print(f\"  Centroid norm: {centroid_norm:.6f}\")\n",
    "print(f\"  Mean radius from centroid: {mean_radius:.6f}\")\n",
    "print(f\"  Max radius from centroid: {max_radius:.6f}\")\n",
    "print(f\"  Bounding hypersphere volume ∝ R^{HIDDEN_DIM} = {max_radius**HIDDEN_DIM:.2e}\")\n",
    "print()\n",
    "print(f\"  ✓ Tokens distributed in hypersphere (standard init)\")\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Recorder class defined\n"
     ]
    }
   ],
   "source": [
    "class ComprehensiveRecorder:\n",
    "    \"\"\"Records embeddings, gradients, optimizer state, logits, loss at every step in bfloat16.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim, record_every_n):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.record_every_n = record_every_n\n",
    "        \n",
    "        # Storage (lists of tensors, keep in RAM)\n",
    "        self.recorded_steps = []\n",
    "        self.embeddings = []      # [n_recorded, vocab_size, hidden_dim]\n",
    "        self.grads = []           # [n_recorded, vocab_size, hidden_dim]\n",
    "        self.momentum = []        # [n_recorded, vocab_size, hidden_dim]\n",
    "        self.variance = []        # [n_recorded, vocab_size, hidden_dim]\n",
    "        self.logits = []          # [n_recorded, vocab_size]\n",
    "        self.losses = []          # [n_recorded]\n",
    "        \n",
    "        # Temporary storage\n",
    "        self.current_step = 0\n",
    "        self.recorded_initial = False\n",
    "        self.grad_before = None\n",
    "        self.loss_value = None\n",
    "        self.logits_sample = None\n",
    "    \n",
    "    def record_initial_state(self, model, optimizer):\n",
    "        \"\"\"Record step 0: initial state before training.\"\"\"\n",
    "        if not self.recorded_initial:\n",
    "            W = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "            \n",
    "            # Step 0: no gradients, no optimizer state yet (zeros)\n",
    "            self.recorded_steps.append(0)\n",
    "            self.embeddings.append(W)\n",
    "            self.grads.append(torch.zeros_like(W))\n",
    "            self.momentum.append(torch.zeros_like(W))\n",
    "            self.variance.append(torch.zeros_like(W))\n",
    "            self.logits.append(torch.zeros(self.vocab_size, dtype=torch.bfloat16))\n",
    "            self.losses.append(torch.tensor(float('nan'), dtype=torch.bfloat16))  # No loss yet\n",
    "            \n",
    "            self.recorded_initial = True\n",
    "            self.current_step = 1\n",
    "            \n",
    "            print(f\"✓ Recorded initial state (step 0)\")\n",
    "    \n",
    "    def record_before_step(self, model, loss, logits):\n",
    "        \"\"\"Call after forward/backward, before optimizer step.\"\"\"\n",
    "        if self.current_step % self.record_every_n == 0:\n",
    "            # Capture gradients in bfloat16\n",
    "            if model.transformer.wte.weight.grad is not None:\n",
    "                self.grad_before = model.transformer.wte.weight.grad.clone().cpu().bfloat16()\n",
    "            else:\n",
    "                self.grad_before = torch.zeros(self.vocab_size, self.hidden_dim, dtype=torch.bfloat16)\n",
    "            \n",
    "            # Capture loss\n",
    "            self.loss_value = loss.item()\n",
    "            \n",
    "            # Capture logits from first sequence, last position in bfloat16\n",
    "            self.logits_sample = logits[0, -1, :].detach().cpu().bfloat16()\n",
    "    \n",
    "    def record_after_step(self, model, optimizer):\n",
    "        \"\"\"Call after optimizer step.\"\"\"\n",
    "        if self.current_step % self.record_every_n == 0:\n",
    "            if self.grad_before is not None and self.loss_value is not None:\n",
    "                # Capture embeddings in bfloat16\n",
    "                W = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "\n",
    "                # Capture optimizer state (Adam momentum and variance)\n",
    "                param = model.transformer.wte.weight\n",
    "                if param in optimizer.state:\n",
    "                    state = optimizer.state[param]\n",
    "                    # Get state tensors if they exist, convert to bfloat16\n",
    "                    mom_src = state.get('exp_avg', None)\n",
    "                    var_src = state.get('exp_avg_sq', None)\n",
    "                    mom = mom_src.clone().cpu().bfloat16() if mom_src is not None else torch.zeros_like(W)\n",
    "                    var = var_src.clone().cpu().bfloat16() if var_src is not None else torch.zeros_like(W)\n",
    "                else:\n",
    "                    mom = torch.zeros_like(W)\n",
    "                    var = torch.zeros_like(W)\n",
    "\n",
    "                # Store everything\n",
    "                self.recorded_steps.append(self.current_step)\n",
    "                self.embeddings.append(W)\n",
    "                self.grads.append(self.grad_before)\n",
    "                self.momentum.append(mom)\n",
    "                self.variance.append(var)\n",
    "                self.logits.append(self.logits_sample)\n",
    "                self.losses.append(torch.tensor(self.loss_value, dtype=torch.bfloat16))\n",
    "\n",
    "                # Clear temp storage\n",
    "                self.grad_before = None\n",
    "                self.loss_value = None\n",
    "                self.logits_sample = None\n",
    "                \n",
    "                # Progress indicator every 20 steps\n",
    "                if self.current_step % 20 == 0:\n",
    "                    print(f\"  Recorded step {self.current_step}\")\n",
    "\n",
    "        self.current_step += 1\n",
    "    \n",
    "    def get_data(self):\n",
    "        \"\"\"Return recorded data as stacked tensors.\"\"\"\n",
    "        print(f\"\\nStacking {len(self.embeddings)} recorded states...\")\n",
    "        \n",
    "        return {\n",
    "            'recorded_steps': torch.tensor(self.recorded_steps, dtype=torch.long),\n",
    "            'embeddings': torch.stack(self.embeddings) if self.embeddings else torch.tensor([]),\n",
    "            'grads': torch.stack(self.grads) if self.grads else torch.tensor([]),\n",
    "            'momentum': torch.stack(self.momentum) if self.momentum else torch.tensor([]),\n",
    "            'variance': torch.stack(self.variance) if self.variance else torch.tensor([]),\n",
    "            'logits': torch.stack(self.logits) if self.logits else torch.tensor([]),\n",
    "            'losses': torch.stack(self.losses) if self.losses else torch.tensor([]),\n",
    "        }\n",
    "\n",
    "print(\"✓ Recorder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Trainer with Instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ InstrumentedTrainer defined\n"
     ]
    }
   ],
   "source": [
    "class InstrumentedTrainer(Trainer):\n",
    "    def __init__(self, recorder, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.recorder = recorder\n",
    "        self.last_logits = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"Override to capture logits.\"\"\"\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Store logits for recorder\n",
    "        self.last_logits = outputs.logits\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        \"\"\"Override to inject recording.\"\"\"\n",
    "        # Standard forward + backward\n",
    "        loss = super().training_step(model, inputs, num_items_in_batch)\n",
    "        \n",
    "        # Record BEFORE optimizer step\n",
    "        self.recorder.record_before_step(model, loss, self.last_logits)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time=None, **kwargs):\n",
    "        \"\"\"Override to record AFTER optimizer step.\"\"\"\n",
    "        # Record AFTER optimizer updates parameters\n",
    "        self.recorder.record_after_step(model, self.optimizer)\n",
    "        \n",
    "        # Call parent\n",
    "        super()._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, **kwargs)\n",
    "\n",
    "print(\"✓ InstrumentedTrainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Trainer ready (Adam, bf16=True, batch_size=32)\n"
     ]
    }
   ],
   "source": [
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "recorder = ComprehensiveRecorder(VOCAB_SIZE, HIDDEN_DIM, RECORD_EVERY_N_STEPS)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=NUM_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    adam_beta1=ADAM_BETA1,\n",
    "    adam_beta2=ADAM_BETA2,\n",
    "    adam_epsilon=ADAM_EPSILON,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=20,\n",
    "    save_steps=NUM_TRAIN_STEPS + 1,  # Don't save checkpoints\n",
    "    save_total_limit=0,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    "    bf16=True,  # Native bfloat16 training\n",
    "    seed=RANDOM_SEED,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = InstrumentedTrainer(\n",
    "    recorder=recorder,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Trainer ready (Adam, bf16=True, batch_size=32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Initial State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Recorded initial state (step 0)\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "recorder.record_initial_state(model, trainer.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "**200 steps should take ~20-30 seconds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING WORDYBIRD 1B TRAINING (CLEAN 200-STEP RUN)\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Vocabulary: 50,257 tokens (GPT-2)\n",
      "  Hidden dim: 64\n",
      "  Trained tokens: 30,590 (60.9%)\n",
      "  Untrained tokens: 19,667 (39.1%)\n",
      "\n",
      "  Initialization: N(0, 0.02) bfloat16-native\n",
      "  Optimizer: Adam (lr=0.001)\n",
      "  Precision: bfloat16 (native)\n",
      "  Batch size: 32\n",
      "  Steps: 200 (continuous, no checkpointing)\n",
      "  Recording: every step\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>10.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>9.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>8.372200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.993100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.783800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>7.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>7.641500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>7.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>7.656100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.635400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Recorded step 20\n",
      "  Recorded step 40\n",
      "  Recorded step 60\n",
      "  Recorded step 80\n",
      "  Recorded step 100\n",
      "  Recorded step 120\n",
      "  Recorded step 140\n",
      "  Recorded step 160\n",
      "  Recorded step 180\n",
      "  Recorded step 200\n",
      "\n",
      "================================================================================\n",
      "✓ Training complete\n",
      "  Elapsed time: 23.9 seconds\n",
      "  Throughput: 8.4 steps/second\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STARTING WORDYBIRD 1B TRAINING (CLEAN 200-STEP RUN)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Vocabulary: {VOCAB_SIZE:,} tokens (GPT-2)\")\n",
    "print(f\"  Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"  Trained tokens: {n_trained:,} ({100*n_trained/VOCAB_SIZE:.1f}%)\")\n",
    "print(f\"  Untrained tokens: {n_untrained:,} ({100*n_untrained/VOCAB_SIZE:.1f}%)\")\n",
    "print()\n",
    "print(f\"  Initialization: N(0, {INIT_SCALE}) bfloat16-native\")\n",
    "print(f\"  Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"  Precision: bfloat16 (native)\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Steps: {NUM_TRAIN_STEPS} (continuous, no checkpointing)\")\n",
    "print(f\"  Recording: every step\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training complete\")\n",
    "print(f\"  Elapsed time: {elapsed:.1f} seconds\")\n",
    "print(f\"  Throughput: {NUM_TRAIN_STEPS / elapsed:.1f} steps/second\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Recorded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data for save...\n",
      "\n",
      "\n",
      "Stacking 201 recorded states...\n",
      "Saving to: ../tensors/Wordybird/1.12f_wordybird_1b.safetensors\n",
      "\n",
      "✓ Saved successfully\n",
      "  File: ../tensors/Wordybird/1.12f_wordybird_1b.safetensors\n",
      "  Size: 5192.3 MB\n",
      "  Save time: 4.4 seconds\n",
      "  Recorded steps: 201\n",
      "  Step range: 0 to 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nPreparing data for save...\\n\")\n",
    "\n",
    "recorded_data = recorder.get_data()\n",
    "\n",
    "save_dict = {\n",
    "    'recorded_steps': recorded_data['recorded_steps'],\n",
    "    'embeddings': recorded_data['embeddings'],\n",
    "    'grads': recorded_data['grads'],\n",
    "    'momentum': recorded_data['momentum'],\n",
    "    'variance': recorded_data['variance'],\n",
    "    'logits': recorded_data['logits'],\n",
    "    'losses': recorded_data['losses'],\n",
    "    # Metadata\n",
    "    'init_scale': torch.tensor(INIT_SCALE, dtype=torch.float32),\n",
    "    'learning_rate': torch.tensor(LEARNING_RATE, dtype=torch.float32),\n",
    "    'weight_decay': torch.tensor(WEIGHT_DECAY, dtype=torch.float32),\n",
    "    'adam_beta1': torch.tensor(ADAM_BETA1, dtype=torch.float32),\n",
    "    'adam_beta2': torch.tensor(ADAM_BETA2, dtype=torch.float32),\n",
    "    'n_trained': torch.tensor(n_trained, dtype=torch.long),\n",
    "    'n_untrained': torch.tensor(n_untrained, dtype=torch.long),\n",
    "}\n",
    "\n",
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILE\n",
    "\n",
    "print(f\"Saving to: {output_path}\")\n",
    "\n",
    "save_start = time.time()\n",
    "save_file(save_dict, str(output_path))\n",
    "save_elapsed = time.time() - save_start\n",
    "\n",
    "file_size_mb = output_path.stat().st_size / 1e6\n",
    "\n",
    "print(f\"\\n✓ Saved successfully\")\n",
    "print(f\"  File: {output_path}\")\n",
    "print(f\"  Size: {file_size_mb:.1f} MB\")\n",
    "print(f\"  Save time: {save_elapsed:.1f} seconds\")\n",
    "print(f\"  Recorded steps: {len(recorded_data['recorded_steps'])}\")\n",
    "print(f\"  Step range: {recorded_data['recorded_steps'][0]} to {recorded_data['recorded_steps'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUICK VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Data shapes:\n",
      "  embeddings: torch.Size([201, 50257, 64])\n",
      "  losses: torch.Size([201])\n",
      "\n",
      "Untrained token displacement (steps 0 → 200):\n",
      "  Max: 4.55e-01\n",
      "  Mean: 4.34e-01\n",
      "  Median: 4.34e-01\n",
      "\n",
      "Loss trajectory:\n",
      "  Step 1: 10.8125\n",
      "  Step 200: 7.7188\n",
      "  Reduction: 3.0938\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"QUICK VERIFICATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "embeddings = recorded_data['embeddings']\n",
    "losses = recorded_data['losses']\n",
    "\n",
    "print(f\"Data shapes:\")\n",
    "print(f\"  embeddings: {embeddings.shape}\")\n",
    "print(f\"  losses: {losses.shape}\")\n",
    "print()\n",
    "\n",
    "# Analyze untrained token movement\n",
    "W_step0 = embeddings[0, untrained_indices].float()\n",
    "W_step200 = embeddings[-1, untrained_indices].float()\n",
    "\n",
    "# Compute displacements\n",
    "displacements = torch.norm(W_step200 - W_step0, dim=1)\n",
    "max_displacement = displacements.max().item()\n",
    "mean_displacement = displacements.mean().item()\n",
    "median_displacement = displacements.median().item()\n",
    "\n",
    "print(f\"Untrained token displacement (steps 0 → 200):\")\n",
    "print(f\"  Max: {max_displacement:.2e}\")\n",
    "print(f\"  Mean: {mean_displacement:.2e}\")\n",
    "print(f\"  Median: {median_displacement:.2e}\")\n",
    "print()\n",
    "\n",
    "print(f\"Loss trajectory:\")\n",
    "print(f\"  Step 1: {losses[1].float().item():.4f}\")\n",
    "print(f\"  Step 200: {losses[-1].float().item():.4f}\")\n",
    "print(f\"  Reduction: {(losses[1].float() - losses[-1].float()).item():.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WORDYBIRD 1B COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Experiment: Clean 200-step run (no checkpoint artifact)\n",
      "  Steps: 0-200 continuous\n",
      "  Data saved: ../tensors/Wordybird/1.12f_wordybird_1b.safetensors\n",
      "  Size: 5192.3 MB\n",
      "\n",
      "Next steps:\n",
      "  1. Run 1.17b lattice hop analysis on WB1b\n",
      "  2. Compare to WB1+WB3 stitched dynamics\n",
      "  3. Determine if step-100 'kick' was a checkpoint artifact\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"WORDYBIRD 1B COMPLETE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Experiment: Clean 200-step run (no checkpoint artifact)\")\n",
    "print(f\"  Steps: 0-200 continuous\")\n",
    "print(f\"  Data saved: {output_path}\")\n",
    "print(f\"  Size: {file_size_mb:.1f} MB\")\n",
    "print()\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  1. Run 1.17b lattice hop analysis on WB1b\")\n",
    "print(f\"  2. Compare to WB1+WB3 stitched dynamics\")\n",
    "print(f\"  3. Determine if step-100 'kick' was a checkpoint artifact\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
