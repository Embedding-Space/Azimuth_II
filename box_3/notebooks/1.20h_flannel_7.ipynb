{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.20h: Flannel 7 - Single Deterministic Run with Gradients\n",
    "\n",
    "**Purpose:** Create canonical reference trajectory for Flannel analysis.\n",
    "\n",
    "## What This Records\n",
    "\n",
    "Single training run (seed 42) for 500 steps, recording:\n",
    "- **W**: Embedding matrix at every step\n",
    "- **∇W**: Gradients at every step (the force field driving token motion)\n",
    "- **Losses**: Training loss at every step\n",
    "\n",
    "This gives us both the trajectory (W) and the forces (∇W) that cause it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parameters set\n"
     ]
    }
   ],
   "source": [
    "# === BATCH EXPERIMENT CONFIG ===\n",
    "NUM_RUNS = 1           # Single canonical run\n",
    "INIT_SEED = 42         # Seed for creating initial W\n",
    "BASE_TRAIN_SEED = 42   # Training seed\n",
    "\n",
    "# === RECORDING CONFIG ===\n",
    "RECORD_CONFIG = {\n",
    "    'W': True,\n",
    "    'grads': True,      # NEW: Record gradients\n",
    "    'momentum': False,\n",
    "    'variance': False,\n",
    "    'logits': False,\n",
    "    'losses': True,\n",
    "}\n",
    "\n",
    "# === MODEL ARCHITECTURE ===\n",
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYER = 2\n",
    "N_HEAD = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# === TRAINING CONFIG ===\n",
    "BATCH_SIZE = 32\n",
    "NUM_TRAIN_STEPS = 500  # NEW: Reduced from 1000 to 500\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# Optimizer: Adam\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "# Initialization\n",
    "INIT_SCALE = 0.02  # N(0, 0.02)\n",
    "\n",
    "# === DATA PATHS ===\n",
    "TOKENIZER_PATH = \"../data/flannel_tokenizer_chars.json\"\n",
    "CORPUS_PATH = \"../data/flannel_model_corpus.txt\"\n",
    "TOKEN_MASK_PATH = \"../tensors/Flannel/live_dead_tokens.safetensors\"\n",
    "OUTPUT_DIR = \"../tensors/Flannel\"\n",
    "OUTPUT_FILE = \"1.20h_flannel_7.safetensors\"\n",
    "\n",
    "print(\"✓ Parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file, load_file\n",
    "import time\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory & Disk Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MEMORY & DISK REQUIREMENTS\n",
      "================================================================================\n",
      "\n",
      "Experiment: Flannel 7 - Single canonical run\n",
      "  Initialization seed: 42\n",
      "  Training seed:       42\n",
      "  Steps:               500\n",
      "\n",
      "Recording: W, grads, losses\n",
      "  W            0.64 GB\n",
      "  grads        0.64 GB\n",
      "  losses       0.00 GB\n",
      "  Total data:  1.28 GB\n",
      "\n",
      "Model parameters: 738,304\n",
      "  Model (bf16):     0.00 GB\n",
      "  Optimizer (fp32): 0.01 GB\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "PEAK RAM:     1.29 GB\n",
      "DISK NEEDED:  1.28 GB\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "✓ Resources within budget\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MEMORY & DISK REQUIREMENTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "bytes_per_element = 2  # bfloat16\n",
    "\n",
    "# Calculate recording size\n",
    "tensor_sizes = {}\n",
    "if RECORD_CONFIG['W']:\n",
    "    tensor_sizes['W'] = NUM_RUNS * (NUM_TRAIN_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_per_element\n",
    "if RECORD_CONFIG['grads']:\n",
    "    tensor_sizes['grads'] = NUM_RUNS * (NUM_TRAIN_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_per_element\n",
    "if RECORD_CONFIG['losses']:\n",
    "    tensor_sizes['losses'] = NUM_RUNS * (NUM_TRAIN_STEPS+1) * bytes_per_element\n",
    "\n",
    "total_recorded = sum(tensor_sizes.values())\n",
    "\n",
    "# Model memory\n",
    "embedding_params = VOCAB_SIZE * HIDDEN_DIM\n",
    "params_per_layer = 12 * HIDDEN_DIM**2\n",
    "transformer_params = N_LAYER * params_per_layer\n",
    "total_model_params = embedding_params + transformer_params\n",
    "model_memory = total_model_params * bytes_per_element\n",
    "optimizer_memory = 2 * total_model_params * 4\n",
    "\n",
    "peak_ram = total_recorded + model_memory + optimizer_memory\n",
    "\n",
    "print(f\"Experiment: Flannel 7 - Single canonical run\")\n",
    "print(f\"  Initialization seed: {INIT_SEED}\")\n",
    "print(f\"  Training seed:       {BASE_TRAIN_SEED}\")\n",
    "print(f\"  Steps:               {NUM_TRAIN_STEPS:,}\")\n",
    "print()\n",
    "print(f\"Recording: {', '.join([k for k, v in RECORD_CONFIG.items() if v])}\")\n",
    "for name, size in tensor_sizes.items():\n",
    "    print(f\"  {name:12} {size/1e9:.2f} GB\")\n",
    "print(f\"  {'Total data:':12} {total_recorded/1e9:.2f} GB\")\n",
    "print()\n",
    "print(f\"Model parameters: {total_model_params:,}\")\n",
    "print(f\"  Model (bf16):     {model_memory/1e9:.2f} GB\")\n",
    "print(f\"  Optimizer (fp32): {optimizer_memory/1e9:.2f} GB\")\n",
    "print()\n",
    "print(f\"{'─'*80}\")\n",
    "print(f\"PEAK RAM:     {peak_ram/1e9:.2f} GB\")\n",
    "print(f\"DISK NEEDED:  {total_recorded/1e9:.2f} GB\")\n",
    "print(f\"{'─'*80}\")\n",
    "\n",
    "if peak_ram <= 24e9:\n",
    "    print(f\"\\n✓ Resources within budget\\n\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  WARNING: Exceeds 24 GB RAM budget!\\n\")\n",
    "\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: ../data/flannel_tokenizer_chars.json\n",
      "  ✓ Vocabulary: 10,000 tokens\n",
      "\n",
      "Loading corpus: ../data/flannel_model_corpus.txt\n",
      "  ✓ Tokens: 1,371,328\n",
      "\n",
      "Loading token masks: ../tensors/Flannel/live_dead_tokens.safetensors\n",
      "  ✓ Live: 6,301 | Dead: 3,699\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "print(f\"Loading tokenizer: {TOKENIZER_PATH}\")\n",
    "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "print(f\"  ✓ Vocabulary: {tokenizer.get_vocab_size():,} tokens\\n\")\n",
    "\n",
    "# Corpus\n",
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "encoding = tokenizer.encode(corpus_text)\n",
    "tokens = encoding.ids\n",
    "corpus_tensor = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "print(f\"  ✓ Tokens: {len(tokens):,}\\n\")\n",
    "\n",
    "# Token masks\n",
    "print(f\"Loading token masks: {TOKEN_MASK_PATH}\")\n",
    "mask_data = load_file(TOKEN_MASK_PATH)\n",
    "dead_indices = mask_data['dead_indices']\n",
    "n_dead = mask_data['dead_mask'].sum().item()\n",
    "n_live = mask_data['live_mask'].sum().item()\n",
    "print(f\"  ✓ Live: {n_live:,} | Dead: {n_dead:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset: 1,371,200 examples\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = TokenDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "print(f\"\\n✓ Dataset: {len(dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Initial Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating initial embedding matrix (seed=42)...\n",
      "\n",
      "  Shape: (10000, 64)\n",
      "  Dtype: torch.bfloat16\n",
      "  Mean:  -0.000041\n",
      "  Std:   0.020019\n",
      "\n",
      "✓ Initial W created\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCreating initial embedding matrix (seed={INIT_SEED})...\\n\")\n",
    "\n",
    "# Set seed for initialization\n",
    "torch.manual_seed(INIT_SEED)\n",
    "np.random.seed(INIT_SEED)\n",
    "\n",
    "# Create initial W in float32, then convert to bfloat16\n",
    "W_initial_f32 = torch.randn(VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32) * INIT_SCALE\n",
    "W_initial = W_initial_f32.to(torch.bfloat16)\n",
    "\n",
    "print(f\"  Shape: {tuple(W_initial.shape)}\")\n",
    "print(f\"  Dtype: {W_initial.dtype}\")\n",
    "print(f\"  Mean:  {W_initial.float().mean():.6f}\")\n",
    "print(f\"  Std:   {W_initial.float().std():.6f}\")\n",
    "print(f\"\\n✓ Initial W created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-allocate Recording Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-allocating recording tensors...\n",
      "\n",
      "  W:        (1, 501, 10000, 64)\n",
      "  grad_W:   (1, 501, 10000, 64)\n",
      "  losses:   (1, 501)\n",
      "\n",
      "✓ All tensors allocated on CPU\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPre-allocating recording tensors...\\n\")\n",
    "\n",
    "tensors = {}\n",
    "\n",
    "if RECORD_CONFIG['W']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE, HIDDEN_DIM)\n",
    "    tensors['W'] = torch.zeros(shape, dtype=torch.bfloat16)\n",
    "    print(f\"  W:        {shape}\")\n",
    "\n",
    "if RECORD_CONFIG['grads']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE, HIDDEN_DIM)\n",
    "    tensors['grad_W'] = torch.zeros(shape, dtype=torch.bfloat16)\n",
    "    print(f\"  grad_W:   {shape}\")\n",
    "\n",
    "if RECORD_CONFIG['losses']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1)\n",
    "    tensors['losses'] = torch.full(shape, float('nan'), dtype=torch.bfloat16)\n",
    "    print(f\"  losses:   {shape}\")\n",
    "\n",
    "print(f\"\\n✓ All tensors allocated on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Recorder class defined\n"
     ]
    }
   ],
   "source": [
    "class BatchRecorder:\n",
    "    \"\"\"Records data directly into pre-allocated tensors.\"\"\"\n",
    "    \n",
    "    def __init__(self, tensors, record_config, run_idx):\n",
    "        self.tensors = tensors\n",
    "        self.config = record_config\n",
    "        self.run_idx = run_idx\n",
    "        self.current_step = 0\n",
    "        self.recorded_initial = False\n",
    "        self.loss_value = None\n",
    "    \n",
    "    def record_initial_state(self, model, optimizer):\n",
    "        \"\"\"Record step 0.\"\"\"\n",
    "        if not self.recorded_initial:\n",
    "            t = 0\n",
    "            if self.config['W']:\n",
    "                self.tensors['W'][self.run_idx, t] = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "            if self.config['grads']:\n",
    "                # No gradient at t=0\n",
    "                self.tensors['grad_W'][self.run_idx, t] = 0\n",
    "            self.recorded_initial = True\n",
    "            self.current_step = 1\n",
    "            print(f\"    ✓ Recorded initial state (t=0)\")\n",
    "    \n",
    "    def record_before_step(self, model, loss, logits):\n",
    "        \"\"\"Capture data after backward, before optimizer step.\"\"\"\n",
    "        if self.config['losses']:\n",
    "            self.loss_value = loss.item()\n",
    "        \n",
    "        # Record gradient BEFORE optimizer step\n",
    "        if self.config['grads']:\n",
    "            t = self.current_step\n",
    "            if t <= self.tensors['grad_W'].shape[1] - 1:\n",
    "                if model.transformer.wte.weight.grad is not None:\n",
    "                    self.tensors['grad_W'][self.run_idx, t] = model.transformer.wte.weight.grad.clone().cpu().bfloat16()\n",
    "    \n",
    "    def record_after_step(self, model, optimizer):\n",
    "        \"\"\"Record data after optimizer step.\"\"\"\n",
    "        t = self.current_step\n",
    "        \n",
    "        if t > self.tensors['W'].shape[1] - 1 if 'W' in self.tensors else float('inf'):\n",
    "            return\n",
    "        \n",
    "        if self.config['W']:\n",
    "            self.tensors['W'][self.run_idx, t] = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "        \n",
    "        if self.config['losses'] and self.loss_value is not None:\n",
    "            self.tensors['losses'][self.run_idx, t] = self.loss_value\n",
    "            self.loss_value = None\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(f\"    Step {t}\")\n",
    "        \n",
    "        self.current_step += 1\n",
    "\n",
    "print(\"✓ Recorder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumented Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ InstrumentedTrainer defined\n"
     ]
    }
   ],
   "source": [
    "class InstrumentedTrainer(Trainer):\n",
    "    def __init__(self, recorder, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.recorder = recorder\n",
    "        self.last_logits = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        self.last_logits = outputs.logits\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        loss = super().training_step(model, inputs, num_items_in_batch)\n",
    "        self.recorder.record_before_step(model, loss, self.last_logits)\n",
    "        return loss\n",
    "\n",
    "    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time=None, **kwargs):\n",
    "        self.recorder.record_after_step(model, self.optimizer)\n",
    "        super()._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, **kwargs)\n",
    "\n",
    "print(\"✓ InstrumentedTrainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FLANNEL 7: CANONICAL TRAINING RUN\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Steps:               500\n",
      "  Initialization seed: 42\n",
      "  Training seed:       42\n",
      "  Recording:           W, grads, losses\n",
      "\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized (seed=42)\n",
      "  ✓ Training seed set to 42\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "{'train_runtime': 13.4044, 'train_samples_per_second': 1193.641, 'train_steps_per_second': 37.301, 'train_loss': 7.2028173828125, 'epoch': 0.011668611435239206}\n",
      "\n",
      "  ✓ Training complete (13.5s)\n",
      "\n",
      "================================================================================\n",
      "✓ Flannel 7 complete\n",
      "  Total time: 13.5s (0.2 minutes)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FLANNEL 7: CANONICAL TRAINING RUN\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Steps:               {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"  Initialization seed: {INIT_SEED}\")\n",
    "print(f\"  Training seed:       {BASE_TRAIN_SEED}\")\n",
    "print(f\"  Recording:           {', '.join([k for k, v in RECORD_CONFIG.items() if v])}\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "experiment_start = time.time()\n",
    "\n",
    "run_idx = 0\n",
    "train_seed = BASE_TRAIN_SEED\n",
    "\n",
    "# Set training seed\n",
    "torch.manual_seed(train_seed)\n",
    "np.random.seed(train_seed)\n",
    "\n",
    "# Create model\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config).to(torch.bfloat16).to(device)\n",
    "\n",
    "# Initialize with W_initial\n",
    "with torch.no_grad():\n",
    "    model.transformer.wte.weight[:] = W_initial.to(device)\n",
    "\n",
    "print(f\"  ✓ Model initialized (seed={INIT_SEED})\")\n",
    "print(f\"  ✓ Training seed set to {train_seed}\")\n",
    "\n",
    "# Create recorder\n",
    "recorder = BatchRecorder(tensors, RECORD_CONFIG, run_idx)\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=NUM_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    adam_beta1=ADAM_BETA1,\n",
    "    adam_beta2=ADAM_BETA2,\n",
    "    adam_epsilon=ADAM_EPSILON,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=1000,\n",
    "    save_steps=NUM_TRAIN_STEPS + 1,\n",
    "    save_total_limit=0,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    "    bf16=True,\n",
    "    seed=train_seed,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=True,\n",
    ")\n",
    "\n",
    "trainer = InstrumentedTrainer(\n",
    "    recorder=recorder,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Record initial state\n",
    "recorder.record_initial_state(model, trainer.optimizer)\n",
    "\n",
    "# Train\n",
    "print(f\"  Training...\")\n",
    "run_start = time.time()\n",
    "trainer.train()\n",
    "run_elapsed = time.time() - run_start\n",
    "\n",
    "print(f\"\\n  ✓ Training complete ({run_elapsed:.1f}s)\")\n",
    "\n",
    "experiment_elapsed = time.time() - experiment_start\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Flannel 7 complete\")\n",
    "print(f\"  Total time: {experiment_elapsed:.1f}s ({experiment_elapsed/60:.1f} minutes)\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data...\n",
      "\n",
      "  W            (1, 501, 10000, 64)           \n",
      "  grad_W       (1, 501, 10000, 64)           \n",
      "  losses       (1, 501)                      \n",
      "\n",
      "Saving to: ../tensors/Flannel/1.20h_flannel_7.safetensors\n",
      "\n",
      "✓ Saved successfully\n",
      "  File: 1.20h_flannel_7.safetensors\n",
      "  Size: 1283.8 MB (1.28 GB)\n",
      "  Save time: 0.3s\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving data...\\n\")\n",
    "\n",
    "# Build save dictionary\n",
    "save_dict = {\n",
    "    # Metadata\n",
    "    'n_runs': torch.tensor(NUM_RUNS, dtype=torch.long),\n",
    "    'init_seed': torch.tensor(INIT_SEED, dtype=torch.long),\n",
    "    'train_seed': torch.tensor(BASE_TRAIN_SEED, dtype=torch.long),\n",
    "    'n_steps': torch.tensor(NUM_TRAIN_STEPS, dtype=torch.long),\n",
    "    'n_live': torch.tensor(n_live, dtype=torch.long),\n",
    "    'n_dead': torch.tensor(n_dead, dtype=torch.long),\n",
    "    'vocab_size': torch.tensor(VOCAB_SIZE, dtype=torch.long),\n",
    "    'hidden_dim': torch.tensor(HIDDEN_DIM, dtype=torch.long),\n",
    "    'init_scale': torch.tensor(INIT_SCALE, dtype=torch.float32),\n",
    "    'learning_rate': torch.tensor(LEARNING_RATE, dtype=torch.float32),\n",
    "    'weight_decay': torch.tensor(WEIGHT_DECAY, dtype=torch.float32),\n",
    "    'adam_beta1': torch.tensor(ADAM_BETA1, dtype=torch.float32),\n",
    "    'adam_beta2': torch.tensor(ADAM_BETA2, dtype=torch.float32),\n",
    "    # Initial W\n",
    "    'W_initial': W_initial,\n",
    "    # Record config\n",
    "    'recorded_W': torch.tensor(RECORD_CONFIG['W'], dtype=torch.bool),\n",
    "    'recorded_grads': torch.tensor(RECORD_CONFIG['grads'], dtype=torch.bool),\n",
    "    'recorded_losses': torch.tensor(RECORD_CONFIG['losses'], dtype=torch.bool),\n",
    "}\n",
    "\n",
    "# Add recorded tensors\n",
    "for name, tensor in tensors.items():\n",
    "    save_dict[name] = tensor\n",
    "    print(f\"  {name:12} {str(tuple(tensor.shape)):30}\")\n",
    "\n",
    "# Save\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILE\n",
    "\n",
    "print(f\"\\nSaving to: {output_path}\\n\")\n",
    "\n",
    "save_start = time.time()\n",
    "save_file(save_dict, str(output_path))\n",
    "save_elapsed = time.time() - save_start\n",
    "\n",
    "file_size_mb = output_path.stat().st_size / 1e6\n",
    "file_size_gb = file_size_mb / 1000\n",
    "\n",
    "print(f\"✓ Saved successfully\")\n",
    "print(f\"  File: {output_path.name}\")\n",
    "print(f\"  Size: {file_size_mb:.1f} MB ({file_size_gb:.2f} GB)\")\n",
    "print(f\"  Save time: {save_elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FLANNEL 7 COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Single deterministic training run:\n",
      "  Seed:       42 (both init and training)\n",
      "  Steps:      500\n",
      "  Recorded:   W, grads, losses\n",
      "\n",
      "Data saved: ../tensors/Flannel/1.20h_flannel_7.safetensors\n",
      "  Size: 1.28 GB\n",
      "  Training time: 0.2 minutes\n",
      "\n",
      "Dead tokens: 3,699 / 10,000 (37.0%)\n",
      "\n",
      "This is now the canonical Flannel reference trajectory.\n",
      "Use this for all single-run analysis going forward.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FLANNEL 7 COMPLETE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Single deterministic training run:\")\n",
    "print(f\"  Seed:       {INIT_SEED} (both init and training)\")\n",
    "print(f\"  Steps:      {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"  Recorded:   {', '.join([k for k, v in RECORD_CONFIG.items() if v])}\")\n",
    "print()\n",
    "print(f\"Data saved: {output_path}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB\")\n",
    "print(f\"  Training time: {experiment_elapsed/60:.1f} minutes\")\n",
    "print()\n",
    "print(f\"Dead tokens: {n_dead:,} / {VOCAB_SIZE:,} ({100*n_dead/VOCAB_SIZE:.1f}%)\")\n",
    "print()\n",
    "print(f\"This is now the canonical Flannel reference trajectory.\")\n",
    "print(f\"Use this for all single-run analysis going forward.\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
