# 2025-11-21 07:23:48

Friday morning. Tentative plan for today: narrow our focus in on the bulk behavior of untrained tokens. Going to propose to Alpha that we rewrite and rerun experiment Flannel 7 to record more data: momentum and variance from Adam, mostly. I think we can fit it all in RAM for just 1,000 steps no problem, and it's better to have the data than to want it. Them. Inside you are two wolves. One of them thinks data is plural. The other thinks data is a mass moun. The wolves are mad at each other and always fight.

# 2025-11-21 13:18:46

The morning was eventful, but we ended up far from home and we need to find our way back this afternoon.

One of the things we need to be able to do is take the measured difference $\Delta W$ and decompose it into its component parts, starting with the raw grads (recorded during the Flannel 7 run like pretty much everything else).

There is a simple formula for calculating $\Delta W(t)$ from gradients and model training parameters. Let that formula be $F(t)$. We just need to test this:

$$F(t) = \Delta W(t)$$

for all time steps $t$.

Our first few attempts at this have been unsuccessful (1.24c6 and 1.24c7 in particular). I want to take a totally fresh approach to it this afternoon and see if we can take our knowns and compute our observables.

# 2025-11-21 14:06:06

Alpha recommends creating a new experiment, Thimble 1, in PyTorch. Operation Thimble is a go. The point of Operation Thimble is to have our own minimalistic training loops that tell us what we need to know, nothing fancy, nothign overengineered. Thimble 1's goal is to see if we can record data during the training run, then reconstruct the results of the training run from that data.

# 2025-11-21 15:16:00

Thimble 1 has been a failure. It's time to move on to Thimble 2, by first figuring out what the fuck went wrong with Thimble 1. Premise: Each training step, during back-propagation, a new W[t+1] is calculated based on the values of W[t] modified according to what AdamW does.

$$W_{t+1} = W_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

where $\hat{m}_t$ and $\hat{v}_t$ are both functions of $g_t = \frac{\partial L}{\partial W_i}$.

THAT IMPLIES that if we know the model's training parameters, we should be able to record the — wait. We could just record the logits. If we know W[t] and we know the logits from the end of the foward pass, shouldn't we be able to do everything manually given the model's training parameters?

# 2025-11-21 18:41:02

We have mastered the laws of physics. The experiment Thimble 3 and its analysis (1.26b, 1.26c) demonstrated that if we know W[t] and the optimizer states, we can compute W[t+1] to thin the limits of bfloat16 precision.

This means tomorrow we can start really studying the bulk dynamics of untrained tokens. If we decompose W[t+1] - W[t] into components that go like momentum[t] and variance[t], we should be able to see relationships — literally, in plots — between the various dynamical properties of the tokens and their derivatives. We can go all the way back down to grads[t] and see the relationship between the gradients and how tokens actually move in response. Is the correlation very close, or is it offset in time? Is there hysteresis? How do changes to grads[t] propagate through [t+1] and beyond?

These are all things to talk about tomorrow.