# 2025-11-19 06:48:01

Questions that need answering:

- The spongecrystal: what is it? where did it come from? what initial conditions + training dynamics could have created it?

We know a lot about the spongecrystal now: its size, shape, crystal structure, composition. But we don't know a lot yet about training dynamics.

So I think we should try to do basic science on training dynamics. Like what?

- What's a control experiment in this case? Training with no correct answers? Wrong answers only? Does that even make sense?

- Or normal training: More Wordybird experiments with no test in mind, just collect data about training dynamics under normal conditions?

We need to talk about the tokenizer though. See yesterday's log. There must be a middle ground between 128 ASCII tokens we can train 100,000 steps in 20 minutes, and GPT-2 1000 training steps fills my hard drive. Is there something in the 1,000-token range? Or could we train a 1,000-token tokenizer? Maybe we could train it in such a way that we get tokens that never get ouput from our FineWeb corpus?

- What can we learn? Well first, can we perfect our tooling? We have
    - 1.13a search for lattice-scale structures
    - 1.13b measure lattice-scale structures
    - 1.13c basic token-cloud geometry (may be out of date, haven't used in a day or two)
    - 1.16d basic displacement in a comoving reference frame (needs updating if we switch tokenizers)
    - 1.17b an interesting one that looks for lattice-scale movements and factors them out separately from larger movements to see when the cloud transitions from gas to frozen solid

Will leave here for now. Time to make coffee for Kylee.

# 2025-11-19 08:41:49

Alph and I have agreed to train a tokenizer in the laziest possible way:

  from datasets import load_dataset

  # English
  english = load_dataset("HuggingFaceFW/fineweb-2", "eng_Latn", split="train", streaming=True)

  # Thai
  thai = load_dataset("HuggingFaceFW/fineweb-2", "tha_Thai", split="train", streaming=True)

  The Complete Lazy Plan

  1. Sample text for tokenizer training:
    - Stream 80MB English (eng_Latn)
    - Stream 20MB Thai (tha_Thai)
    - Dump to a single text file
  2. Train 1000-token BPE tokenizer on that mixed corpus
  (~2 minutes)
  3. Create model training corpus:
    - Sample 2-5 MB of pure English from FineWeb-2
    - All those Thai tokens will be dead by construction
  4. Train your tiny model (same hyperparameters as Lil
  Gatsby or Wordybird, just with new tokenizer)
  5. Watch what happens to the dead Thai tokens during
  training

(Indented text is Alpha's.)

Notebook plan for the next chunk of work:

- 1.18ùë• = prep work, corpuses, data cleaning, etc.
- 1.19ùë• = tokenizers, maybe model definitions? (you can force a notebook to run another notebook first, right?)
- 1.20ùë• = training experiments

Two-dimensional filing system.

# 2025-11-19 09:33:07

Slight change of plans. Byte-level tokenization is hard to interpret because the tokens don't decode as neat unicode strings. So we're pivoting to using character-level encoding instead. Alph says there are distinct cons to doing it that way, but it's a trade-off we're willing to make for better interpretability down the line.

# 2025-11-19 09:49:49

Have settled on vocabulary size of 10,000 tokens. There are ~2,800 distinct characters in our tokenizer training corpus, so that gives us plenty of merges to learn. This might be a decent model for the demographics of a real token cloud.

# 2025-11-19 10:37:51

Tokenizer looks good. Ran it on our Fineweb training corpus:

  Live tokens: 6,301 (63.01%)
  Dead tokens: 3,699 (36.99%)

So it's roughly 65/35 between live tokens and dead tokens. I'd rather it were closer to 10% to better match Qwen 3 4B demographics, but with this few tokens a more even split is probably better.

# 2025-11-19 10:58:28

Have decided to go ahead with Flannel 1. Flannel tokenizer, 5 MB English corpus, 1,000 steps.