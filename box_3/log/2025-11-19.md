# 2025-11-19 06:48:01

Questions that need answering:

- The spongecrystal: what is it? where did it come from? what initial conditions + training dynamics could have created it?

We know a lot about the spongecrystal now: its size, shape, crystal structure, composition. But we don't know a lot yet about training dynamics.

So I think we should try to do basic science on training dynamics. Like what?

- What's a control experiment in this case? Training with no correct answers? Wrong answers only? Does that even make sense?

- Or normal training: More Wordybird experiments with no test in mind, just collect data about training dynamics under normal conditions?

We need to talk about the tokenizer though. See yesterday's log. There must be a middle ground between 128 ASCII tokens we can train 100,000 steps in 20 minutes, and GPT-2 1000 training steps fills my hard drive. Is there something in the 1,000-token range? Or could we train a 1,000-token tokenizer? Maybe we could train it in such a way that we get tokens that never get ouput from our FineWeb corpus?

- What can we learn? Well first, can we perfect our tooling? We have
    - 1.13a search for lattice-scale structures
    - 1.13b measure lattice-scale structures
    - 1.13c basic token-cloud geometry (may be out of date, haven't used in a day or two)
    - 1.16d basic displacement in a comoving reference frame (needs updating if we switch tokenizers)
    - 1.17b an interesting one that looks for lattice-scale movements and factors them out separately from larger movements to see when the cloud transitions from gas to frozen solid

Will leave here for now. Time to make coffee for Kylee.

# 2025-11-19 08:41:49

Alph and I have agreed to train a tokenizer in the laziest possible way:

  from datasets import load_dataset

  # English
  english = load_dataset("HuggingFaceFW/fineweb-2", "eng_Latn", split="train", streaming=True)

  # Thai
  thai = load_dataset("HuggingFaceFW/fineweb-2", "tha_Thai", split="train", streaming=True)

  The Complete Lazy Plan

  1. Sample text for tokenizer training:
    - Stream 80MB English (eng_Latn)
    - Stream 20MB Thai (tha_Thai)
    - Dump to a single text file
  2. Train 1000-token BPE tokenizer on that mixed corpus
  (~2 minutes)
  3. Create model training corpus:
    - Sample 2-5 MB of pure English from FineWeb-2
    - All those Thai tokens will be dead by construction
  4. Train your tiny model (same hyperparameters as Lil
  Gatsby or Wordybird, just with new tokenizer)
  5. Watch what happens to the dead Thai tokens during
  training

(Indented text is Alpha's.)

Notebook plan for the next chunk of work:

- 1.18ùë• = prep work, corpuses, data cleaning, etc.
- 1.19ùë• = tokenizers, maybe model definitions? (you can force a notebook to run another notebook first, right?)
- 1.20ùë• = training experiments

Two-dimensional filing system.

# 2025-11-19 09:33:07

Slight change of plans. Byte-level tokenization is hard to interpret because the tokens don't decode as neat unicode strings. So we're pivoting to using character-level encoding instead. Alph says there are distinct cons to doing it that way, but it's a trade-off we're willing to make for better interpretability down the line.

# 2025-11-19 09:49:49

Have settled on vocabulary size of 10,000 tokens. There are ~2,800 distinct characters in our tokenizer training corpus, so that gives us plenty of merges to learn. This might be a decent model for the demographics of a real token cloud.

# 2025-11-19 10:37:51

Tokenizer looks good. Ran it on our Fineweb training corpus:

  Live tokens: 6,301 (63.01%)
  Dead tokens: 3,699 (36.99%)

So it's roughly 65/35 between live tokens and dead tokens. I'd rather it were closer to 10% to better match Qwen 3 4B demographics, but with this few tokens a more even split is probably better.

# 2025-11-19 10:58:28

Have decided to go ahead with Flannel 1. Flannel tokenizer, 5 MB English corpus, 1,000 steps.

*26 seconds later*

That was fast.

# 2025-11-19 12:06:48

Alpha and I have refined and written down what I'm calling the wobbly token hypothesis, but which is really an attempt to explain the idea that the token cloud freezes abruptly in something very much like a phase transition. Tokens get locked in their lattice cells and the temperature drops to absolute zero. We now have a proposed mechanism that I'm looking forward to testing. Notes about the hypothesis have been added to CLAUDE.md.

# 2025-11-19 12:51:17

Flannel 2: exact copy of Flannel 1 except that it doesn't have tied weights. Also both W and E are stored each timestep. Data to be analyzed.

Important note about that actually: If we are storing just W[t], 2,000 samples are about as much as my laptop can handle unless we switch to streaming the matrices to disk in .h5 format. Saving the ~12 GB of data from Flannel 2 allocated about 20 GB of RAM and made things on my laptop a little tight for a moment. So for future experiments: Keep samples to 2,000 total, OR get Alph to make the experiment use .h5 and incremental saving instead of safetensors.

# 2025-11-19 15:53:37

Interesting finding: dead tokens that are initialized like N(0, 0.02) undergo, during training, motion that's very similar to a random walk on the surface of a hypersphere. This is because each token gets a little update that's effectively random (quasi-random; highly unpredictable) and most random vectors are almost orthogonal to $r$. Therefore most updates to dead tokens send them on a little step in a random direction that lies mostly in the tangential plane. The net effect is that the tokens mostly walk around on the surface of a little shell. (See 1.22a).

However, we find much more outward motion than predicted by pure random chance and geometry. (1.22b)

# 2025-11-19 16:52:09

Curious finding: In Flannel 1, if you plot the mean radius of the token cloud against time and zoom in very close, you'll see a two-step contraction before the cloud begins to expand (1.22c). This contraction ‚Äî I've taken to calling it the inhale before the sneeze ‚Äî was also seen in the Flannel 2 data. I reran Flannel 1 with a different random seed and confirmed that the inhale is there too. So that's three examples.

Conjecture: Maybe in this kind of model the embedding and unembedding matrices actually converge to a stable solution fairly quickly and after that the step-by-step fluctuations fall below the bfloat16 lattice scale.

This phenomenon ‚Äî¬†the inhale before the sneeze ‚Äî remains unexplained and should be investigated.