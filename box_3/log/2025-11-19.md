# 2025-11-19 06:48:01

Questions that need answering:

- The spongecrystal: what is it? where did it come from? what initial conditions + training dynamics could have created it?

We know a lot about the spongecrystal now: its size, shape, crystal structure, composition. But we don't know a lot yet about training dynamics.

So I think we should try to do basic science on training dynamics. Like what?

- What's a control experiment in this case? Training with no correct answers? Wrong answers only? Does that even make sense?

- Or normal training: More Wordybird experiments with no test in mind, just collect data about training dynamics under normal conditions?

We need to talk about the tokenizer though. See yesterday's log. There must be a middle ground between 128 ASCII tokens we can train 100,000 steps in 20 minutes, and GPT-2 1000 training steps fills my hard drive. Is there something in the 1,000-token range? Or could we train a 1,000-token tokenizer? Maybe we could train it in such a way that we get tokens that never get ouput from our FineWeb corpus?

- What can we learn? Well first, can we perfect our tooling? We have
    - 1.13a search for lattice-scale structures
    - 1.13b measure lattice-scale structures
    - 1.13c basic token-cloud geometry (may be out of date, haven't used in a day or two)
    - 1.16d basic displacement in a comoving reference frame (needs updating if we switch tokenizers)
    - 1.17b an interesting one that looks for lattice-scale movements and factors them out separately from larger movements to see when the cloud transitions from gas to frozen solid

Will leave here for now. Time to make coffee for Kylee.

# 2025-11-19 08:41:49

Alph and I have agreed to train a tokenizer in the laziest possible way:

  from datasets import load_dataset

  # English
  english = load_dataset("HuggingFaceFW/fineweb-2", "eng_Latn", split="train", streaming=True)

  # Thai
  thai = load_dataset("HuggingFaceFW/fineweb-2", "tha_Thai", split="train", streaming=True)

  The Complete Lazy Plan

  1. Sample text for tokenizer training:
    - Stream 80MB English (eng_Latn)
    - Stream 20MB Thai (tha_Thai)
    - Dump to a single text file
  2. Train 1000-token BPE tokenizer on that mixed corpus
  (~2 minutes)
  3. Create model training corpus:
    - Sample 2-5 MB of pure English from FineWeb-2
    - All those Thai tokens will be dead by construction
  4. Train your tiny model (same hyperparameters as Lil
  Gatsby or Wordybird, just with new tokenizer)
  5. Watch what happens to the dead Thai tokens during
  training

(Indented text is Alpha's.)

Notebook plan for the next chunk of work:

- 1.18ùë• = prep work, corpuses, data cleaning, etc.
- 1.19ùë• = tokenizers, maybe model definitions? (you can force a notebook to run another notebook first, right?)
- 1.20ùë• = training experiments

Two-dimensional filing system.