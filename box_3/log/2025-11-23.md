# 2025-11-23 09:48:36

Major methodological (dare I use the word) breakthrough this morning. When we're doing our experiments in which we really usually want to load whole tensors at a time (W[6001, 10000, 64] e.g.) it's better to write to HDF5 files in ~2GB chunks. Alph settled on [1500, 10000, 64] as the chunk shape for the four important tensors. This lets an analysis notebook load the whole tensor in four big bites. Or I guess five. Maybe we should revisit our chunk shapes if we're going to have 6,001 layers in the tensor.

OH. Why not [6001, 10000, 64]? Because chunks have to be smaller than 4 GB.

# 2025-11-23 10:09:15

Alph has converted the Thimble 6 HDF5 file to a new `thimble_6_chunky.h5` that's consistent with what I wrote above. Performs very well. We'll use this from now on.

# 2025-11-23 10:25:29

Have discovered that bfloat16 data in Thimble 6 was stored as float16. Odds of this being a problem are very low, but have decided to proceed with Thimble 7 anyway. Nice clean data saved efficiently and faithfully.

# 2025-11-23 10:49:57

Thimble 7 bombed out during training in a way that irritates me. Alph had inserted a print statement — no reason, just for show — that tried to print the value of a recorded parameter that had already been deallocated when the data file was closed, so the notebook bombed. However, the data file had already been closed, so we got the data. It's just the experiment technically crashed at the end. Must make sure from now on Alph does not put anything decorative in long-running compute cells. Those cells need to stand alone.