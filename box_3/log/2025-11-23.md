# 2025-11-23 09:48:36

Major methodological (dare I use the word) breakthrough this morning. When we're doing our experiments in which we really usually want to load whole tensors at a time (W[6001, 10000, 64] e.g.) it's better to write to HDF5 files in ~2GB chunks. Alph settled on [1500, 10000, 64] as the chunk shape for the four important tensors. This lets an analysis notebook load the whole tensor in four big bites. Or I guess five. Maybe we should revisit our chunk shapes if we're going to have 6,001 layers in the tensor.

OH. Why not [6001, 10000, 64]? Because chunks have to be smaller than 4 GB.

# 2025-11-23 10:09:15

Alph has converted the Thimble 6 HDF5 file to a new `thimble_6_chunky.h5` that's consistent with what I wrote above. Performs very well. We'll use this from now on.

