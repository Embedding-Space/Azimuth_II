{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce72d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01txlbb81jj5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target storage: 10 GB = 10,737,418,240 bytes\n",
      "Steps to save: 1,001\n",
      "Hidden dim: 64\n",
      "Precision: bfloat16 (2 bytes)\n",
      "\n",
      "Maximum vocab size: 83,802 tokens\n",
      "\n",
      "Storage per timestep: 10.23 MB\n",
      "\n",
      "Common tokenizer sizes for reference:\n",
      "  GPT-2: 50,257 tokens\n",
      "  BERT: 30,522 tokens\n",
      "  GPT-2 (our max): 83,802 tokens\n",
      "\n",
      "If we used GPT-2 (50,257 tokens):\n",
      "  Storage needed: 6.00 GB\n",
      "\n",
      "  Every   1 step(s):  1,001 samples =   6.00 GB\n",
      "  Every  10 step(s):    101 samples =   0.61 GB\n",
      "  Every 100 step(s):     11 samples =   0.07 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reverse engineering: What vocab size fits in 10 GB for 100k steps?\n",
    "\n",
    "max_storage_gb = 10\n",
    "steps = 1_000 + 1  # Include step 0\n",
    "hidden_dim = 64\n",
    "bytes_per_value = 2  # bfloat16\n",
    "\n",
    "# Convert GB to bytes\n",
    "max_storage_bytes = max_storage_gb * (1024**3)\n",
    "\n",
    "print(f\"Target storage: {max_storage_gb} GB = {max_storage_bytes:,} bytes\")\n",
    "print(f\"Steps to save: {steps:,}\")\n",
    "print(f\"Hidden dim: {hidden_dim}\")\n",
    "print(f\"Precision: bfloat16 ({bytes_per_value} bytes)\")\n",
    "print()\n",
    "\n",
    "# Calculate max vocab size\n",
    "# Total bytes = steps * vocab_size * hidden_dim * bytes_per_value\n",
    "# Solving for vocab_size:\n",
    "# vocab_size = total_bytes / (steps * hidden_dim * bytes_per_value)\n",
    "\n",
    "max_vocab_size = max_storage_bytes / (steps * hidden_dim * bytes_per_value)\n",
    "\n",
    "print(f\"Maximum vocab size: {max_vocab_size:,.0f} tokens\")\n",
    "print()\n",
    "\n",
    "# What does this look like per step?\n",
    "bytes_per_step = max_vocab_size * hidden_dim * bytes_per_value\n",
    "mb_per_step = bytes_per_step / (1024**2)\n",
    "\n",
    "print(f\"Storage per timestep: {mb_per_step:.2f} MB\")\n",
    "print()\n",
    "\n",
    "# How does this compare to common tokenizers?\n",
    "print(\"Common tokenizer sizes for reference:\")\n",
    "print(f\"  GPT-2: 50,257 tokens\")\n",
    "print(f\"  BERT: 30,522 tokens\") \n",
    "print(f\"  GPT-2 (our max): {int(max_vocab_size):,} tokens\")\n",
    "print()\n",
    "\n",
    "# If we used GPT-2, how much would we actually need?\n",
    "gpt2_vocab = 50_257\n",
    "gpt2_storage_gb = (gpt2_vocab * hidden_dim * bytes_per_value * steps) / (1024**3)\n",
    "\n",
    "print(f\"If we used GPT-2 (50,257 tokens):\")\n",
    "print(f\"  Storage needed: {gpt2_storage_gb:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# What if we sample every Nth step instead?\n",
    "for sample_rate in [1, 10, 100]:\n",
    "    sampled_steps = (steps - 1) // sample_rate + 1\n",
    "    sampled_gb = (gpt2_vocab * hidden_dim * bytes_per_value * sampled_steps) / (1024**3)\n",
    "    print(f\"  Every {sample_rate:3d} step(s): {sampled_steps:6,} samples = {sampled_gb:6.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "l3a9lo1g9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WORDYBIRD MEMORY REQUIREMENTS\n",
      "======================================================================\n",
      "\n",
      "Embedding matrices (E + W):\n",
      "  Parameters: 6,432,896\n",
      "  FP32: 24.5 MB\n",
      "  BF16: 12.3 MB\n",
      "\n",
      "Transformer layers (4 layers):\n",
      "  Parameters: 196,608\n",
      "  FP32: 0.8 MB\n",
      "\n",
      "Total model:\n",
      "  Parameters: 6,629,504\n",
      "  FP32: 25.3 MB\n",
      "\n",
      "======================================================================\n",
      "TRAINING MEMORY (per batch)\n",
      "======================================================================\n",
      "\n",
      "Batch size  1:\n",
      "  Model:          25.3 MB\n",
      "  Activations:     0.4 MB\n",
      "  Gradients:      25.3 MB\n",
      "  Optimizer:      50.6 MB\n",
      "  Total:         101.5 MB (0.10 GB)\n",
      "\n",
      "Batch size  8:\n",
      "  Model:          25.3 MB\n",
      "  Activations:     3.0 MB\n",
      "  Gradients:      25.3 MB\n",
      "  Optimizer:      50.6 MB\n",
      "  Total:         104.2 MB (0.10 GB)\n",
      "\n",
      "Batch size 16:\n",
      "  Model:          25.3 MB\n",
      "  Activations:     6.0 MB\n",
      "  Gradients:      25.3 MB\n",
      "  Optimizer:      50.6 MB\n",
      "  Total:         107.2 MB (0.10 GB)\n",
      "\n",
      "Batch size 32:\n",
      "  Model:          25.3 MB\n",
      "  Activations:    12.0 MB\n",
      "  Gradients:      25.3 MB\n",
      "  Optimizer:      50.6 MB\n",
      "  Total:         113.2 MB (0.11 GB)\n",
      "\n",
      "Batch size 64:\n",
      "  Model:          25.3 MB\n",
      "  Activations:    24.0 MB\n",
      "  Gradients:      25.3 MB\n",
      "  Optimizer:      50.6 MB\n",
      "  Total:         125.2 MB (0.12 GB)\n",
      "\n",
      "======================================================================\n",
      "Your M4 Pro has 48 GB RAM\n",
      "Safe budget: ~24 GB for training (leaving room for OS/other processes)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Wordybird memory requirements\n",
    "\n",
    "import torch\n",
    "\n",
    "# Model architecture (same as Lil Gatsby except vocab)\n",
    "vocab_size = 50_257  # GPT-2\n",
    "hidden_dim = 64\n",
    "n_layers = 4\n",
    "n_heads = 4\n",
    "seq_length = 128\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"WORDYBIRD MEMORY REQUIREMENTS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# E and W matrices (the big ones)\n",
    "e_w_params = 2 * vocab_size * hidden_dim  # E and W\n",
    "e_w_bytes_fp32 = e_w_params * 4  # float32\n",
    "e_w_bytes_bf16 = e_w_params * 2  # bfloat16\n",
    "e_w_mb_fp32 = e_w_bytes_fp32 / (1024**2)\n",
    "e_w_mb_bf16 = e_w_bytes_bf16 / (1024**2)\n",
    "\n",
    "print(f\"Embedding matrices (E + W):\")\n",
    "print(f\"  Parameters: {e_w_params:,}\")\n",
    "print(f\"  FP32: {e_w_mb_fp32:.1f} MB\")\n",
    "print(f\"  BF16: {e_w_mb_bf16:.1f} MB\")\n",
    "print()\n",
    "\n",
    "# Rest of the model (transformer layers, etc.)\n",
    "# Rough estimate: attention (QKV + output) + FFN\n",
    "params_per_layer = (\n",
    "    4 * hidden_dim * hidden_dim +  # QKV + output projection\n",
    "    2 * hidden_dim * (4 * hidden_dim)  # FFN (typically 4x expansion)\n",
    ")\n",
    "other_params = n_layers * params_per_layer\n",
    "other_mb_fp32 = (other_params * 4) / (1024**2)\n",
    "\n",
    "print(f\"Transformer layers ({n_layers} layers):\")\n",
    "print(f\"  Parameters: {other_params:,}\")\n",
    "print(f\"  FP32: {other_mb_fp32:.1f} MB\")\n",
    "print()\n",
    "\n",
    "total_params = e_w_params + other_params\n",
    "total_mb_fp32 = e_w_mb_fp32 + other_mb_fp32\n",
    "\n",
    "print(f\"Total model:\")\n",
    "print(f\"  Parameters: {total_params:,}\")\n",
    "print(f\"  FP32: {total_mb_fp32:.1f} MB\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING MEMORY (per batch)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Activations scale with batch_size\n",
    "def estimate_memory(batch_size):\n",
    "    # Model parameters (fixed)\n",
    "    model_mb = e_w_mb_fp32 + other_mb_fp32\n",
    "    \n",
    "    # Activations (rough estimate)\n",
    "    # Each layer stores: input, attention weights, FFN intermediate\n",
    "    activation_elements = batch_size * seq_length * hidden_dim * n_layers * 3\n",
    "    activation_mb = (activation_elements * 4) / (1024**2)  # fp32\n",
    "    \n",
    "    # Gradients (same size as parameters during training)\n",
    "    gradient_mb = model_mb\n",
    "    \n",
    "    # Optimizer state (AdamW stores 2 moments per parameter)\n",
    "    optimizer_mb = model_mb * 2\n",
    "    \n",
    "    # Total\n",
    "    total_mb = model_mb + activation_mb + gradient_mb + optimizer_mb\n",
    "    \n",
    "    return {\n",
    "        'model': model_mb,\n",
    "        'activations': activation_mb,\n",
    "        'gradients': gradient_mb,\n",
    "        'optimizer': optimizer_mb,\n",
    "        'total': total_mb\n",
    "    }\n",
    "\n",
    "for batch_size in [1, 8, 16, 32, 64]:\n",
    "    mem = estimate_memory(batch_size)\n",
    "    print(f\"Batch size {batch_size:2d}:\")\n",
    "    print(f\"  Model:       {mem['model']:7.1f} MB\")\n",
    "    print(f\"  Activations: {mem['activations']:7.1f} MB\")\n",
    "    print(f\"  Gradients:   {mem['gradients']:7.1f} MB\")\n",
    "    print(f\"  Optimizer:   {mem['optimizer']:7.1f} MB\")\n",
    "    print(f\"  Total:       {mem['total']:7.1f} MB ({mem['total']/1024:.2f} GB)\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Your M4 Pro has 48 GB RAM\")\n",
    "print(f\"Safe budget: ~24 GB for training (leaving room for OS/other processes)\")\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
