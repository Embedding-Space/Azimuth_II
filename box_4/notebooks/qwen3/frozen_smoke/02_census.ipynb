{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Frozen Smoke Census\n",
    "\n",
    "**Purpose:** Complete population counts and classification of all vectors in the frozen smoke.\n",
    "\n",
    "---\n",
    "\n",
    "## Classification Scheme\n",
    "\n",
    "| Class | Definition | Character |\n",
    "|-------|------------|----------|\n",
    "| **Black Hole** | Vector with count > 1 | Multiple tokens collapsed to same point |\n",
    "| **Singleton** | Vector with count = 1 | One token, unique position |\n",
    "\n",
    "We further subdivide by spatial location (core vs Oort Cloud) and connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T21:05:26.894170Z",
     "iopub.status.busy": "2025-11-27T21:05:26.893944Z",
     "iopub.status.idle": "2025-11-27T21:05:30.086306Z",
     "shell.execute_reply": "2025-11-27T21:05:30.085844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 151,936\n",
      "Neighborhood tokens: 2,212\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../../../tensors/Qwen3-4B-Instruct-2507\")\n",
    "\n",
    "# Load data\n",
    "W = load_file(DATA_DIR / \"W_unembed.safetensors\")['W'].view(torch.bfloat16)\n",
    "masks = load_file(DATA_DIR / \"masks.safetensors\")\n",
    "neighborhood_mask = masks['neighborhood_mask']\n",
    "\n",
    "# Tokenizer for decoding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\")\n",
    "\n",
    "print(f\"Total vocabulary: {len(W):,}\")\n",
    "print(f\"Neighborhood tokens: {neighborhood_mask.sum().item():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T21:05:30.087657Z",
     "iopub.status.busy": "2025-11-27T21:05:30.087523Z",
     "iopub.status.idle": "2025-11-27T21:05:30.105304Z",
     "shell.execute_reply": "2025-11-27T21:05:30.104939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique vectors: 125\n",
      "Tokens: 2,212\n",
      "Compression ratio: 17.7x\n"
     ]
    }
   ],
   "source": [
    "# Extract neighborhood\n",
    "neighborhood_indices = torch.where(neighborhood_mask)[0]\n",
    "W_neighborhood = W[neighborhood_indices]\n",
    "\n",
    "# Find unique vectors\n",
    "unique_bits, inverse_indices, counts = torch.unique(\n",
    "    W_neighborhood.view(torch.uint16).to(torch.int32), dim=0,\n",
    "    return_inverse=True, return_counts=True\n",
    ")\n",
    "\n",
    "n_unique = len(unique_bits)\n",
    "n_tokens = neighborhood_mask.sum().item()\n",
    "\n",
    "print(f\"Unique vectors: {n_unique}\")\n",
    "print(f\"Tokens: {n_tokens:,}\")\n",
    "print(f\"Compression ratio: {n_tokens / n_unique:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Black Hole Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T21:05:30.106546Z",
     "iopub.status.busy": "2025-11-27T21:05:30.106471Z",
     "iopub.status.idle": "2025-11-27T21:05:30.150813Z",
     "shell.execute_reply": "2025-11-27T21:05:30.150371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black holes: 13\n",
      "Total tokens in black holes: 2,100\n",
      "\n",
      "  BH  Vector Idx  Tokens         Sample Token IDs               Sample Decoded\n",
      " BH4          40     814  [80091, 119346, 119348]             '２０', '珊�', '珊�'\n",
      " BH6          61     704          [125, 177, 178]                '�', '�', '�'\n",
      "BH10          75     306    [124, 123876, 123948]               '�', '�', 'ติ'\n",
      " BH3          39     228 [124350, 124658, 125147]          'เรีย', 'ย์', 'ติด'\n",
      " BH0          25      11 [123939, 131955, 131957]          'ร์', 'ฟุต', 'ฝ่าย'\n",
      " BH9          71      10 [119349, 125087, 126630]          '珊󠄁', 'ช่วย', 'จัง'\n",
      " BH1          26       6 [126268, 132713, 138041]        'สี', 'ประกัน', 'ซี่'\n",
      "BH11          79       5 [132383, 132398, 139050]        'เย็น', 'นี', 'เพิ่ง'\n",
      " BH5          43       4 [135619, 138490, 140815] 'เปอร์', 'ร่วมกัน', 'ที่น่า'\n",
      "BH12          80       4 [136831, 138068, 138072]      'สติ', 'ể', ' จากนั้น'\n",
      " BH2          34       3 [126775, 140303, 147056]            'รัฐ', 'ซ้ำ', '切'\n",
      " BH8          63       3    [180, 138979, 141503]   '�', 'รับรอง', 'การบริหาร'\n",
      " BH7          62       2         [126816, 147836]                   'สมั', 'אַ'\n"
     ]
    }
   ],
   "source": [
    "# Identify black holes\n",
    "black_hole_mask = counts > 1\n",
    "black_hole_indices = torch.where(black_hole_mask)[0]\n",
    "n_black_holes = len(black_hole_indices)\n",
    "\n",
    "print(f\"Black holes: {n_black_holes}\")\n",
    "print(f\"Total tokens in black holes: {counts[black_hole_mask].sum().item():,}\")\n",
    "print()\n",
    "\n",
    "# Create black hole table\n",
    "bh_data = []\n",
    "for i, bh_idx in enumerate(black_hole_indices):\n",
    "    bh_count = counts[bh_idx].item()\n",
    "    \n",
    "    # Find tokens in this black hole\n",
    "    token_indices = [neighborhood_indices[j].item() for j in range(len(inverse_indices)) \n",
    "                     if inverse_indices[j] == bh_idx]\n",
    "    \n",
    "    # Sample tokens for display\n",
    "    sample_tokens = token_indices[:3]\n",
    "    sample_decoded = [repr(tokenizer.decode([t])) for t in sample_tokens]\n",
    "    \n",
    "    bh_data.append({\n",
    "        'BH': f'BH{i}',\n",
    "        'Vector Idx': bh_idx.item(),\n",
    "        'Tokens': bh_count,\n",
    "        'Sample Token IDs': str(sample_tokens[:3]),\n",
    "        'Sample Decoded': ', '.join(sample_decoded)\n",
    "    })\n",
    "\n",
    "bh_df = pd.DataFrame(bh_data)\n",
    "bh_df = bh_df.sort_values('Tokens', ascending=False).reset_index(drop=True)\n",
    "print(bh_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Singleton Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T21:05:30.151886Z",
     "iopub.status.busy": "2025-11-27T21:05:30.151822Z",
     "iopub.status.idle": "2025-11-27T21:05:30.155907Z",
     "shell.execute_reply": "2025-11-27T21:05:30.155515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singletons: 112\n",
      "\n",
      "Singleton demographics:\n",
      "  Thai: 85 (75.9%)\n",
      "  CJK: 1 (0.9%)\n",
      "  Special: 2 (1.8%)\n",
      "  Other: 24 (21.4%)\n"
     ]
    }
   ],
   "source": [
    "# Identify singletons\n",
    "singleton_mask = counts == 1\n",
    "singleton_indices = torch.where(singleton_mask)[0]\n",
    "n_singletons = len(singleton_indices)\n",
    "\n",
    "print(f\"Singletons: {n_singletons}\")\n",
    "print()\n",
    "\n",
    "# Check for Thai, CJK, special characters\n",
    "thai_count = 0\n",
    "cjk_count = 0\n",
    "special_count = 0\n",
    "other_count = 0\n",
    "\n",
    "for s_idx in singleton_indices:\n",
    "    # Find the token\n",
    "    token_idx = neighborhood_indices[(inverse_indices == s_idx).nonzero()[0]].item()\n",
    "    decoded = tokenizer.decode([token_idx])\n",
    "    \n",
    "    is_thai = any('\\u0e00' <= c <= '\\u0e7f' for c in decoded)\n",
    "    is_cjk = any('\\u4e00' <= c <= '\\u9fff' or '\\u3400' <= c <= '\\u4dbf' for c in decoded)\n",
    "    is_special = decoded.startswith('<|') and decoded.endswith('|>')\n",
    "    \n",
    "    if is_thai:\n",
    "        thai_count += 1\n",
    "    elif is_cjk:\n",
    "        cjk_count += 1\n",
    "    elif is_special:\n",
    "        special_count += 1\n",
    "    else:\n",
    "        other_count += 1\n",
    "\n",
    "print(f\"Singleton demographics:\")\n",
    "print(f\"  Thai: {thai_count} ({thai_count/n_singletons*100:.1f}%)\")\n",
    "print(f\"  CJK: {cjk_count} ({cjk_count/n_singletons*100:.1f}%)\")\n",
    "print(f\"  Special: {special_count} ({special_count/n_singletons*100:.1f}%)\")\n",
    "print(f\"  Other: {other_count} ({other_count/n_singletons*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Spatial Distribution: Core vs Oort Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T21:05:30.156893Z",
     "iopub.status.busy": "2025-11-27T21:05:30.156817Z",
     "iopub.status.idle": "2025-11-27T21:05:30.159640Z",
     "shell.execute_reply": "2025-11-27T21:05:30.159268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORE (L2 < 5e-05):\n",
      "  Vectors: 75\n",
      "  Tokens: 2,159\n",
      "  Density: 28.8\n",
      "\n",
      "OORT CLOUD (L2 >= 5e-05):\n",
      "  Vectors: 50\n",
      "  Tokens: 53\n",
      "  Density: 1.1\n"
     ]
    }
   ],
   "source": [
    "# Compute L2 distances from center (biggest black hole)\n",
    "unique_bf16 = unique_bits.to(torch.uint16).view(torch.bfloat16)\n",
    "W_unique = unique_bf16.float()\n",
    "\n",
    "center_idx = counts.argmax().item()\n",
    "r = torch.norm(W_unique - W_unique[center_idx], dim=1).numpy()\n",
    "\n",
    "# Core boundary at r = 0.00005 (where density drops to 1)\n",
    "CORE_BOUNDARY = 0.00005\n",
    "\n",
    "core_mask = r < CORE_BOUNDARY\n",
    "oort_mask = r >= CORE_BOUNDARY\n",
    "\n",
    "core_vectors = core_mask.sum()\n",
    "core_tokens = counts.numpy()[core_mask].sum()\n",
    "oort_vectors = oort_mask.sum()\n",
    "oort_tokens = counts.numpy()[oort_mask].sum()\n",
    "\n",
    "print(f\"CORE (L2 < {CORE_BOUNDARY}):\")\n",
    "print(f\"  Vectors: {core_vectors}\")\n",
    "print(f\"  Tokens: {core_tokens:,}\")\n",
    "print(f\"  Density: {core_tokens/core_vectors:.1f}\")\n",
    "print()\n",
    "print(f\"OORT CLOUD (L2 >= {CORE_BOUNDARY}):\")\n",
    "print(f\"  Vectors: {oort_vectors}\")\n",
    "print(f\"  Tokens: {oort_tokens}\")\n",
    "print(f\"  Density: {oort_tokens/oort_vectors:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T21:05:30.160546Z",
     "iopub.status.busy": "2025-11-27T21:05:30.160471Z",
     "iopub.status.idle": "2025-11-27T21:05:30.162421Z",
     "shell.execute_reply": "2025-11-27T21:05:30.162142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-tabulation (vector counts):\n",
      "                      Core       Oort      Total\n",
      "    Black Holes         12          1         13\n",
      "     Singletons         63         49        112\n",
      "          Total         75         50        125\n"
     ]
    }
   ],
   "source": [
    "# Cross-tabulate: BH/Singleton vs Core/Oort\n",
    "bh_core = (black_hole_mask.numpy() & core_mask).sum()\n",
    "bh_oort = (black_hole_mask.numpy() & oort_mask).sum()\n",
    "single_core = (singleton_mask.numpy() & core_mask).sum()\n",
    "single_oort = (singleton_mask.numpy() & oort_mask).sum()\n",
    "\n",
    "print(\"\\nCross-tabulation (vector counts):\")\n",
    "print(f\"{'':>15} {'Core':>10} {'Oort':>10} {'Total':>10}\")\n",
    "print(f\"{'Black Holes':>15} {bh_core:>10} {bh_oort:>10} {bh_core + bh_oort:>10}\")\n",
    "print(f\"{'Singletons':>15} {single_core:>10} {single_oort:>10} {single_core + single_oort:>10}\")\n",
    "print(f\"{'Total':>15} {core_vectors:>10} {oort_vectors:>10} {n_unique:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## The Outlier\n",
    "\n",
    "The most distant vector in our selection—sitting at the L∞ = 5 exponent boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T21:05:30.163326Z",
     "iopub.status.busy": "2025-11-27T21:05:30.163270Z",
     "iopub.status.idle": "2025-11-27T21:05:30.165474Z",
     "shell.execute_reply": "2025-11-27T21:05:30.165112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE OUTLIER\n",
      "  Vector index: 0\n",
      "  L2 distance from center: 0.005495\n",
      "  Token count: 1\n",
      "  Token ID: 27487\n",
      "  Decoded: '��取'\n",
      "  Thai: False, CJK: True\n"
     ]
    }
   ],
   "source": [
    "# Find The Outlier\n",
    "outlier_idx = np.argmax(r)\n",
    "outlier_dist = r[outlier_idx]\n",
    "outlier_count = counts[outlier_idx].item()\n",
    "\n",
    "# Find its token\n",
    "outlier_token = neighborhood_indices[(inverse_indices == outlier_idx).nonzero()[0]].item()\n",
    "outlier_decoded = tokenizer.decode([outlier_token])\n",
    "\n",
    "print(f\"THE OUTLIER\")\n",
    "print(f\"  Vector index: {outlier_idx}\")\n",
    "print(f\"  L2 distance from center: {outlier_dist:.6f}\")\n",
    "print(f\"  Token count: {outlier_count}\")\n",
    "print(f\"  Token ID: {outlier_token}\")\n",
    "print(f\"  Decoded: {repr(outlier_decoded)}\")\n",
    "\n",
    "# Check for Thai/CJK\n",
    "is_thai = any('\\u0e00' <= c <= '\\u0e7f' for c in outlier_decoded)\n",
    "is_cjk = any('\\u4e00' <= c <= '\\u9fff' for c in outlier_decoded)\n",
    "print(f\"  Thai: {is_thai}, CJK: {is_cjk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T21:05:30.166390Z",
     "iopub.status.busy": "2025-11-27T21:05:30.166331Z",
     "iopub.status.idle": "2025-11-27T21:05:30.168668Z",
     "shell.execute_reply": "2025-11-27T21:05:30.168314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FROZEN SMOKE CENSUS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total tokens in selection: 2,212\n",
      "Unique vectors: 125\n",
      "\n",
      "BLACK HOLES: 13\n",
      "  Tokens: 2,100\n",
      "  Populations: [814, 704, 306, 228, 11, 10, 6, 5, 4, 4, 3, 3, 2]\n",
      "\n",
      "SINGLETONS: 112\n",
      "\n",
      "CORE (L2 < 5e-05): 75 vectors, 2,159 tokens\n",
      "OORT CLOUD (L2 >= 5e-05): 50 vectors, 53 tokens\n",
      "\n",
      "THE OUTLIER: Token 27487, '��取', r = 0.005495\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FROZEN SMOKE CENSUS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Total tokens in selection: {n_tokens:,}\")\n",
    "print(f\"Unique vectors: {n_unique}\")\n",
    "print()\n",
    "print(f\"BLACK HOLES: {n_black_holes}\")\n",
    "print(f\"  Tokens: {counts[black_hole_mask].sum().item():,}\")\n",
    "print(f\"  Populations: {sorted(counts[black_hole_mask].tolist(), reverse=True)}\")\n",
    "print()\n",
    "print(f\"SINGLETONS: {n_singletons}\")\n",
    "print()\n",
    "print(f\"CORE (L2 < {CORE_BOUNDARY}): {core_vectors} vectors, {core_tokens:,} tokens\")\n",
    "print(f\"OORT CLOUD (L2 >= {CORE_BOUNDARY}): {oort_vectors} vectors, {oort_tokens} tokens\")\n",
    "print()\n",
    "print(f\"THE OUTLIER: Token {outlier_token}, {repr(outlier_decoded)}, r = {outlier_dist:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
