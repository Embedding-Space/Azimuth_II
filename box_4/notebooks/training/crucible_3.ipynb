{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crucible 3: h_mean Persistence and Live/Dead Separation\n",
    "\n",
    "**Hypothesis:** h_mean maintains a consistent direction over training, causing dead tokens to drift in a straight line (antiparallel to h) while live tokens drift in the opposite direction (parallel to h). This creates systematic separation between the two populations.\n",
    "\n",
    "**What's new vs Crucible 2:**\n",
    "- Records **all tokens** (live + dead), not just dead tokens\n",
    "- Extracts **h_mean[t]** at each training step\n",
    "- Simplified data collection (no m, v, g)\n",
    "- Focused on h_mean autocorrelation and centroid dynamics\n",
    "\n",
    "**Data captured:**\n",
    "- W[t]: full embedding matrix (all 10k tokens, bfloat16 as uint16)\n",
    "- h_mean[t]: average hidden state per step (float32)\n",
    "- loss[t]: training loss\n",
    "\n",
    "**Questions to answer:**\n",
    "1. Does h_mean autocorrelation stay high (~0.95) over 500 steps, or decay?\n",
    "2. Do live and dead token centroids separate over time?\n",
    "3. Does h_mean point toward the live centroid?\n",
    "\n",
    "**Storage:** ~0.64 GB total (much smaller than Crucible 2's 2.1 GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "TOTAL_STEPS = 500\n",
    "BATCH_SIZE = 128\n",
    "SEQ_LEN = 128\n",
    "LEARNING_RATE = 1e-3      # Same as Crucible 1 & 2\n",
    "WEIGHT_DECAY = 0.0        # Same as Crucible 1 & 2\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "EPSILON = 1e-8\n",
    "\n",
    "# Model parameters\n",
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 64\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 2\n",
    "\n",
    "# Reproducibility — SAME SEED as Crucible 1 & 2 for comparability\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Paths\n",
    "CORPUS_PATH = '../../data/flannel_model_corpus.txt'\n",
    "TOKENIZER_PATH = '../../data/flannel_tokenizer_chars.json'\n",
    "DEAD_MASK_PATH = '../../tensors/Flannel/live_dead_tokens.safetensors'\n",
    "OUTPUT_DIR = '../../tensors/Crucible-3'\n",
    "\n",
    "print(f\"Crucible 3: {TOTAL_STEPS} steps, lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY}\")\n",
    "print(f\"Recording full W (all {VOCAB_SIZE} tokens) + h_mean[t]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from safetensors.torch import save_file, load_file\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(f\"Random seed set to {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Dead Token Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "print(f\"Loaded tokenizer with vocab size {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "masks = load_file(DEAD_MASK_PATH)\n",
    "dead_mask = masks['dead_mask'].bool()\n",
    "dead_indices = masks['dead_indices'].long()\n",
    "live_mask = ~dead_mask\n",
    "n_dead = dead_mask.sum().item()\n",
    "n_live = live_mask.sum().item()\n",
    "\n",
    "print(f\"Dead tokens: {n_dead}\")\n",
    "print(f\"Live tokens: {n_live}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, corpus_path, tokenizer, seq_len):\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        encoding = tokenizer.encode(text)\n",
    "        self.tokens = encoding.ids\n",
    "        self.seq_len = seq_len\n",
    "        self.n_sequences = len(self.tokens) // seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_sequences\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_len\n",
    "        chunk = self.tokens[start:start + self.seq_len]\n",
    "        return torch.tensor(chunk, dtype=torch.long)\n",
    "\n",
    "\n",
    "dataset = TextDataset(CORPUS_PATH, tokenizer, SEQ_LEN)\n",
    "print(f\"Dataset: {len(dataset)} sequences of length {SEQ_LEN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Modified to optionally return hidden state h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, num_heads, seq_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(seq_len, hidden_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=0.0,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.ln_f = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.pos_embedding.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, input_ids, return_hidden=False):\n",
    "        \"\"\"If return_hidden=True, returns (logits, h) where h is the final hidden state.\"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        tok_emb = self.embedding(input_ids)\n",
    "        pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embedding(pos_ids)\n",
    "        \n",
    "        hidden = tok_emb + pos_emb\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len, device=input_ids.device)\n",
    "        hidden = self.transformer(hidden, mask=causal_mask, is_causal=True)\n",
    "        hidden = self.ln_f(hidden)  # This is h\n",
    "        \n",
    "        logits = hidden @ self.embedding.weight.T\n",
    "        \n",
    "        if return_hidden:\n",
    "            return logits, hidden\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = TinyLM(VOCAB_SIZE, HIDDEN_DIM, NUM_LAYERS, NUM_HEADS, SEQ_LEN)\n",
    "model = model.to(device).to(torch.bfloat16)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(BETA1, BETA2),\n",
    "    eps=EPSILON,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Setup\n",
    "\n",
    "Memory budget for 500 steps:\n",
    "- W: 501 × 10000 × 64 × 2 bytes ≈ 641 MB\n",
    "- h_mean: 500 × 64 × 4 bytes ≈ 0.13 MB\n",
    "- loss: 501 × 4 bytes ≈ 0.002 MB\n",
    "\n",
    "Total: ~641 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pre-allocate tensors\n",
    "W_history = torch.zeros(TOTAL_STEPS + 1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "h_mean_history = torch.zeros(TOTAL_STEPS, HIDDEN_DIM, dtype=torch.float32)\n",
    "loss_history = torch.zeros(TOTAL_STEPS + 1, dtype=torch.float32)\n",
    "\n",
    "# Memory estimate\n",
    "total_bytes = (\n",
    "    W_history.numel() * 2 +\n",
    "    h_mean_history.numel() * 4 +\n",
    "    loss_history.numel() * 4\n",
    ")\n",
    "print(f\"Pre-allocated {total_bytes / 1e9:.3f} GB for data collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Capture initial state (t=0)\n",
    "W_history[0] = model.embedding.weight.detach().cpu().to(torch.bfloat16)\n",
    "loss_history[0] = float('nan')\n",
    "\n",
    "print(f\"Starting training for {TOTAL_STEPS} steps...\")\n",
    "\n",
    "model.train()\n",
    "step = 0\n",
    "epoch = 0\n",
    "\n",
    "pbar = tqdm(total=TOTAL_STEPS, desc=\"Training\")\n",
    "\n",
    "while step < TOTAL_STEPS:\n",
    "    epoch += 1\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        if step >= TOTAL_STEPS:\n",
    "            break\n",
    "            \n",
    "        input_ids = batch.to(device)\n",
    "        \n",
    "        # Forward pass with hidden state extraction\n",
    "        with torch.autocast(device_type=device if device != 'mps' else 'cpu', dtype=torch.bfloat16):\n",
    "            logits, h = model(input_ids, return_hidden=True)\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            loss = loss_fn(shift_logits.view(-1, VOCAB_SIZE), shift_labels.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # === Capture state AFTER optimizer step ===\n",
    "        \n",
    "        # W[t] (all tokens)\n",
    "        W_history[step] = model.embedding.weight.detach().cpu().to(torch.bfloat16)\n",
    "        \n",
    "        # h_mean[t] (averaged over batch and sequence)\n",
    "        h_mean = h.mean(dim=(0, 1)).cpu().float()  # [D]\n",
    "        h_mean_history[step - 1] = h_mean\n",
    "        \n",
    "        # Loss[t]\n",
    "        loss_history[step] = loss.item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'epoch': epoch})\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "print(f\"\\nTraining complete. Final loss: {loss.item():.4f}\")\n",
    "print(f\"Completed {epoch} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CRUCIBLE 3 STATS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Centroid stats\n",
    "W_dead = W_history[:, dead_mask, :].float()\n",
    "W_live = W_history[:, live_mask, :].float()\n",
    "\n",
    "centroid_dead = W_dead.mean(dim=1)  # (501, 64)\n",
    "centroid_live = W_live.mean(dim=1)  # (501, 64)\n",
    "\n",
    "centroid_dead_norm = torch.norm(centroid_dead, dim=1)\n",
    "centroid_live_norm = torch.norm(centroid_live, dim=1)\n",
    "\n",
    "print(f\"\\nDead token centroid:\")\n",
    "print(f\"  Norm at t=0: {centroid_dead_norm[0].item():.6f}\")\n",
    "print(f\"  Norm at t=500: {centroid_dead_norm[-1].item():.6f}\")\n",
    "print(f\"  Total displacement: {torch.norm(centroid_dead[-1] - centroid_dead[0]).item():.6f}\")\n",
    "\n",
    "print(f\"\\nLive token centroid:\")\n",
    "print(f\"  Norm at t=0: {centroid_live_norm[0].item():.6f}\")\n",
    "print(f\"  Norm at t=500: {centroid_live_norm[-1].item():.6f}\")\n",
    "print(f\"  Total displacement: {torch.norm(centroid_live[-1] - centroid_live[0]).item():.6f}\")\n",
    "\n",
    "# Separation\n",
    "separation = torch.norm(centroid_dead - centroid_live, dim=1)\n",
    "print(f\"\\nLive/Dead separation:\")\n",
    "print(f\"  Distance at t=0: {separation[0].item():.6f}\")\n",
    "print(f\"  Distance at t=500: {separation[-1].item():.6f}\")\n",
    "\n",
    "# h_mean stats\n",
    "h_mean_norm = torch.norm(h_mean_history, dim=1)\n",
    "print(f\"\\nh_mean:\")\n",
    "print(f\"  L2 norm range: [{h_mean_norm.min().item():.6f}, {h_mean_norm.max().item():.6f}]\")\n",
    "print(f\"  Mean L2 norm: {h_mean_norm.mean().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = {\n",
    "    'W': W_history.view(torch.uint16),  # (501, 10000, 64) uint16\n",
    "    'h_mean': h_mean_history,            # (500, 64) float32\n",
    "    'loss': loss_history,                # (501,) float32\n",
    "    'dead_mask': dead_mask,\n",
    "    'dead_indices': dead_indices,\n",
    "}\n",
    "\n",
    "save_path = output_path / 'crucible_3_trajectory.safetensors'\n",
    "save_file(data_to_save, str(save_path))\n",
    "\n",
    "print(f\"Saved to {save_path}\")\n",
    "print(f\"File size: {save_path.stat().st_size / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'experiment': 'Crucible 3',\n",
    "    'series': 'Crucible',\n",
    "    'date': '2025-11-28',\n",
    "    'total_steps': TOTAL_STEPS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'seq_len': SEQ_LEN,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'beta1': BETA1,\n",
    "    'beta2': BETA2,\n",
    "    'epsilon': EPSILON,\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'random_seed': RANDOM_SEED,\n",
    "    'n_dead_tokens': n_dead,\n",
    "    'n_live_tokens': n_live,\n",
    "    'final_loss': loss_history[-1].item(),\n",
    "    'total_epochs': epoch,\n",
    "    'device': device,\n",
    "    'data_shapes': {\n",
    "        'W': list(W_history.shape),\n",
    "        'h_mean': list(h_mean_history.shape),\n",
    "        'loss': list(loss_history.shape),\n",
    "    },\n",
    "    'notes': 'h_mean persistence and live/dead separation study. Records all tokens + h_mean[t]. Same seed as Crucible 1 & 2.'\n",
    "}\n",
    "\n",
    "with open(output_path / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Saved metadata.json\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CRUCIBLE 3 COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
