{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Crucible 2: High-Resolution Phase Transition\n",
    "\n",
    "A 500-step experiment with full instrumentation to study the hot-to-cold transition.\n",
    "\n",
    "**What's new vs Crucible 1:**\n",
    "- Shorter run (500 steps) focused on the transition period\n",
    "- Records optimizer state: m[t] (momentum) and v[t] (variance)\n",
    "- Records raw gradients g[t] before Adam processing\n",
    "- Enables analysis of phase space (m, v) dynamics and gradient structure\n",
    "\n",
    "**Data captured:**\n",
    "- W[t]: dead token embeddings (bfloat16 as uint16)\n",
    "- m[t]: Adam momentum (float32)\n",
    "- v[t]: Adam variance (float32)\n",
    "- g[t]: raw gradients (float32)\n",
    "- ΔW′[t]: displacement in lattice-cell units (float32)\n",
    "- loss[t]: training loss\n",
    "\n",
    "**Known issue:** ΔW′ contains `inf` values where W[t-1] ≈ 0. The ULP calculation gives ULP = 2^(-134) for zero weights, so ΔW/ULP explodes. These are edge cases where \"displacement in lattice units\" is undefined. Filter with `torch.isfinite()` in downstream analysis.\n",
    "\n",
    "**Goal:** Understand *why* tokens transition in cohorts. Are they clustered in (m, v) phase space near the critical boundary? Are their gradients aligned?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crucible 2: 500 steps, lr=0.001, weight_decay=0.0\n",
      "High-resolution recording of W, m, v, g\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "TOTAL_STEPS = 500\n",
    "BATCH_SIZE = 128\n",
    "SEQ_LEN = 128\n",
    "LEARNING_RATE = 1e-3      # Same as Crucible 1\n",
    "WEIGHT_DECAY = 0.0        # Same as Crucible 1\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "EPSILON = 1e-8\n",
    "\n",
    "# Model parameters\n",
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 64\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 2\n",
    "\n",
    "# Reproducibility — SAME SEED as Crucible 1 for comparability\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Motion detection threshold\n",
    "MOTION_THRESHOLD = 0.1\n",
    "\n",
    "# Paths\n",
    "CORPUS_PATH = '../../data/flannel_model_corpus.txt'\n",
    "TOKENIZER_PATH = '../../data/flannel_tokenizer_chars.json'\n",
    "DEAD_MASK_PATH = '../../tensors/Flannel/live_dead_tokens.safetensors'\n",
    "OUTPUT_DIR = '../../tensors/Crucible-2'\n",
    "\n",
    "print(f\"Crucible 2: {TOTAL_STEPS} steps, lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY}\")\n",
    "print(f\"High-resolution recording of W, m, v, g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from safetensors.torch import save_file, load_file\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(f\"Random seed set to {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Dead Token Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer with vocab size 10000\n",
      "Dead tokens: 3699\n",
      "Live tokens: 6301\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "print(f\"Loaded tokenizer with vocab size {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "masks = load_file(DEAD_MASK_PATH)\n",
    "dead_mask = masks['dead_mask'].bool()\n",
    "dead_indices = masks['dead_indices'].long()\n",
    "n_dead = dead_mask.sum().item()\n",
    "\n",
    "print(f\"Dead tokens: {n_dead}\")\n",
    "print(f\"Live tokens: {VOCAB_SIZE - n_dead}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 10713 sequences of length 128\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, corpus_path, tokenizer, seq_len):\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        encoding = tokenizer.encode(text)\n",
    "        self.tokens = encoding.ids\n",
    "        self.seq_len = seq_len\n",
    "        self.n_sequences = len(self.tokens) // seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_sequences\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_len\n",
    "        chunk = self.tokens[start:start + self.seq_len]\n",
    "        return torch.tensor(chunk, dtype=torch.long)\n",
    "\n",
    "\n",
    "dataset = TextDataset(CORPUS_PATH, tokenizer, SEQ_LEN)\n",
    "print(f\"Dataset: {len(dataset)} sequences of length {SEQ_LEN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 748,288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jefferyharrell/Projects/Azimuth_II/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, num_heads, seq_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(seq_len, hidden_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=0.0,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.ln_f = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.pos_embedding.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        tok_emb = self.embedding(input_ids)\n",
    "        pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embedding(pos_ids)\n",
    "        \n",
    "        hidden = tok_emb + pos_emb\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len, device=input_ids.device)\n",
    "        hidden = self.transformer(hidden, mask=causal_mask, is_causal=True)\n",
    "        hidden = self.ln_f(hidden)\n",
    "        \n",
    "        logits = hidden @ self.embedding.weight.T\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = TinyLM(VOCAB_SIZE, HIDDEN_DIM, NUM_LAYERS, NUM_HEADS, SEQ_LEN)\n",
    "model = model.to(device).to(torch.bfloat16)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: AdamW (lr=0.001, weight_decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(BETA1, BETA2),\n",
    "    eps=EPSILON,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## ULP Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ulp_bf16(tensor_bf16: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute ULP for each element of a bfloat16 tensor.\"\"\"\n",
    "    bits = tensor_bf16.view(torch.uint16).to(torch.int32)\n",
    "    exponent = (bits >> 7) & 0xFF\n",
    "    effective_exp = torch.where(exponent == 0, torch.ones_like(exponent), exponent)\n",
    "    ulp = torch.pow(2.0, (effective_exp - 134).float())\n",
    "    return ulp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Data Collection Setup\n",
    "\n",
    "Memory budget for 500 steps:\n",
    "- W: 501 × 3699 × 64 × 2 bytes ≈ 237 MB\n",
    "- m: 501 × 3699 × 64 × 4 bytes ≈ 475 MB\n",
    "- v: 501 × 3699 × 64 × 4 bytes ≈ 475 MB\n",
    "- g: 500 × 3699 × 64 × 4 bytes ≈ 474 MB\n",
    "- ΔW′: 500 × 3699 × 64 × 4 bytes ≈ 474 MB\n",
    "\n",
    "Total: ~2.1 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-allocated 2.13 GB for data collection\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pre-allocate tensors\n",
    "W_history = torch.zeros(TOTAL_STEPS + 1, n_dead, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "m_history = torch.zeros(TOTAL_STEPS + 1, n_dead, HIDDEN_DIM, dtype=torch.float32)\n",
    "v_history = torch.zeros(TOTAL_STEPS + 1, n_dead, HIDDEN_DIM, dtype=torch.float32)\n",
    "g_history = torch.zeros(TOTAL_STEPS, n_dead, HIDDEN_DIM, dtype=torch.float32)\n",
    "delta_W_prime_history = torch.zeros(TOTAL_STEPS, n_dead, HIDDEN_DIM, dtype=torch.float32)\n",
    "loss_history = torch.zeros(TOTAL_STEPS + 1, dtype=torch.float32)\n",
    "\n",
    "# Last motion tracker\n",
    "last_motion_step = torch.full((n_dead,), -1, dtype=torch.int32)\n",
    "\n",
    "# Memory estimate\n",
    "total_bytes = (\n",
    "    W_history.numel() * 2 +\n",
    "    m_history.numel() * 4 +\n",
    "    v_history.numel() * 4 +\n",
    "    g_history.numel() * 4 +\n",
    "    delta_W_prime_history.numel() * 4 +\n",
    "    loss_history.numel() * 4 +\n",
    "    last_motion_step.numel() * 4\n",
    ")\n",
    "print(f\"Pre-allocated {total_bytes / 1e9:.2f} GB for data collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Find Embedding Parameter in Optimizer\n",
    "\n",
    "We need to extract m and v specifically for the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding parameter location in optimizer: group 0, param 0\n"
     ]
    }
   ],
   "source": [
    "# Find which param group index corresponds to embedding.weight\n",
    "embedding_param_id = id(model.embedding.weight)\n",
    "embedding_param_idx = None\n",
    "\n",
    "for group_idx, group in enumerate(optimizer.param_groups):\n",
    "    for param_idx, param in enumerate(group['params']):\n",
    "        if id(param) == embedding_param_id:\n",
    "            embedding_param_idx = (group_idx, param_idx)\n",
    "            break\n",
    "    if embedding_param_idx is not None:\n",
    "        break\n",
    "\n",
    "print(f\"Embedding parameter location in optimizer: group {embedding_param_idx[0]}, param {embedding_param_idx[1]}\")\n",
    "\n",
    "# Helper to get optimizer state for embedding\n",
    "def get_embedding_optimizer_state():\n",
    "    state = optimizer.state[model.embedding.weight]\n",
    "    if 'exp_avg' in state:\n",
    "        m = state['exp_avg'].detach().cpu()[dead_mask].float()\n",
    "        v = state['exp_avg_sq'].detach().cpu()[dead_mask].float()\n",
    "        return m, v\n",
    "    else:\n",
    "        # Before first step, no state yet\n",
    "        return torch.zeros(n_dead, HIDDEN_DIM), torch.zeros(n_dead, HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 500 steps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [00:33<00:00, 14.98it/s, loss=6.1562, epoch=7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Final loss: 6.1562\n",
      "Completed 7 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Capture initial state (t=0)\n",
    "W_prev = model.embedding.weight.detach().cpu()[dead_mask].to(torch.bfloat16)\n",
    "W_history[0] = W_prev\n",
    "m_history[0] = torch.zeros(n_dead, HIDDEN_DIM)  # No momentum yet\n",
    "v_history[0] = torch.zeros(n_dead, HIDDEN_DIM)  # No variance yet\n",
    "loss_history[0] = float('nan')\n",
    "\n",
    "print(f\"Starting training for {TOTAL_STEPS} steps...\")\n",
    "\n",
    "model.train()\n",
    "step = 0\n",
    "epoch = 0\n",
    "\n",
    "pbar = tqdm(total=TOTAL_STEPS, desc=\"Training\")\n",
    "\n",
    "while step < TOTAL_STEPS:\n",
    "    epoch += 1\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        if step >= TOTAL_STEPS:\n",
    "            break\n",
    "            \n",
    "        input_ids = batch.to(device)\n",
    "        \n",
    "        with torch.autocast(device_type=device if device != 'mps' else 'cpu', dtype=torch.bfloat16):\n",
    "            logits = model(input_ids)\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            loss = loss_fn(shift_logits.view(-1, VOCAB_SIZE), shift_labels.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # === Capture gradient BEFORE optimizer step ===\n",
    "        grad = model.embedding.weight.grad.detach().cpu()[dead_mask].float()\n",
    "        g_history[step] = grad\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # === Capture state AFTER optimizer step ===\n",
    "        W_curr = model.embedding.weight.detach().cpu()[dead_mask].to(torch.bfloat16)\n",
    "        m_curr, v_curr = get_embedding_optimizer_state()\n",
    "        \n",
    "        # W[t]\n",
    "        W_history[step] = W_curr\n",
    "        \n",
    "        # m[t] and v[t] (after this step's update)\n",
    "        m_history[step] = m_curr\n",
    "        v_history[step] = v_curr\n",
    "        \n",
    "        # ΔW = W[t] - W[t-1]\n",
    "        delta_W = W_curr.float() - W_prev.float()\n",
    "        \n",
    "        # ULP at W[t-1]\n",
    "        ulp = compute_ulp_bf16(W_prev)\n",
    "        \n",
    "        # ΔW′ = ΔW / ULP (lattice displacement)\n",
    "        delta_W_prime = delta_W / ulp\n",
    "        delta_W_prime_history[step - 1] = delta_W_prime\n",
    "        \n",
    "        # Loss\n",
    "        loss_history[step] = loss.item()\n",
    "        \n",
    "        # Update last_motion_step for tokens that moved\n",
    "        displacement_magnitude = torch.norm(delta_W_prime, dim=1)\n",
    "        moved = displacement_magnitude >= MOTION_THRESHOLD\n",
    "        last_motion_step[moved] = step\n",
    "        \n",
    "        # Update W_prev for next iteration\n",
    "        W_prev = W_curr\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'epoch': epoch})\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "print(f\"\\nTraining complete. Final loss: {loss.item():.4f}\")\n",
    "print(f\"Completed {epoch} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Quick Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CRUCIBLE 2 STATS\n",
      "============================================================\n",
      "\n",
      "Displacement |ΔW′|₂:\n",
      "  Range: [0.00, inf]\n",
      "  Mean at step 1: 7450.73\n",
      "  Mean at step 500: 0.70\n",
      "\n",
      "Momentum |m|:\n",
      "  Range: [1.32e-05, 8.18e-04]\n",
      "\n",
      "Variance v (mean across dims):\n",
      "  Range: [2.19e-12, 4.09e-10]\n",
      "\n",
      "Gradient |g|:\n",
      "  Range: [1.14e-05, 1.23e-03]\n",
      "\n",
      "Motion:\n",
      "  Last motion (any token): step 500\n",
      "  Tokens still moving at step 500: 2270\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CRUCIBLE 2 STATS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Displacement stats\n",
    "disp_l2 = torch.norm(delta_W_prime_history, dim=2)  # (500, 3699)\n",
    "print(f\"\\nDisplacement |ΔW′|₂:\")\n",
    "print(f\"  Range: [{disp_l2.min():.2f}, {disp_l2.max():.2f}]\")\n",
    "print(f\"  Mean at step 1: {disp_l2[0].mean():.2f}\")\n",
    "print(f\"  Mean at step 500: {disp_l2[-1].mean():.2f}\")\n",
    "\n",
    "# Momentum stats\n",
    "m_norm = torch.norm(m_history[1:], dim=2)  # (500, 3699)\n",
    "print(f\"\\nMomentum |m|:\")\n",
    "print(f\"  Range: [{m_norm.min():.2e}, {m_norm.max():.2e}]\")\n",
    "\n",
    "# Variance stats\n",
    "v_mean = v_history[1:].mean(dim=2)  # (500, 3699)\n",
    "print(f\"\\nVariance v (mean across dims):\")\n",
    "print(f\"  Range: [{v_mean.min():.2e}, {v_mean.max():.2e}]\")\n",
    "\n",
    "# Gradient stats\n",
    "g_norm = torch.norm(g_history, dim=2)  # (500, 3699)\n",
    "print(f\"\\nGradient |g|:\")\n",
    "print(f\"  Range: [{g_norm.min():.2e}, {g_norm.max():.2e}]\")\n",
    "\n",
    "# Motion stats\n",
    "global_last_motion = last_motion_step.max().item()\n",
    "tokens_still_moving = (last_motion_step == TOTAL_STEPS).sum().item()\n",
    "print(f\"\\nMotion:\")\n",
    "print(f\"  Last motion (any token): step {global_last_motion}\")\n",
    "print(f\"  Tokens still moving at step 500: {tokens_still_moving}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../tensors/Crucible-2/crucible_2_trajectory.safetensors\n",
      "File size: 2.13 GB\n"
     ]
    }
   ],
   "source": [
    "data_to_save = {\n",
    "    'W': W_history.view(torch.uint16),        # (501, 3699, 64) uint16\n",
    "    'm': m_history,                            # (501, 3699, 64) float32\n",
    "    'v': v_history,                            # (501, 3699, 64) float32\n",
    "    'g': g_history,                            # (500, 3699, 64) float32\n",
    "    'delta_W_prime': delta_W_prime_history,    # (500, 3699, 64) float32\n",
    "    'loss': loss_history,                      # (501,) float32\n",
    "    'last_motion_step': last_motion_step,      # (3699,) int32\n",
    "    'dead_mask': dead_mask,\n",
    "    'dead_indices': dead_indices,\n",
    "}\n",
    "\n",
    "save_path = output_path / 'crucible_2_trajectory.safetensors'\n",
    "save_file(data_to_save, str(save_path))\n",
    "\n",
    "print(f\"Saved to {save_path}\")\n",
    "print(f\"File size: {save_path.stat().st_size / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metadata.json\n",
      "\n",
      "============================================================\n",
      "CRUCIBLE 2 COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "    'experiment': 'Crucible 2',\n",
    "    'series': 'Crucible',\n",
    "    'date': '2025-11-25',\n",
    "    'total_steps': TOTAL_STEPS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'seq_len': SEQ_LEN,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'beta1': BETA1,\n",
    "    'beta2': BETA2,\n",
    "    'epsilon': EPSILON,\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'random_seed': RANDOM_SEED,\n",
    "    'motion_threshold': MOTION_THRESHOLD,\n",
    "    'n_dead_tokens': n_dead,\n",
    "    'final_loss': loss_history[-1].item(),\n",
    "    'total_epochs': epoch,\n",
    "    'device': device,\n",
    "    'global_last_motion': int(global_last_motion),\n",
    "    'tokens_still_moving': int(tokens_still_moving),\n",
    "    'data_shapes': {\n",
    "        'W': list(W_history.shape),\n",
    "        'm': list(m_history.shape),\n",
    "        'v': list(v_history.shape),\n",
    "        'g': list(g_history.shape),\n",
    "        'delta_W_prime': list(delta_W_prime_history.shape),\n",
    "        'loss': list(loss_history.shape),\n",
    "        'last_motion_step': list(last_motion_step.shape),\n",
    "    },\n",
    "    'notes': 'High-resolution phase transition study. Same seed as Crucible 1. Records W, m, v, g.'\n",
    "}\n",
    "\n",
    "with open(output_path / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Saved metadata.json\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CRUCIBLE 2 COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
