{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.10b: Random Bytes Tokenization Test\n",
    "\n",
    "**Testing the binary garbage hypothesis**\n",
    "\n",
    "## The Hypothesis\n",
    "\n",
    "Jeffery's insight: **The tokenizer operates on bytes, not Unicode.**\n",
    "\n",
    "What if halo tokens (unreachable via Unicode round-trip) are actually *reachable via non-UTF-8 byte sequences*?\n",
    "\n",
    "**Proposed mechanism:**\n",
    "1. Qwen's training data includes binary garbage (corrupted files, non-UTF-8 encodings, PDF fragments, etc.)\n",
    "2. Tokenizer encodes these byte sequences → produces \"halo\" tokens\n",
    "3. Model receives gradients for these tokens → they develop normal embeddings\n",
    "4. But we can't decode them to valid Unicode → round-trip test fails → labeled \"unreachable\"\n",
    "\n",
    "**Meanwhile:**\n",
    "- Cluster tokens = valid Unicode that never appeared in training (Thai script, etc.)\n",
    "- No gradients → geometric collapse\n",
    "- Different category entirely\n",
    "\n",
    "## The Test\n",
    "\n",
    "Feed the tokenizer **random bytes** (not valid UTF-8) and see which token categories appear:\n",
    "\n",
    "**If hypothesis is correct:**\n",
    "- Halo tokens should appear **disproportionately often**\n",
    "- Random bytes → invalid UTF-8 → tokens that can't round-trip → halo tokens\n",
    "\n",
    "**Null hypothesis:**\n",
    "- Random bytes produce mostly bulk tokens (same distribution as valid UTF-8 text)\n",
    "- Halo tokens appear at baseline rate (~0.9% of vocabulary)\n",
    "\n",
    "## Method\n",
    "\n",
    "1. Generate random byte sequences (various lengths)\n",
    "2. Tokenize them (allow errors, don't enforce UTF-8 validity)\n",
    "3. Collect all produced token IDs\n",
    "4. Classify each token: cluster / halo / bulk\n",
    "5. Compare distributions\n",
    "\n",
    "**Statistical validity:**\n",
    "- Generate millions of bytes\n",
    "- Tokenize thousands of sequences\n",
    "- Count token category frequencies\n",
    "- Compute enrichment: P(halo | random bytes) vs P(halo | vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\"\n",
    "HF_MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# Input data\n",
    "CLUSTER_TOKENS_PATH = \"../tensors/Qwen3-4B-Instruct-2507/1.4h_cluster_tokens.safetensors\"\n",
    "REACHABILITY_PATH = \"../tensors/Qwen3-4B-Instruct-2507/1.8d_full_vocab_reachability.safetensors\"\n",
    "\n",
    "# Random byte generation\n",
    "NUM_SEQUENCES = 10000  # Number of random byte sequences to generate\n",
    "MIN_SEQ_LENGTH = 10\n",
    "MAX_SEQ_LENGTH = 1000\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1168adb30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from safetensors.torch import load_file\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Token Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading token classifications...\n",
      "\n",
      "✓ Loaded token classifications\n",
      "  Cluster tokens: 2,212\n",
      "  Halo tokens: 1,423\n",
      "  Bulk tokens: 148,034\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading token classifications...\\n\")\n",
    "\n",
    "# Load cluster tokens\n",
    "cluster_data = load_file(CLUSTER_TOKENS_PATH)\n",
    "cluster_token_ids = set(cluster_data['cluster_token_ids'].tolist())\n",
    "\n",
    "# Load halo tokens (unreachable outside cluster)\n",
    "reachability_data = load_file(REACHABILITY_PATH)\n",
    "halo_token_ids = set(reachability_data['unreachable_outside_cluster'].tolist())\n",
    "\n",
    "print(f\"✓ Loaded token classifications\")\n",
    "print(f\"  Cluster tokens: {len(cluster_token_ids):,}\")\n",
    "print(f\"  Halo tokens: {len(halo_token_ids):,}\")\n",
    "print(f\"  Bulk tokens: {151669 - len(cluster_token_ids) - len(halo_token_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer: Qwen/Qwen3-4B-Instruct-2507\n",
      "\n",
      "✓ Tokenizer loaded\n",
      "  Vocabulary size: 151,669 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading tokenizer: {HF_MODEL_NAME}\\n\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "print(f\"✓ Tokenizer loaded\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Tokenize Random Bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GENERATING RANDOM BYTE SEQUENCES\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Number of sequences: 10,000\n",
      "  Sequence length: 10-1000 bytes\n",
      "\n",
      "Tokenizing...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing random bytes: 100%|██████████| 10000/10000 [00:03<00:00, 2719.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Tokenization complete\n",
      "  Sequences processed: 10,000 / 10,000\n",
      "  Failed sequences: 0\n",
      "  Total tokens produced: 5,421,516\n",
      "  Unique tokens: 5,848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GENERATING RANDOM BYTE SEQUENCES\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Number of sequences: {NUM_SEQUENCES:,}\")\n",
    "print(f\"  Sequence length: {MIN_SEQ_LENGTH}-{MAX_SEQ_LENGTH} bytes\")\n",
    "print(f\"\\nTokenizing...\\n\")\n",
    "\n",
    "# Collect all token IDs produced\n",
    "all_tokens = []\n",
    "failed_sequences = 0\n",
    "\n",
    "for i in tqdm(range(NUM_SEQUENCES), desc=\"Tokenizing random bytes\"):\n",
    "    # Generate random byte sequence\n",
    "    seq_length = np.random.randint(MIN_SEQ_LENGTH, MAX_SEQ_LENGTH + 1)\n",
    "    random_bytes = bytes(np.random.randint(0, 256, size=seq_length, dtype=np.uint8))\n",
    "    \n",
    "    try:\n",
    "        # Convert bytes to string using Latin-1 encoding\n",
    "        # Latin-1 maps bytes 0-255 to Unicode U+0000-U+00FF (lossless)\n",
    "        # This allows any byte sequence to be represented as a \"string\"\n",
    "        byte_string = random_bytes.decode('latin-1')\n",
    "        \n",
    "        # Tokenize the byte string\n",
    "        token_ids = tokenizer.encode(byte_string, add_special_tokens=False)\n",
    "        all_tokens.extend(token_ids)\n",
    "    except Exception as e:\n",
    "        # Track failures but continue\n",
    "        failed_sequences += 1\n",
    "        if failed_sequences <= 5:  # Show first 5 errors\n",
    "            print(f\"  Warning: Failed to tokenize sequence {i}: {e}\")\n",
    "\n",
    "print(f\"\\n✓ Tokenization complete\")\n",
    "print(f\"  Sequences processed: {NUM_SEQUENCES - failed_sequences:,} / {NUM_SEQUENCES:,}\")\n",
    "print(f\"  Failed sequences: {failed_sequences:,}\")\n",
    "print(f\"  Total tokens produced: {len(all_tokens):,}\")\n",
    "print(f\"  Unique tokens: {len(set(all_tokens)):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLASSIFYING TOKENS\n",
      "======================================================================\n",
      "\n",
      "Token distribution from random bytes:\n",
      "  Cluster tokens: 0 (0.00%)\n",
      "  Halo tokens: 1,187,940 (21.91%)\n",
      "  Bulk tokens: 4,233,576 (78.09%)\n",
      "  Total: 5,421,516\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLASSIFYING TOKENS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Count tokens by category\n",
    "cluster_count = 0\n",
    "halo_count = 0\n",
    "bulk_count = 0\n",
    "\n",
    "for token_id in all_tokens:\n",
    "    if token_id in cluster_token_ids:\n",
    "        cluster_count += 1\n",
    "    elif token_id in halo_token_ids:\n",
    "        halo_count += 1\n",
    "    else:\n",
    "        bulk_count += 1\n",
    "\n",
    "total_tokens = len(all_tokens)\n",
    "\n",
    "if total_tokens == 0:\n",
    "    print(\"ERROR: No tokens were produced!\")\n",
    "    print(\"Cannot compute statistics with zero tokens.\")\n",
    "else:\n",
    "    print(f\"Token distribution from random bytes:\")\n",
    "    print(f\"  Cluster tokens: {cluster_count:,} ({100*cluster_count/total_tokens:.2f}%)\")\n",
    "    print(f\"  Halo tokens: {halo_count:,} ({100*halo_count/total_tokens:.2f}%)\")\n",
    "    print(f\"  Bulk tokens: {bulk_count:,} ({100*bulk_count/total_tokens:.2f}%)\")\n",
    "    print(f\"  Total: {total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Vocabulary Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPARISON TO VOCABULARY BASELINE\n",
      "======================================================================\n",
      "\n",
      "Baseline (vocabulary proportions):\n",
      "  Cluster: 1.46%\n",
      "  Halo: 0.94%\n",
      "  Bulk: 97.60%\n",
      "\n",
      "Observed (from random bytes):\n",
      "  Cluster: 0.00%\n",
      "  Halo: 21.91%\n",
      "  Bulk: 78.09%\n",
      "\n",
      "Enrichment (observed / baseline):\n",
      "  Cluster: 0.00x\n",
      "  Halo: 23.35x ← ENRICHED\n",
      "  Bulk: 0.80x\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARISON TO VOCABULARY BASELINE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Vocabulary proportions (if tokens were sampled uniformly)\n",
    "vocab_cluster_pct = 100 * len(cluster_token_ids) / vocab_size\n",
    "vocab_halo_pct = 100 * len(halo_token_ids) / vocab_size\n",
    "vocab_bulk_pct = 100 * (vocab_size - len(cluster_token_ids) - len(halo_token_ids)) / vocab_size\n",
    "\n",
    "# Observed proportions from random bytes\n",
    "obs_cluster_pct = 100 * cluster_count / total_tokens\n",
    "obs_halo_pct = 100 * halo_count / total_tokens\n",
    "obs_bulk_pct = 100 * bulk_count / total_tokens\n",
    "\n",
    "# Enrichment (observed / expected)\n",
    "cluster_enrichment = obs_cluster_pct / vocab_cluster_pct if vocab_cluster_pct > 0 else 0\n",
    "halo_enrichment = obs_halo_pct / vocab_halo_pct if vocab_halo_pct > 0 else 0\n",
    "bulk_enrichment = obs_bulk_pct / vocab_bulk_pct if vocab_bulk_pct > 0 else 0\n",
    "\n",
    "print(\"Baseline (vocabulary proportions):\")\n",
    "print(f\"  Cluster: {vocab_cluster_pct:.2f}%\")\n",
    "print(f\"  Halo: {vocab_halo_pct:.2f}%\")\n",
    "print(f\"  Bulk: {vocab_bulk_pct:.2f}%\")\n",
    "\n",
    "print(f\"\\nObserved (from random bytes):\")\n",
    "print(f\"  Cluster: {obs_cluster_pct:.2f}%\")\n",
    "print(f\"  Halo: {obs_halo_pct:.2f}%\")\n",
    "print(f\"  Bulk: {obs_bulk_pct:.2f}%\")\n",
    "\n",
    "print(f\"\\nEnrichment (observed / baseline):\")\n",
    "print(f\"  Cluster: {cluster_enrichment:.2f}x\")\n",
    "print(f\"  Halo: {halo_enrichment:.2f}x {'← ENRICHED' if halo_enrichment > 1.5 else ''}\")\n",
    "print(f\"  Bulk: {bulk_enrichment:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STATISTICAL SIGNIFICANCE\n",
      "======================================================================\n",
      "\n",
      "Chi-square goodness-of-fit test:\n",
      "  Null hypothesis: Random bytes produce tokens uniformly from vocabulary\n",
      "  Chi-square statistic: 25709026.65\n",
      "  p-value: 0.00e+00\n",
      "\n",
      "  ✓ SIGNIFICANT: Random bytes do NOT sample uniformly (p < 0.001)\n",
      "    The distribution is significantly different from baseline.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STATISTICAL SIGNIFICANCE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Chi-square test: are observed frequencies different from expected?\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "# Expected frequencies (if tokens were sampled uniformly from vocab)\n",
    "expected = [\n",
    "    total_tokens * len(cluster_token_ids) / vocab_size,\n",
    "    total_tokens * len(halo_token_ids) / vocab_size,\n",
    "    total_tokens * (vocab_size - len(cluster_token_ids) - len(halo_token_ids)) / vocab_size,\n",
    "]\n",
    "\n",
    "# Observed frequencies\n",
    "observed = [cluster_count, halo_count, bulk_count]\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p_value = chisquare(observed, expected)\n",
    "\n",
    "print(f\"Chi-square goodness-of-fit test:\")\n",
    "print(f\"  Null hypothesis: Random bytes produce tokens uniformly from vocabulary\")\n",
    "print(f\"  Chi-square statistic: {chi2:.2f}\")\n",
    "print(f\"  p-value: {p_value:.2e}\")\n",
    "\n",
    "if p_value < 0.001:\n",
    "    print(f\"\\n  ✓ SIGNIFICANT: Random bytes do NOT sample uniformly (p < 0.001)\")\n",
    "    print(f\"    The distribution is significantly different from baseline.\")\n",
    "elif p_value < 0.05:\n",
    "    print(f\"\\n  ✓ SIGNIFICANT: Random bytes do NOT sample uniformly (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"\\n  ✗ NOT SIGNIFICANT: Cannot reject null hypothesis (p ≥ 0.05)\")\n",
    "    print(f\"    Random bytes might sample uniformly from vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Tokens from Random Bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MOST COMMON TOKENS FROM RANDOM BYTES\n",
      "======================================================================\n",
      "\n",
      "Top 20 tokens produced by random bytes:\n",
      "\n",
      "  Token ID |  Count | Category | Decoded\n",
      "  ---------+--------+----------+------------------------------\n",
      "       126 | 585,833 |     Halo | '�'\n",
      "       236 | 20,148 |     Halo | '�'\n",
      "     41873 | 20,081 |     Bulk | '¼'\n",
      "       206 | 19,954 |     Bulk | '\\x12'\n",
      "     59497 | 19,950 |     Bulk | '¹'\n",
      "       250 | 19,927 |     Halo | '�'\n",
      "       214 | 19,925 |     Bulk | '\\x1a'\n",
      "       253 | 19,912 |     Halo | '�'\n",
      "       216 | 19,907 |     Bulk | '\\x1c'\n",
      "       212 | 19,895 |     Bulk | '\\x18'\n",
      "     45913 | 19,874 |     Bulk | 'Ç'\n",
      "        22 | 19,865 |     Bulk | '7'\n",
      "       221 | 19,861 |     Bulk | '\\x7f'\n",
      "    131371 | 19,858 |     Bulk | 'Ô'\n",
      "       219 | 19,857 |     Bulk | '\\x1f'\n",
      "       205 | 19,849 |     Bulk | '\\x11'\n",
      "       190 | 19,847 |     Bulk | '\\x02'\n",
      "       249 | 19,841 |     Halo | '�'\n",
      "       232 | 19,841 |     Halo | '�'\n",
      "       217 | 19,832 |     Bulk | '\\x1d'\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MOST COMMON TOKENS FROM RANDOM BYTES\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "token_counter = Counter(all_tokens)\n",
    "most_common = token_counter.most_common(20)\n",
    "\n",
    "print(f\"Top 20 tokens produced by random bytes:\\n\")\n",
    "print(f\"  {'Token ID':>8} | {'Count':>6} | {'Category':>8} | Decoded\")\n",
    "print(f\"  {'-'*8}-+-{'-'*6}-+-{'-'*8}-+{'-'*30}\")\n",
    "\n",
    "for token_id, count in most_common:\n",
    "    # Classify\n",
    "    if token_id in cluster_token_ids:\n",
    "        category = \"Cluster\"\n",
    "    elif token_id in halo_token_ids:\n",
    "        category = \"Halo\"\n",
    "    else:\n",
    "        category = \"Bulk\"\n",
    "    \n",
    "    # Decode\n",
    "    decoded = tokenizer.decode([token_id])\n",
    "    decoded_display = repr(decoded)[:28]\n",
    "    \n",
    "    print(f\"  {token_id:8d} | {count:6,} | {category:>8} | {decoded_display}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "**If halo enrichment > 1.5x:**\n",
    "- ✓ **Hypothesis supported**: Random bytes disproportionately produce halo tokens\n",
    "- Halo tokens are reachable via non-UTF-8 byte sequences\n",
    "- They likely appeared in Qwen's training data as binary garbage\n",
    "- Round-trip test fails because they can't be decoded to valid Unicode\n",
    "- But they're not \"dead\"—they received gradients and have normal embeddings\n",
    "\n",
    "**If halo enrichment ≈ 1.0x:**\n",
    "- ✗ **Hypothesis not supported**: Random bytes sample uniformly from vocabulary\n",
    "- Halo tokens are not preferentially produced by binary garbage\n",
    "- Need alternative explanation for why they're outside the cluster\n",
    "\n",
    "**If cluster enrichment > 1.0x:**\n",
    "- Unexpected! Cluster tokens appearing from random bytes?\n",
    "- Would suggest cluster tokens are also reachable via byte sequences\n",
    "- But they collapsed geometrically—why?\n",
    "\n",
    "**Key insight:**\n",
    "\n",
    "This test distinguishes between:\n",
    "- **Halo tokens**: Reachable via bytes, not via Unicode (binary garbage)\n",
    "- **Cluster tokens**: Not reachable via bytes OR Unicode (truly unused)\n",
    "- **Bulk tokens**: Reachable via Unicode (normal training data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
