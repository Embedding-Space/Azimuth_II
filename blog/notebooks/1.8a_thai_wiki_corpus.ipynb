{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.8a: Thai Wikipedia Corpus\n",
    "\n",
    "This notebook builds a test corpus of Thai Wikipedia articles to test whether cluster tokens appear in natural Thai text.\n",
    "\n",
    "## The Question\n",
    "\n",
    "We've discovered that:\n",
    "- 71.4% of cluster tokens (1,579 / 2,212) are Thai script\n",
    "- This represents 61.4% of ALL Thai tokens in the vocabulary\n",
    "- Cluster is 47.9× enriched for Thai compared to full vocabulary\n",
    "\n",
    "**But are these tokens actually USED in Thai text?**\n",
    "\n",
    "Hypothesis: Maybe cluster tokens are legitimate but rare (technical terms, archaic language, etc.)\n",
    "\n",
    "Test: Sample 100 random Thai Wikipedia articles and check if cluster tokens appear.\n",
    "\n",
    "## Method\n",
    "\n",
    "1. **Sampling strategy**: Use Wikipedia's random article API\n",
    "   - Fetch 100 random Thai Wikipedia articles\n",
    "   - Save page IDs for reproducibility\n",
    "   - On subsequent runs, re-fetch same articles by ID\n",
    "\n",
    "2. **Data storage**: Save each article as JSON with metadata\n",
    "   - Page ID, title, URL\n",
    "   - Fetch date\n",
    "   - Raw text (cleaned)\n",
    "   - Character/word counts\n",
    "\n",
    "3. **Reproducibility**: First run is non-deterministic (random sampling), but page IDs are saved so subsequent runs fetch identical articles\n",
    "\n",
    "## Why 100 articles?\n",
    "\n",
    "- Large enough for statistical validity\n",
    "- Covers diverse topics (science, culture, history, etc.)\n",
    "- Expected token count: ~200k-500k tokens\n",
    "- If cluster tokens don't appear in 100 diverse articles, they're genuinely absent (not just \"rare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling parameters\n",
    "NUM_ARTICLES = 100\n",
    "LANGUAGE = 'th'  # Thai Wikipedia\n",
    "\n",
    "# File paths\n",
    "PAGE_IDS_FILE = '../data/1.8a_thai_wiki_page_ids.txt'\n",
    "OUTPUT_DIR = '../data/1.8a_thai_wiki_corpus/'\n",
    "\n",
    "# Wikipedia API\n",
    "WIKI_API_URL = 'https://th.wikipedia.org/w/api.php'\n",
    "WIKI_BASE_URL = 'https://th.wikipedia.org/wiki/'\n",
    "\n",
    "# User-Agent (Wikipedia requires this for API requests)\n",
    "USER_AGENT = 'AzimuthTokenResearch/1.0 (https://github.com/yourname/azimuth; contact@example.com)'\n",
    "\n",
    "# Rate limiting (be nice to Wikipedia)\n",
    "REQUEST_DELAY = 0.1  # seconds between requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def fetch_random_page_ids(n, delay=0.1):\n",
    "    \"\"\"\n",
    "    Fetch N random Thai Wikipedia page IDs using the random API.\n",
    "    \n",
    "    Returns list of page IDs (integers).\n",
    "    \"\"\"\n",
    "    page_ids = []\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    \n",
    "    print(f\"Fetching {n} random Thai Wikipedia page IDs...\")\n",
    "    \n",
    "    for i in tqdm(range(n), desc=\"Fetching page IDs\"):\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'list': 'random',\n",
    "            'rnnamespace': 0,  # Main namespace only (articles)\n",
    "            'rnlimit': 1,\n",
    "            'format': 'json'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(WIKI_API_URL, params=params, headers=headers)\n",
    "        data = response.json()\n",
    "        \n",
    "        page_id = data['query']['random'][0]['id']\n",
    "        page_ids.append(page_id)\n",
    "        \n",
    "        # Be nice to Wikipedia\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    return page_ids\n",
    "\n",
    "\n",
    "def fetch_article_by_id(page_id, delay=0.1):\n",
    "    \"\"\"\n",
    "    Fetch article content by page ID.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - page_id\n",
    "    - title\n",
    "    - url\n",
    "    - text (cleaned)\n",
    "    - char_count\n",
    "    - fetch_date\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    \n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'pageids': page_id,\n",
    "        'prop': 'extracts|info',\n",
    "        'exintro': False,  # Get full article, not just intro\n",
    "        'explaintext': True,  # Plain text, no HTML\n",
    "        'inprop': 'url',\n",
    "        'format': 'json'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(WIKI_API_URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "    \n",
    "    page_data = data['query']['pages'][str(page_id)]\n",
    "    \n",
    "    # Extract text and clean\n",
    "    text = page_data.get('extract', '')\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\n\\n+', '\\n\\n', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    article = {\n",
    "        'page_id': page_id,\n",
    "        'title': page_data.get('title', ''),\n",
    "        'url': page_data.get('fullurl', ''),\n",
    "        'text': text,\n",
    "        'char_count': len(text),\n",
    "        'fetch_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    time.sleep(delay)\n",
    "    \n",
    "    return article\n",
    "\n",
    "\n",
    "def save_article(article, output_dir):\n",
    "    \"\"\"\n",
    "    Save article as JSON file.\n",
    "    \n",
    "    Filename: {page_id}.json\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir) / f\"{article['page_id']}.json\"\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(article, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Output directory: ../data/1.8a_thai_wiki_corpus/\n",
      "✓ Page IDs file: ../data/1.8a_thai_wiki_page_ids.txt\n"
     ]
    }
   ],
   "source": [
    "# Create output directories\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(PAGE_IDS_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"✓ Page IDs file: {PAGE_IDS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Page IDs (Reproducible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAMPLING STRATEGY\n",
      "======================================================================\n",
      "\n",
      "⚠ No existing page IDs found (FIRST RUN)\n",
      "  Fetching 100 random Thai Wikipedia articles...\n",
      "\n",
      "Fetching 100 random Thai Wikipedia page IDs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching page IDs: 100%|██████████| 100/100 [00:33<00:00,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved 100 page IDs to ../data/1.8a_thai_wiki_page_ids.txt\n",
      "  Page ID range: [5449, 1489839]\n",
      "\n",
      "  Subsequent runs will use these same page IDs (reproducible)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SAMPLING STRATEGY\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "if Path(PAGE_IDS_FILE).exists():\n",
    "    print(\"✓ Found existing page IDs (REPRODUCIBLE MODE)\")\n",
    "    print(f\"  Loading from: {PAGE_IDS_FILE}\\n\")\n",
    "    \n",
    "    with open(PAGE_IDS_FILE, 'r') as f:\n",
    "        page_ids = [int(line.strip()) for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"✓ Loaded {len(page_ids)} page IDs\")\n",
    "    print(f\"  Page ID range: [{min(page_ids)}, {max(page_ids)}]\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ No existing page IDs found (FIRST RUN)\")\n",
    "    print(f\"  Fetching {NUM_ARTICLES} random Thai Wikipedia articles...\\n\")\n",
    "    \n",
    "    # Fetch random page IDs\n",
    "    page_ids = fetch_random_page_ids(NUM_ARTICLES, delay=REQUEST_DELAY)\n",
    "    \n",
    "    # Save for reproducibility\n",
    "    with open(PAGE_IDS_FILE, 'w') as f:\n",
    "        for page_id in page_ids:\n",
    "            f.write(f\"{page_id}\\n\")\n",
    "    \n",
    "    print(f\"\\n✓ Saved {len(page_ids)} page IDs to {PAGE_IDS_FILE}\")\n",
    "    print(f\"  Page ID range: [{min(page_ids)}, {max(page_ids)}]\")\n",
    "    print(f\"\\n  Subsequent runs will use these same page IDs (reproducible)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Articles by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FETCHING ARTICLES\n",
      "======================================================================\n",
      "\n",
      "Fetching 100 articles from Thai Wikipedia...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching articles: 100%|██████████| 100/100 [00:53<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully fetched 100 articles\n",
      "✓ All articles fetched successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FETCHING ARTICLES\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Fetching {len(page_ids)} articles from Thai Wikipedia...\\n\")\n",
    "\n",
    "articles = []\n",
    "failed = []\n",
    "\n",
    "for page_id in tqdm(page_ids, desc=\"Fetching articles\"):\n",
    "    try:\n",
    "        article = fetch_article_by_id(page_id, delay=REQUEST_DELAY)\n",
    "        save_article(article, OUTPUT_DIR)\n",
    "        articles.append(article)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠ Failed to fetch page {page_id}: {e}\")\n",
    "        failed.append(page_id)\n",
    "\n",
    "print(f\"\\n✓ Successfully fetched {len(articles)} articles\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"⚠ Failed to fetch {len(failed)} articles: {failed}\")\n",
    "else:\n",
    "    print(\"✓ All articles fetched successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CORPUS STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Corpus size: 100 articles\n",
      "\n",
      "Character counts:\n",
      "  Total: 62,243 characters\n",
      "  Average: 622 characters per article\n",
      "  Range: [41, 5,534]\n",
      "\n",
      "Sample of article titles (first 10):\n",
      "   1. โอลด์เฟิร์ม\n",
      "   2. ฤทธิ เบญจฤทธิ์\n",
      "   3. แม่น้ำอามูร์\n",
      "   4. อาทิศังกราจารย์\n",
      "   5. พระเจ้าเฮนรีที่ 7 แห่งอังกฤษ\n",
      "   6. พ.ศ. 421\n",
      "   7. ทางหลวงแผ่นดินหมายเลข 1356\n",
      "   8. อาสนวิหารการเสด็จขึ้นสู่สวรรค์ (อัลมาเตอ)\n",
      "   9. เก๋งนุกิจราชบริหาร\n",
      "  10. เร ดัง\n",
      "  ... (90 more)\n",
      "\n",
      "Article length distribution:\n",
      "     <1k:  82 articles ( 82.0%)\n",
      "    1-5k:  17 articles ( 17.0%)\n",
      "   5-10k:   1 articles (  1.0%)\n",
      "  10-50k:   0 articles (  0.0%)\n",
      "    >50k:   0 articles (  0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CORPUS STATISTICS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Count statistics\n",
    "total_chars = sum(a['char_count'] for a in articles)\n",
    "avg_chars = total_chars / len(articles) if articles else 0\n",
    "min_chars = min(a['char_count'] for a in articles) if articles else 0\n",
    "max_chars = max(a['char_count'] for a in articles) if articles else 0\n",
    "\n",
    "print(f\"Corpus size: {len(articles)} articles\")\n",
    "print(f\"\\nCharacter counts:\")\n",
    "print(f\"  Total: {total_chars:,} characters\")\n",
    "print(f\"  Average: {avg_chars:,.0f} characters per article\")\n",
    "print(f\"  Range: [{min_chars:,}, {max_chars:,}]\")\n",
    "\n",
    "# Show sample of titles\n",
    "print(f\"\\nSample of article titles (first 10):\")\n",
    "for i, article in enumerate(articles[:10], 1):\n",
    "    print(f\"  {i:2d}. {article['title']}\")\n",
    "\n",
    "if len(articles) > 10:\n",
    "    print(f\"  ... ({len(articles) - 10} more)\")\n",
    "\n",
    "# Character count distribution\n",
    "print(f\"\\nArticle length distribution:\")\n",
    "bins = [0, 1000, 5000, 10000, 50000, float('inf')]\n",
    "labels = ['<1k', '1-5k', '5-10k', '10-50k', '>50k']\n",
    "\n",
    "for i, (lower, upper) in enumerate(zip(bins[:-1], bins[1:])):\n",
    "    count = sum(1 for a in articles if lower <= a['char_count'] < upper)\n",
    "    pct = 100 * count / len(articles) if articles else 0\n",
    "    print(f\"  {labels[i]:>6s}: {count:3d} articles ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Corpus Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved corpus metadata to ../data/1.8a_thai_wiki_corpus/corpus_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Create corpus-level metadata\n",
    "metadata = {\n",
    "    'corpus_name': '1.8a Thai Wikipedia Sample',\n",
    "    'num_articles': len(articles),\n",
    "    'language': LANGUAGE,\n",
    "    'source': 'Thai Wikipedia (th.wikipedia.org)',\n",
    "    'sampling_method': 'Random article API',\n",
    "    'creation_date': datetime.now().isoformat(),\n",
    "    'total_characters': total_chars,\n",
    "    'page_ids_file': PAGE_IDS_FILE,\n",
    "    'articles': [\n",
    "        {\n",
    "            'page_id': a['page_id'],\n",
    "            'title': a['title'],\n",
    "            'char_count': a['char_count'],\n",
    "            'url': a['url']\n",
    "        }\n",
    "        for a in articles\n",
    "    ]\n",
    "}\n",
    "\n",
    "metadata_path = Path(OUTPUT_DIR) / 'corpus_metadata.json'\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Saved corpus metadata to {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has created a reproducible Thai Wikipedia corpus for testing cluster token usage.\n",
    "\n",
    "**What we've done:**\n",
    "\n",
    "1. Sampled 100 random Thai Wikipedia articles\n",
    "2. Saved page IDs for reproducibility\n",
    "3. Fetched and stored article text with metadata\n",
    "4. Created corpus statistics\n",
    "\n",
    "**Files created:**\n",
    "\n",
    "- `{PAGE_IDS_FILE}`: List of 100 page IDs (one per line)\n",
    "- `{OUTPUT_DIR}/*.json`: Individual article JSON files\n",
    "- `{OUTPUT_DIR}/corpus_metadata.json`: Corpus-level metadata\n",
    "\n",
    "**Reproducibility:**\n",
    "\n",
    "- First run: Fetches random articles (non-deterministic)\n",
    "- Subsequent runs: Re-fetches same articles by ID (deterministic)\n",
    "- Page IDs committed to git for transparency\n",
    "\n",
    "**Next step:**\n",
    "\n",
    "**1.8b**: Tokenize corpus with Qwen tokenizer and count cluster token appearances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
