{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 13.3e: What Are You, Stubborn Token?\n",
    "\n",
    "**Identity crisis of the one live token that refuses to leave the dead core.**\n",
    "\n",
    "From 13.3d, we know:\n",
    "- **SGD step 10000:** Main class has 50 dead + 50 live tokens (100 total)\n",
    "- **Adam step 10000:** Main class has 50 dead + **1 live token** (51 total)\n",
    "\n",
    "**Questions:**\n",
    "1. Which live token stays with the dead core in Adam?\n",
    "2. Which 50 live tokens stay in SGD's core?\n",
    "3. What's special about them? Low frequency? Rare characters?\n",
    "4. How many times do they appear in the Gatsby corpus?\n",
    "\n",
    "**Note:** Lil Gatsby uses pure ASCII (bytes 0-127), no fancy tokenizer. Each token ID = ASCII byte value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sources\n",
    "SGD_DATA_PATH = \"../tensors/Lil_Gatsby/13.1a_training_data.safetensors\"\n",
    "ADAM_DATA_PATH = \"../tensors/Lil_Gatsby/13.1b_training_data.safetensors\"\n",
    "\n",
    "# Gatsby corpus (to count byte occurrences)\n",
    "CORPUS_PATH = \"../data/the_great_gatsby.txt\"\n",
    "\n",
    "# Final step to analyze\n",
    "FINAL_STEP = 10000\n",
    "\n",
    "# Equivalence threshold\n",
    "EQUIVALENCE_THRESHOLD = 1.0\n",
    "\n",
    "# ASCII vocabulary\n",
    "VOCAB_SIZE = 128\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import safe_open\n",
    "from collections import deque, Counter\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Detect available device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SGD (13.1a)\n",
      "================================================================================\n",
      "Loading: ../tensors/Lil_Gatsby/13.1a_training_data.safetensors\n",
      "  Embeddings: torch.Size([10001, 128, 64])\n",
      "  Dead: 50, Live: 78\n",
      "\n",
      "================================================================================\n",
      "Adam (13.1b)\n",
      "================================================================================\n",
      "Loading: ../tensors/Lil_Gatsby/13.1b_training_data.safetensors\n",
      "  Embeddings: torch.Size([10001, 128, 64])\n",
      "  Dead: 50, Live: 78\n",
      "\n",
      "✓ Data loaded\n"
     ]
    }
   ],
   "source": [
    "def load_training_data(path):\n",
    "    \"\"\"Load embeddings and metadata from safetensors.\"\"\"\n",
    "    print(f\"Loading: {path}\")\n",
    "    with safe_open(path, framework='pt', device='cpu') as f:\n",
    "        embeddings_bf16 = f.get_tensor('embeddings')\n",
    "        dead_token_ids = f.get_tensor('dead_token_ids')\n",
    "        live_token_ids = f.get_tensor('live_token_ids')\n",
    "        recorded_steps = f.get_tensor('recorded_steps')\n",
    "    \n",
    "    print(f\"  Embeddings: {embeddings_bf16.shape}\")\n",
    "    print(f\"  Dead: {len(dead_token_ids)}, Live: {len(live_token_ids)}\")\n",
    "    \n",
    "    return embeddings_bf16, dead_token_ids, live_token_ids, recorded_steps\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SGD (13.1a)\")\n",
    "print(\"=\" * 80)\n",
    "sgd_emb, sgd_dead, sgd_live, sgd_steps = load_training_data(SGD_DATA_PATH)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Adam (13.1b)\")\n",
    "print(\"=\" * 80)\n",
    "adam_emb, adam_dead, adam_live, adam_steps = load_training_data(ADAM_DATA_PATH)\n",
    "\n",
    "print(\"\\n✓ Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Find Main Class at Step 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing step 10000 (SGD)\n",
      "Analyzing step 10000 (Adam)\n",
      "\n",
      "Computing pairwise cosine similarity...\n",
      "Finding equivalence classes...\n",
      "\n",
      "SGD: 5 classes, main size = 100\n",
      "Adam: 77 classes, main size = 51\n",
      "\n",
      "✓ Classes identified\n"
     ]
    }
   ],
   "source": [
    "def find_equivalence_classes_bfs(cos_sim_matrix, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Find connected components in the equivalence graph.\n",
    "    \n",
    "    Returns:\n",
    "        classes: list of lists (token IDs in each class)\n",
    "    \"\"\"\n",
    "    n = cos_sim_matrix.shape[0]\n",
    "    adjacency = cos_sim_matrix >= threshold\n",
    "    \n",
    "    visited = torch.zeros(n, dtype=torch.bool)\n",
    "    classes = []\n",
    "    \n",
    "    for start in range(n):\n",
    "        if visited[start]:\n",
    "            continue\n",
    "        \n",
    "        component = []\n",
    "        queue = deque([start])\n",
    "        visited[start] = True\n",
    "        \n",
    "        while queue:\n",
    "            node = queue.popleft()\n",
    "            component.append(node)\n",
    "            \n",
    "            neighbors = torch.where(adjacency[node])[0]\n",
    "            for neighbor in neighbors:\n",
    "                if not visited[neighbor]:\n",
    "                    visited[neighbor] = True\n",
    "                    queue.append(neighbor.item())\n",
    "        \n",
    "        classes.append(component)\n",
    "    \n",
    "    # Sort by size (largest first)\n",
    "    classes.sort(key=len, reverse=True)\n",
    "    \n",
    "    return classes\n",
    "\n",
    "def compute_pairwise_cosine_similarity(embeddings_step, device):\n",
    "    \"\"\"\n",
    "    Compute pairwise cosine similarity for one step.\n",
    "    \"\"\"\n",
    "    emb_f32 = embeddings_step.to(torch.float32).to(device)\n",
    "    emb_norm = emb_f32 / emb_f32.norm(p=2, dim=1, keepdim=True)\n",
    "    cos_sim = emb_norm @ emb_norm.T\n",
    "    \n",
    "    # Round to BF16 and back\n",
    "    cos_sim_bf16 = cos_sim.to(torch.bfloat16).to(torch.float32).cpu()\n",
    "    \n",
    "    return cos_sim_bf16\n",
    "\n",
    "# Get final step index\n",
    "sgd_final_idx = (sgd_steps - FINAL_STEP).abs().argmin().item()\n",
    "adam_final_idx = (adam_steps - FINAL_STEP).abs().argmin().item()\n",
    "\n",
    "print(f\"Analyzing step {sgd_steps[sgd_final_idx].item()} (SGD)\")\n",
    "print(f\"Analyzing step {adam_steps[adam_final_idx].item()} (Adam)\\n\")\n",
    "\n",
    "# Compute pairwise cosine similarity at final step\n",
    "print(\"Computing pairwise cosine similarity...\")\n",
    "sgd_cos_sim_final = compute_pairwise_cosine_similarity(sgd_emb[sgd_final_idx], device)\n",
    "adam_cos_sim_final = compute_pairwise_cosine_similarity(adam_emb[adam_final_idx], device)\n",
    "\n",
    "# Find equivalence classes\n",
    "print(\"Finding equivalence classes...\")\n",
    "sgd_classes = find_equivalence_classes_bfs(sgd_cos_sim_final, threshold=EQUIVALENCE_THRESHOLD)\n",
    "adam_classes = find_equivalence_classes_bfs(adam_cos_sim_final, threshold=EQUIVALENCE_THRESHOLD)\n",
    "\n",
    "print(f\"\\nSGD: {len(sgd_classes)} classes, main size = {len(sgd_classes[0])}\")\n",
    "print(f\"Adam: {len(adam_classes)} classes, main size = {len(adam_classes[0])}\")\n",
    "\n",
    "print(\"\\n✓ Classes identified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Identify Live Tokens in Main Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SGD MAIN CLASS (Step 10000)\n",
      "================================================================================\n",
      "Total: 100 tokens\n",
      "  Dead: 50\n",
      "  Live: 50\n",
      "\n",
      "Live token IDs in main class (first 20): [13, 33, 36, 40, 41, 42, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 65]\n",
      "  ... (30 more)\n",
      "\n",
      "================================================================================\n",
      "ADAM MAIN CLASS (Step 10000)\n",
      "================================================================================\n",
      "Total: 51 tokens\n",
      "  Dead: 50\n",
      "  Live: 1\n",
      "\n",
      "Live token ID(s) in main class: [13]\n"
     ]
    }
   ],
   "source": [
    "# Convert to sets for fast lookup\n",
    "sgd_dead_set = set(sgd_dead.tolist())\n",
    "sgd_live_set = set(sgd_live.tolist())\n",
    "adam_dead_set = set(adam_dead.tolist())\n",
    "adam_live_set = set(adam_live.tolist())\n",
    "\n",
    "# SGD main class\n",
    "sgd_main = sgd_classes[0]\n",
    "sgd_main_live = sorted([tid for tid in sgd_main if tid in sgd_live_set])\n",
    "sgd_main_dead = sorted([tid for tid in sgd_main if tid in sgd_dead_set])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SGD MAIN CLASS (Step 10000)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total: {len(sgd_main)} tokens\")\n",
    "print(f\"  Dead: {len(sgd_main_dead)}\")\n",
    "print(f\"  Live: {len(sgd_main_live)}\")\n",
    "print(f\"\\nLive token IDs in main class (first 20): {sgd_main_live[:20]}\")\n",
    "if len(sgd_main_live) > 20:\n",
    "    print(f\"  ... ({len(sgd_main_live) - 20} more)\")\n",
    "\n",
    "# Adam main class\n",
    "adam_main = adam_classes[0]\n",
    "adam_main_live = sorted([tid for tid in adam_main if tid in adam_live_set])\n",
    "adam_main_dead = sorted([tid for tid in adam_main if tid in adam_dead_set])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ADAM MAIN CLASS (Step 10000)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total: {len(adam_main)} tokens\")\n",
    "print(f\"  Dead: {len(adam_main_dead)}\")\n",
    "print(f\"  Live: {len(adam_main_live)}\")\n",
    "print(f\"\\nLive token ID(s) in main class: {adam_main_live}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Decode ASCII Byte IDs to Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SGD LIVE TOKENS IN MAIN CLASS (Decoded)\n",
      "================================================================================\n",
      "  Byte  13 (0x0d):   \\x0d  [CR]\n",
      "  Byte  33 (0x21):    '!'  [punctuation]\n",
      "  Byte  36 (0x24):    '$'  [punctuation]\n",
      "  Byte  40 (0x28):    '('  [punctuation]\n",
      "  Byte  41 (0x29):    ')'  [punctuation]\n",
      "  Byte  42 (0x2a):    '*'  [punctuation]\n",
      "  Byte  48 (0x30):    '0'  [digit]\n",
      "  Byte  49 (0x31):    '1'  [digit]\n",
      "  Byte  50 (0x32):    '2'  [digit]\n",
      "  Byte  51 (0x33):    '3'  [digit]\n",
      "  Byte  52 (0x34):    '4'  [digit]\n",
      "  Byte  53 (0x35):    '5'  [digit]\n",
      "  Byte  54 (0x36):    '6'  [digit]\n",
      "  Byte  55 (0x37):    '7'  [digit]\n",
      "  Byte  56 (0x38):    '8'  [digit]\n",
      "  Byte  57 (0x39):    '9'  [digit]\n",
      "  Byte  58 (0x3a):    ':'  [punctuation]\n",
      "  Byte  59 (0x3b):    ';'  [punctuation]\n",
      "  Byte  63 (0x3f):    '?'  [punctuation]\n",
      "  Byte  65 (0x41):    'A'  [letter]\n",
      "  Byte  66 (0x42):    'B'  [letter]\n",
      "  Byte  67 (0x43):    'C'  [letter]\n",
      "  Byte  68 (0x44):    'D'  [letter]\n",
      "  Byte  69 (0x45):    'E'  [letter]\n",
      "  Byte  70 (0x46):    'F'  [letter]\n",
      "  Byte  71 (0x47):    'G'  [letter]\n",
      "  Byte  72 (0x48):    'H'  [letter]\n",
      "  Byte  74 (0x4a):    'J'  [letter]\n",
      "  Byte  75 (0x4b):    'K'  [letter]\n",
      "  Byte  76 (0x4c):    'L'  [letter]\n",
      "  ... (20 more)\n",
      "\n",
      "================================================================================\n",
      "ADAM LIVE TOKEN IN MAIN CLASS (Decoded)\n",
      "================================================================================\n",
      "  Byte  13 (0x0d):   \\x0d  [CR]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_ascii_byte(byte_id):\n",
    "    \"\"\"\n",
    "    Decode ASCII byte ID to human-readable string.\n",
    "    \n",
    "    Returns:\n",
    "        (byte_id, char_repr, description)\n",
    "    \"\"\"\n",
    "    if byte_id < 0 or byte_id >= 128:\n",
    "        return (byte_id, \"<invalid>\", \"Out of ASCII range\")\n",
    "    \n",
    "    char = chr(byte_id)\n",
    "    \n",
    "    # Representation\n",
    "    if byte_id < 32:\n",
    "        # Control characters\n",
    "        char_repr = f\"\\\\x{byte_id:02x}\"\n",
    "        control_names = {\n",
    "            0: \"NUL\", 9: \"TAB\", 10: \"LF\", 13: \"CR\",\n",
    "        }\n",
    "        desc = control_names.get(byte_id, \"control\")\n",
    "    elif byte_id == 32:\n",
    "        char_repr = \"' '\"\n",
    "        desc = \"space\"\n",
    "    elif byte_id == 127:\n",
    "        char_repr = \"\\\\x7f\"\n",
    "        desc = \"DEL\"\n",
    "    else:\n",
    "        # Printable ASCII\n",
    "        char_repr = f\"'{char}'\"\n",
    "        if char.isdigit():\n",
    "            desc = \"digit\"\n",
    "        elif char.isalpha():\n",
    "            desc = \"letter\"\n",
    "        else:\n",
    "            desc = \"punctuation\"\n",
    "    \n",
    "    return (byte_id, char_repr, desc)\n",
    "\n",
    "def print_ascii_tokens(token_ids, title):\n",
    "    \"\"\"\n",
    "    Print decoded ASCII token table.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(title)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for tid in token_ids[:30]:  # Show first 30\n",
    "        byte_id, char_repr, desc = decode_ascii_byte(tid)\n",
    "        print(f\"  Byte {byte_id:3d} (0x{byte_id:02x}): {char_repr:>6s}  [{desc}]\")\n",
    "    \n",
    "    if len(token_ids) > 30:\n",
    "        print(f\"  ... ({len(token_ids) - 30} more)\")\n",
    "    print()\n",
    "\n",
    "# SGD live tokens in main class\n",
    "print_ascii_tokens(sgd_main_live, \"SGD LIVE TOKENS IN MAIN CLASS (Decoded)\")\n",
    "\n",
    "# Adam live token in main class\n",
    "print_ascii_tokens(adam_main_live, \"ADAM LIVE TOKEN IN MAIN CLASS (Decoded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Count Byte Occurrences in Gatsby Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus: ../data/the_great_gatsby.txt\n",
      "  Corpus length: 265905 characters\n",
      "  Byte count: 265905\n",
      "\n",
      "✓ Corpus loaded and counted\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "with open(CORPUS_PATH, 'r', encoding='ascii') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "print(f\"  Corpus length: {len(corpus_text)} characters\")\n",
    "\n",
    "# Convert to byte array\n",
    "corpus_bytes = corpus_text.encode('ascii')\n",
    "print(f\"  Byte count: {len(corpus_bytes)}\")\n",
    "\n",
    "# Count occurrences\n",
    "byte_counts = Counter(corpus_bytes)\n",
    "\n",
    "print(\"\\n✓ Corpus loaded and counted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SGD Main Class Live Bytes - FREQUENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total bytes: 50\n",
      "Bytes with 0 occurrences: 1\n",
      "Bytes with 1 occurrence: 1\n",
      "Bytes with 2-10 occurrences: 16\n",
      "Bytes with >10 occurrences: 32\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Rarest bytes (sorted by frequency):\n",
      "--------------------------------------------------------------------------------\n",
      "  Byte  13 (  \\x0d,           CR):      0 occurrences\n",
      "  Byte  90 (   'Z',       letter):      1 occurrences\n",
      "  Byte  36 (   '$',  punctuation):      2 occurrences\n",
      "  Byte  52 (   '4',        digit):      2 occurrences\n",
      "  Byte  55 (   '7',        digit):      2 occurrences\n",
      "  Byte  88 (   'X',       letter):      2 occurrences\n",
      "  Byte  91 (   '[',  punctuation):      2 occurrences\n",
      "  Byte  93 (   ']',  punctuation):      2 occurrences\n",
      "  Byte  56 (   '8',        digit):      3 occurrences\n",
      "  Byte  50 (   '2',        digit):      4 occurrences\n",
      "  Byte  81 (   'Q',       letter):      4 occurrences\n",
      "  Byte  54 (   '6',        digit):      5 occurrences\n",
      "  Byte  42 (   '*',  punctuation):      6 occurrences\n",
      "  Byte  40 (   '(',  punctuation):      7 occurrences\n",
      "  Byte  41 (   ')',  punctuation):      7 occurrences\n",
      "  Byte  51 (   '3',        digit):      7 occurrences\n",
      "  Byte  53 (   '5',        digit):      9 occurrences\n",
      "  Byte  85 (   'U',       letter):      9 occurrences\n",
      "  Byte  57 (   '9',        digit):     11 occurrences\n",
      "  Byte  49 (   '1',        digit):     16 occurrences\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Most common bytes in this group:\n",
      "--------------------------------------------------------------------------------\n",
      "  Byte 120 (   'x',       letter):    288 occurrences\n",
      "  Byte  68 (   'D',       letter):    307 occurrences\n",
      "  Byte  77 (   'M',       letter):    314 occurrences\n",
      "  Byte  65 (   'A',       letter):    320 occurrences\n",
      "  Byte  83 (   'S',       letter):    324 occurrences\n",
      "  Byte  63 (   '?',  punctuation):    328 occurrences\n",
      "  Byte  71 (   'G',       letter):    376 occurrences\n",
      "  Byte  72 (   'H',       letter):    421 occurrences\n",
      "  Byte  87 (   'W',       letter):    498 occurrences\n",
      "  Byte  84 (   'T',       letter):    690 occurrences\n",
      "\n",
      "================================================================================\n",
      "Adam Main Class Live Byte - FREQUENCY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total bytes: 1\n",
      "Bytes with 0 occurrences: 1\n",
      "Bytes with 1 occurrence: 0\n",
      "Bytes with 2-10 occurrences: 0\n",
      "Bytes with >10 occurrences: 0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Rarest bytes (sorted by frequency):\n",
      "--------------------------------------------------------------------------------\n",
      "  Byte  13 (  \\x0d,           CR):      0 occurrences\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_byte_frequencies(byte_ids, byte_counts, name):\n",
    "    \"\"\"\n",
    "    Print frequency statistics for a list of byte IDs.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{name} - FREQUENCY ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    freq_data = []\n",
    "    for byte_id in byte_ids:\n",
    "        count = byte_counts.get(byte_id, 0)\n",
    "        _, char_repr, desc = decode_ascii_byte(byte_id)\n",
    "        freq_data.append((byte_id, char_repr, desc, count))\n",
    "    \n",
    "    # Sort by frequency (ascending)\n",
    "    freq_data.sort(key=lambda x: x[3])\n",
    "    \n",
    "    print(f\"\\nTotal bytes: {len(freq_data)}\")\n",
    "    print(f\"Bytes with 0 occurrences: {sum(1 for _, _, _, c in freq_data if c == 0)}\")\n",
    "    print(f\"Bytes with 1 occurrence: {sum(1 for _, _, _, c in freq_data if c == 1)}\")\n",
    "    print(f\"Bytes with 2-10 occurrences: {sum(1 for _, _, _, c in freq_data if 2 <= c <= 10)}\")\n",
    "    print(f\"Bytes with >10 occurrences: {sum(1 for _, _, _, c in freq_data if c > 10)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Rarest bytes (sorted by frequency):\")\n",
    "    print(\"-\" * 80)\n",
    "    for byte_id, char_repr, desc, count in freq_data[:20]:\n",
    "        print(f\"  Byte {byte_id:3d} ({char_repr:>6s}, {desc:>12s}): {count:6,} occurrences\")\n",
    "    \n",
    "    if len(freq_data) > 20:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"Most common bytes in this group:\")\n",
    "        print(\"-\" * 80)\n",
    "        for byte_id, char_repr, desc, count in freq_data[-10:]:\n",
    "            print(f\"  Byte {byte_id:3d} ({char_repr:>6s}, {desc:>12s}): {count:6,} occurrences\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Analyze SGD live bytes in main class\n",
    "analyze_byte_frequencies(sgd_main_live, byte_counts, \"SGD Main Class Live Bytes\")\n",
    "\n",
    "# Analyze Adam's stubborn byte\n",
    "analyze_byte_frequencies(adam_main_live, byte_counts, \"Adam Main Class Live Byte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## The Stubborn Token: Special Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "THE STUBBORN TOKEN\n",
      "================================================================================\n",
      "\n",
      "Byte ID: 13 (0x0d)\n",
      "Character: \\x0d\n",
      "Type: CR\n",
      "Occurrences in Gatsby: 0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Context: How does this compare to the dead core?\n",
      "--------------------------------------------------------------------------------\n",
      "Dead tokens in main class: 50\n",
      "Dead token occurrence range in Gatsby: 0 (by definition)\n",
      "\n",
      "The stubborn token has 0 occurrence(s).\n",
      "  → This token is EFFECTIVELY DEAD (zero gradients)!\n",
      "  → Must have been miscategorized as 'live' somehow.\n"
     ]
    }
   ],
   "source": [
    "if len(adam_main_live) == 1:\n",
    "    stubborn_id = adam_main_live[0]\n",
    "    stubborn_byte_id, stubborn_char, stubborn_desc = decode_ascii_byte(stubborn_id)\n",
    "    stubborn_count = byte_counts.get(stubborn_id, 0)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"THE STUBBORN TOKEN\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nByte ID: {stubborn_id} (0x{stubborn_id:02x})\")\n",
    "    print(f\"Character: {stubborn_char}\")\n",
    "    print(f\"Type: {stubborn_desc}\")\n",
    "    print(f\"Occurrences in Gatsby: {stubborn_count:,}\")\n",
    "    \n",
    "    # Compare to dead tokens\n",
    "    print(f\"\\n\" + \"-\" * 80)\n",
    "    print(\"Context: How does this compare to the dead core?\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Dead tokens in main class: {len(adam_main_dead)}\")\n",
    "    print(f\"Dead token occurrence range in Gatsby: 0 (by definition)\")\n",
    "    print(f\"\\nThe stubborn token has {stubborn_count:,} occurrence(s).\")\n",
    "    \n",
    "    if stubborn_count == 0:\n",
    "        print(\"  → This token is EFFECTIVELY DEAD (zero gradients)!\")\n",
    "        print(\"  → Must have been miscategorized as 'live' somehow.\")\n",
    "    elif stubborn_count == 1:\n",
    "        print(\"  → Near-dead: appears exactly once in the entire corpus.\")\n",
    "        print(\"  → Gradient updates are negligible.\")\n",
    "    elif stubborn_count <= 10:\n",
    "        print(\"  → Very rare: appears only a handful of times.\")\n",
    "        print(\"  → Gradient signal is weak but present.\")\n",
    "    else:\n",
    "        print(f\"  → Surprisingly common (top {100 * (1 - stubborn_count / len(corpus_bytes)):.1f}% of corpus bytes)\")\n",
    "        print(\"  → This is weird—strong gradient signal but still frozen!\")\n",
    "    \n",
    "    # Show a few examples from corpus if count is small\n",
    "    if 0 < stubborn_count <= 10:\n",
    "        print(f\"\\n\" + \"-\" * 80)\n",
    "        print(f\"Occurrences in corpus (showing all {stubborn_count}):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Find positions\n",
    "        positions = [i for i, b in enumerate(corpus_bytes) if b == stubborn_id]\n",
    "        \n",
    "        for i, pos in enumerate(positions, 1):\n",
    "            # Extract context (±40 chars)\n",
    "            start = max(0, pos - 40)\n",
    "            end = min(len(corpus_text), pos + 40)\n",
    "            context = corpus_text[start:end]\n",
    "            \n",
    "            # Highlight the character\n",
    "            rel_pos = pos - start\n",
    "            highlighted = context[:rel_pos] + f\"[{context[rel_pos]}]\" + context[rel_pos+1:]\n",
    "            \n",
    "            print(f\"\\n  Occurrence {i} at position {pos}:\")\n",
    "            print(f\"    ...{highlighted}...\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nExpected 1 stubborn token in Adam main class, found {len(adam_main_live)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Stubborn token investigation complete\n"
     ]
    }
   ],
   "source": [
    "print(\"✓ Stubborn token investigation complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
