{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.10c: GB2312 Chinese Character Tokenization Test\n",
    "\n",
    "**Testing the legacy encoding hypothesis**\n",
    "\n",
    "## The Hypothesis\n",
    "\n",
    "From 1.10b, we found that **random bytes produce halo tokens at 23.35× enrichment**.\n",
    "\n",
    "This suggests halo tokens appear in Qwen's training data as **non-UTF-8 byte sequences**.\n",
    "\n",
    "**New question:** Did Qwen train on pre-Unicode Chinese text?\n",
    "\n",
    "## GB2312 Encoding\n",
    "\n",
    "GB2312 (1980) is a legacy Chinese character encoding used before Unicode:\n",
    "- 2 bytes per character\n",
    "- ~6,700 Simplified Chinese characters\n",
    "- High byte: 0xA1–0xF7 (area code)\n",
    "- Low byte: 0xA1–0xFE (position code)\n",
    "\n",
    "**Character areas:**\n",
    "- Areas 1-9 (0xA1A1–0xA9FE): Symbols, punctuation, kana, Greek, Cyrillic\n",
    "- Areas 16-55 (0xB0A1–0xD7FE): Level 1 Chinese (3,755 common characters)\n",
    "- Areas 56-87 (0xD8A1–0xF7FE): Level 2 Chinese (3,008 less common characters)\n",
    "\n",
    "## The Test\n",
    "\n",
    "Feed the tokenizer **all valid GB2312 Chinese character byte pairs** and measure halo token enrichment.\n",
    "\n",
    "**If Qwen trained on GB2312 text:**\n",
    "- Halo enrichment should be **higher** than random bytes (> 23.35x)\n",
    "- GB2312 byte pairs specifically target the byte sequences that would appear in legacy Chinese text\n",
    "- This is a **targeted test** vs. random byte noise\n",
    "\n",
    "**If Qwen never saw GB2312:**\n",
    "- Enrichment similar to random bytes (~23x) or lower\n",
    "- No special affinity for these specific byte patterns\n",
    "\n",
    "## Method\n",
    "\n",
    "1. Generate all valid GB2312 Chinese character byte pairs (0xB0A1–0xF7FE)\n",
    "2. Convert to strings via Latin-1 (lossless byte→string)\n",
    "3. Tokenize each byte pair individually\n",
    "4. Classify tokens: cluster / halo / bulk\n",
    "5. Compare to vocabulary baseline and random bytes baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\"\n",
    "HF_MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# Input data\n",
    "CLUSTER_TOKENS_PATH = \"../tensors/Qwen3-4B-Instruct-2507/1.4h_cluster_tokens.safetensors\"\n",
    "REACHABILITY_PATH = \"../tensors/Qwen3-4B-Instruct-2507/1.8d_full_vocab_reachability.safetensors\"\n",
    "\n",
    "# GB2312 Chinese character ranges\n",
    "# Level 1: Common characters (areas 16-55)\n",
    "LEVEL1_HIGH_START = 0xB0\n",
    "LEVEL1_HIGH_END = 0xD7\n",
    "\n",
    "# Level 2: Less common characters (areas 56-87)\n",
    "LEVEL2_HIGH_START = 0xD8\n",
    "LEVEL2_HIGH_END = 0xF7\n",
    "\n",
    "# Low byte range (same for both levels)\n",
    "LOW_BYTE_START = 0xA1\n",
    "LOW_BYTE_END = 0xFE\n",
    "\n",
    "# Baseline from 1.10b (random bytes)\n",
    "RANDOM_BYTES_HALO_ENRICHMENT = 23.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from safetensors.torch import load_file\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Token Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading token classifications...\n",
      "\n",
      "✓ Loaded token classifications\n",
      "  Cluster tokens: 2,212\n",
      "  Halo tokens: 1,423\n",
      "  Bulk tokens: 148,034\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading token classifications...\\n\")\n",
    "\n",
    "# Load cluster tokens\n",
    "cluster_data = load_file(CLUSTER_TOKENS_PATH)\n",
    "cluster_token_ids = set(cluster_data['cluster_token_ids'].tolist())\n",
    "\n",
    "# Load halo tokens (unreachable outside cluster)\n",
    "reachability_data = load_file(REACHABILITY_PATH)\n",
    "halo_token_ids = set(reachability_data['unreachable_outside_cluster'].tolist())\n",
    "\n",
    "print(f\"✓ Loaded token classifications\")\n",
    "print(f\"  Cluster tokens: {len(cluster_token_ids):,}\")\n",
    "print(f\"  Halo tokens: {len(halo_token_ids):,}\")\n",
    "print(f\"  Bulk tokens: {151669 - len(cluster_token_ids) - len(halo_token_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer: Qwen/Qwen3-4B-Instruct-2507\n",
      "\n",
      "✓ Tokenizer loaded\n",
      "  Vocabulary size: 151,669 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading tokenizer: {HF_MODEL_NAME}\\n\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "print(f\"✓ Tokenizer loaded\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate GB2312 Byte Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GENERATING GB2312 CHINESE CHARACTER BYTE PAIRS\n",
      "======================================================================\n",
      "\n",
      "✓ Generated 6,768 GB2312 byte pairs\n",
      "\n",
      "Byte range coverage:\n",
      "  Level 1 (0xB0A1-0xD7FE): 3,760 pairs\n",
      "  Level 2 (0xD8A1-0xF7FE): 3,008 pairs\n",
      "  Total: 6,768 pairs\n",
      "\n",
      "First 5 byte pairs (hex):\n",
      "  1. 0xB0A1\n",
      "  2. 0xB0A2\n",
      "  3. 0xB0A3\n",
      "  4. 0xB0A4\n",
      "  5. 0xB0A5\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GENERATING GB2312 CHINESE CHARACTER BYTE PAIRS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Generate all valid GB2312 Chinese character byte pairs\n",
    "gb2312_byte_pairs = []\n",
    "\n",
    "# Level 1: Common characters (0xB0A1 - 0xD7FE)\n",
    "for high_byte in range(LEVEL1_HIGH_START, LEVEL1_HIGH_END + 1):\n",
    "    for low_byte in range(LOW_BYTE_START, LOW_BYTE_END + 1):\n",
    "        gb2312_byte_pairs.append(bytes([high_byte, low_byte]))\n",
    "\n",
    "# Level 2: Less common characters (0xD8A1 - 0xF7FE)\n",
    "for high_byte in range(LEVEL2_HIGH_START, LEVEL2_HIGH_END + 1):\n",
    "    for low_byte in range(LOW_BYTE_START, LOW_BYTE_END + 1):\n",
    "        gb2312_byte_pairs.append(bytes([high_byte, low_byte]))\n",
    "\n",
    "print(f\"✓ Generated {len(gb2312_byte_pairs):,} GB2312 byte pairs\")\n",
    "print(f\"\\nByte range coverage:\")\n",
    "level1_count = (LEVEL1_HIGH_END - LEVEL1_HIGH_START + 1) * (LOW_BYTE_END - LOW_BYTE_START + 1)\n",
    "level2_count = (LEVEL2_HIGH_END - LEVEL2_HIGH_START + 1) * (LOW_BYTE_END - LOW_BYTE_START + 1)\n",
    "print(f\"  Level 1 (0xB0A1-0xD7FE): {level1_count:,} pairs\")\n",
    "print(f\"  Level 2 (0xD8A1-0xF7FE): {level2_count:,} pairs\")\n",
    "print(f\"  Total: {len(gb2312_byte_pairs):,} pairs\")\n",
    "\n",
    "# Show first few examples\n",
    "print(f\"\\nFirst 5 byte pairs (hex):\")\n",
    "for i in range(min(5, len(gb2312_byte_pairs))):\n",
    "    pair = gb2312_byte_pairs[i]\n",
    "    print(f\"  {i+1}. 0x{pair[0]:02X}{pair[1]:02X}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize GB2312 Byte Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TOKENIZING GB2312 BYTE PAIRS\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing GB2312 pairs: 100%|██████████| 6768/6768 [00:00<00:00, 20989.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Warning: Failed to tokenize 0xD7FA: 'gb2312' codec can't decode byte 0xd7 in position 0: illegal multibyte sequence\n",
      "  Warning: Failed to tokenize 0xD7FB: 'gb2312' codec can't decode byte 0xd7 in position 0: illegal multibyte sequence\n",
      "  Warning: Failed to tokenize 0xD7FC: 'gb2312' codec can't decode byte 0xd7 in position 0: illegal multibyte sequence\n",
      "  Warning: Failed to tokenize 0xD7FD: 'gb2312' codec can't decode byte 0xd7 in position 0: illegal multibyte sequence\n",
      "  Warning: Failed to tokenize 0xD7FE: 'gb2312' codec can't decode byte 0xd7 in position 0: illegal multibyte sequence\n",
      "\n",
      "✓ Tokenization complete\n",
      "  Byte pairs processed: 6,763 / 6,768\n",
      "  Failed pairs: 5\n",
      "  Total tokens produced: 6,885\n",
      "  Unique tokens: 6,789\n",
      "  Tokens per byte pair (avg): 1.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TOKENIZING GB2312 BYTE PAIRS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Collect all token IDs produced\n",
    "all_tokens = []\n",
    "failed_pairs = 0\n",
    "\n",
    "for byte_pair in tqdm(gb2312_byte_pairs, desc=\"Tokenizing GB2312 pairs\"):\n",
    "    try:\n",
    "        # Convert bytes to string using GB2312 encoding\n",
    "        byte_string = byte_pair.decode('gb2312')\n",
    "        \n",
    "        # Tokenize\n",
    "        token_ids = tokenizer.encode(byte_string, add_special_tokens=False)\n",
    "        all_tokens.extend(token_ids)\n",
    "    except Exception as e:\n",
    "        failed_pairs += 1\n",
    "        if failed_pairs <= 5:\n",
    "            print(f\"  Warning: Failed to tokenize 0x{byte_pair[0]:02X}{byte_pair[1]:02X}: {e}\")\n",
    "\n",
    "print(f\"\\n✓ Tokenization complete\")\n",
    "print(f\"  Byte pairs processed: {len(gb2312_byte_pairs) - failed_pairs:,} / {len(gb2312_byte_pairs):,}\")\n",
    "print(f\"  Failed pairs: {failed_pairs:,}\")\n",
    "print(f\"  Total tokens produced: {len(all_tokens):,}\")\n",
    "print(f\"  Unique tokens: {len(set(all_tokens)):,}\")\n",
    "print(f\"  Tokens per byte pair (avg): {len(all_tokens) / len(gb2312_byte_pairs):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLASSIFYING TOKENS\n",
      "======================================================================\n",
      "\n",
      "Token distribution from GB2312 byte pairs:\n",
      "  Cluster tokens: 0 (0.00%)\n",
      "  Halo tokens: 244 (3.54%)\n",
      "  Bulk tokens: 6,641 (96.46%)\n",
      "  Total: 6,885\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLASSIFYING TOKENS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Count tokens by category\n",
    "cluster_count = 0\n",
    "halo_count = 0\n",
    "bulk_count = 0\n",
    "\n",
    "for token_id in all_tokens:\n",
    "    if token_id in cluster_token_ids:\n",
    "        cluster_count += 1\n",
    "    elif token_id in halo_token_ids:\n",
    "        halo_count += 1\n",
    "    else:\n",
    "        bulk_count += 1\n",
    "\n",
    "total_tokens = len(all_tokens)\n",
    "\n",
    "if total_tokens == 0:\n",
    "    print(\"ERROR: No tokens were produced!\")\n",
    "    print(\"Cannot compute statistics with zero tokens.\")\n",
    "else:\n",
    "    print(f\"Token distribution from GB2312 byte pairs:\")\n",
    "    print(f\"  Cluster tokens: {cluster_count:,} ({100*cluster_count/total_tokens:.2f}%)\")\n",
    "    print(f\"  Halo tokens: {halo_count:,} ({100*halo_count/total_tokens:.2f}%)\")\n",
    "    print(f\"  Bulk tokens: {bulk_count:,} ({100*bulk_count/total_tokens:.2f}%)\")\n",
    "    print(f\"  Total: {total_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPARISON TO BASELINES\n",
      "======================================================================\n",
      "\n",
      "Baseline 1: Vocabulary (uniform sampling):\n",
      "  Cluster: 1.46%\n",
      "  Halo: 0.94%\n",
      "  Bulk: 97.60%\n",
      "\n",
      "Baseline 2: Random bytes (from 1.10b):\n",
      "  Halo enrichment: 23.35x\n",
      "\n",
      "Observed (GB2312 byte pairs):\n",
      "  Cluster: 0.00%\n",
      "  Halo: 3.54%\n",
      "  Bulk: 96.46%\n",
      "\n",
      "Enrichment vs. vocabulary:\n",
      "  Cluster: 0.00x\n",
      "  Halo: 3.78x ← LOWER THAN RANDOM BYTES (23.35x)\n",
      "\n",
      "  ✗ GB2312 byte pairs produce FEWER halo tokens than random noise\n",
      "    Qwen likely did not train on GB2312 text.\n",
      "  Bulk: 0.99x\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARISON TO BASELINES\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Vocabulary proportions (uniform sampling baseline)\n",
    "vocab_cluster_pct = 100 * len(cluster_token_ids) / vocab_size\n",
    "vocab_halo_pct = 100 * len(halo_token_ids) / vocab_size\n",
    "vocab_bulk_pct = 100 * (vocab_size - len(cluster_token_ids) - len(halo_token_ids)) / vocab_size\n",
    "\n",
    "# Observed proportions from GB2312\n",
    "obs_cluster_pct = 100 * cluster_count / total_tokens\n",
    "obs_halo_pct = 100 * halo_count / total_tokens\n",
    "obs_bulk_pct = 100 * bulk_count / total_tokens\n",
    "\n",
    "# Enrichment (observed / expected)\n",
    "cluster_enrichment = obs_cluster_pct / vocab_cluster_pct if vocab_cluster_pct > 0 else 0\n",
    "halo_enrichment = obs_halo_pct / vocab_halo_pct if vocab_halo_pct > 0 else 0\n",
    "bulk_enrichment = obs_bulk_pct / vocab_bulk_pct if vocab_bulk_pct > 0 else 0\n",
    "\n",
    "print(\"Baseline 1: Vocabulary (uniform sampling):\")\n",
    "print(f\"  Cluster: {vocab_cluster_pct:.2f}%\")\n",
    "print(f\"  Halo: {vocab_halo_pct:.2f}%\")\n",
    "print(f\"  Bulk: {vocab_bulk_pct:.2f}%\")\n",
    "\n",
    "print(f\"\\nBaseline 2: Random bytes (from 1.10b):\")\n",
    "print(f\"  Halo enrichment: {RANDOM_BYTES_HALO_ENRICHMENT:.2f}x\")\n",
    "\n",
    "print(f\"\\nObserved (GB2312 byte pairs):\")\n",
    "print(f\"  Cluster: {obs_cluster_pct:.2f}%\")\n",
    "print(f\"  Halo: {obs_halo_pct:.2f}%\")\n",
    "print(f\"  Bulk: {obs_bulk_pct:.2f}%\")\n",
    "\n",
    "print(f\"\\nEnrichment vs. vocabulary:\")\n",
    "print(f\"  Cluster: {cluster_enrichment:.2f}x\")\n",
    "print(f\"  Halo: {halo_enrichment:.2f}x\", end=\"\")\n",
    "\n",
    "# Compare to random bytes baseline\n",
    "if halo_enrichment > RANDOM_BYTES_HALO_ENRICHMENT * 1.2:\n",
    "    print(f\" ← HIGHER THAN RANDOM BYTES ({RANDOM_BYTES_HALO_ENRICHMENT:.2f}x)\")\n",
    "    print(f\"\\n  ✓ GB2312 byte pairs show STRONGER halo affinity than random noise!\")\n",
    "    print(f\"    This suggests Qwen was trained on GB2312-encoded Chinese text.\")\n",
    "elif halo_enrichment > RANDOM_BYTES_HALO_ENRICHMENT * 0.8:\n",
    "    print(f\" ← SIMILAR TO RANDOM BYTES ({RANDOM_BYTES_HALO_ENRICHMENT:.2f}x)\")\n",
    "    print(f\"\\n  ≈ GB2312 byte pairs behave like random noise\")\n",
    "    print(f\"    No special affinity for these byte patterns.\")\n",
    "else:\n",
    "    print(f\" ← LOWER THAN RANDOM BYTES ({RANDOM_BYTES_HALO_ENRICHMENT:.2f}x)\")\n",
    "    print(f\"\\n  ✗ GB2312 byte pairs produce FEWER halo tokens than random noise\")\n",
    "    print(f\"    Qwen likely did not train on GB2312 text.\")\n",
    "\n",
    "print(f\"  Bulk: {bulk_enrichment:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STATISTICAL SIGNIFICANCE\n",
      "======================================================================\n",
      "\n",
      "Chi-square goodness-of-fit test:\n",
      "  Null hypothesis: GB2312 byte pairs produce tokens uniformly from vocabulary\n",
      "  Chi-square statistic: 599.59\n",
      "  p-value: 6.31e-131\n",
      "\n",
      "  ✓ SIGNIFICANT: Distribution is non-uniform (p < 0.001)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STATISTICAL SIGNIFICANCE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "# Expected frequencies (if tokens were sampled uniformly from vocab)\n",
    "expected = [\n",
    "    total_tokens * len(cluster_token_ids) / vocab_size,\n",
    "    total_tokens * len(halo_token_ids) / vocab_size,\n",
    "    total_tokens * (vocab_size - len(cluster_token_ids) - len(halo_token_ids)) / vocab_size,\n",
    "]\n",
    "\n",
    "# Observed frequencies\n",
    "observed = [cluster_count, halo_count, bulk_count]\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p_value = chisquare(observed, expected)\n",
    "\n",
    "print(f\"Chi-square goodness-of-fit test:\")\n",
    "print(f\"  Null hypothesis: GB2312 byte pairs produce tokens uniformly from vocabulary\")\n",
    "print(f\"  Chi-square statistic: {chi2:.2f}\")\n",
    "print(f\"  p-value: {p_value:.2e}\")\n",
    "\n",
    "if p_value < 0.001:\n",
    "    print(f\"\\n  ✓ SIGNIFICANT: Distribution is non-uniform (p < 0.001)\")\n",
    "elif p_value < 0.05:\n",
    "    print(f\"\\n  ✓ SIGNIFICANT: Distribution is non-uniform (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"\\n  ✗ NOT SIGNIFICANT: Cannot reject null hypothesis (p ≥ 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Tokens from GB2312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MOST COMMON TOKENS FROM GB2312 BYTE PAIRS\n",
      "======================================================================\n",
      "\n",
      "Top 20 tokens produced by GB2312 byte pairs:\n",
      "\n",
      "  Token ID |  Count | Category | Decoded\n",
      "  ---------+--------+----------+------------------------------\n",
      "       114 |      8 |     Halo | '�'\n",
      "       222 |      6 |     Halo | '�'\n",
      "      3490 |      4 |     Halo | '�'\n",
      "       251 |      4 |     Halo | '�'\n",
      "       113 |      4 |     Halo | '�'\n",
      "       116 |      4 |     Halo | '�'\n",
      "       248 |      3 |     Halo | '�'\n",
      "       224 |      3 |     Halo | '�'\n",
      "     13519 |      3 |     Halo | '�'\n",
      "       244 |      3 |     Halo | '�'\n",
      "       112 |      3 |     Halo | '�'\n",
      "     20778 |      3 |     Halo | '�'\n",
      "       110 |      3 |     Halo | '�'\n",
      "       241 |      3 |     Halo | '�'\n",
      "        96 |      3 |     Halo | '�'\n",
      "       253 |      3 |     Halo | '�'\n",
      "     24864 |      3 |     Halo | '�'\n",
      "       233 |      3 |     Halo | '�'\n",
      "        99 |      3 |     Halo | '�'\n",
      "     37472 |      3 |     Halo | '�'\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MOST COMMON TOKENS FROM GB2312 BYTE PAIRS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "token_counter = Counter(all_tokens)\n",
    "most_common = token_counter.most_common(20)\n",
    "\n",
    "print(f\"Top 20 tokens produced by GB2312 byte pairs:\\n\")\n",
    "print(f\"  {'Token ID':>8} | {'Count':>6} | {'Category':>8} | Decoded\")\n",
    "print(f\"  {'-'*8}-+-{'-'*6}-+-{'-'*8}-+{'-'*30}\")\n",
    "\n",
    "for token_id, count in most_common:\n",
    "    # Classify\n",
    "    if token_id in cluster_token_ids:\n",
    "        category = \"Cluster\"\n",
    "    elif token_id in halo_token_ids:\n",
    "        category = \"Halo\"\n",
    "    else:\n",
    "        category = \"Bulk\"\n",
    "    \n",
    "    # Decode\n",
    "    decoded = tokenizer.decode([token_id])\n",
    "    decoded_display = repr(decoded)[:28]\n",
    "    \n",
    "    print(f\"  {token_id:8d} | {count:6,} | {category:>8} | {decoded_display}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Halo Tokens Hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HALO TOKEN COVERAGE\n",
      "======================================================================\n",
      "\n",
      "Halo token coverage:\n",
      "  Total halo tokens in vocabulary: 1,423\n",
      "  Unique halo tokens produced: 148\n",
      "  Coverage: 10.4%\n",
      "\n",
      "  ≈ MODERATE COVERAGE: GB2312 hits some halo tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"HALO TOKEN COVERAGE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# How many unique halo tokens did we hit?\n",
    "unique_halo_tokens = set(token_id for token_id in all_tokens if token_id in halo_token_ids)\n",
    "\n",
    "print(f\"Halo token coverage:\")\n",
    "print(f\"  Total halo tokens in vocabulary: {len(halo_token_ids):,}\")\n",
    "print(f\"  Unique halo tokens produced: {len(unique_halo_tokens):,}\")\n",
    "print(f\"  Coverage: {100 * len(unique_halo_tokens) / len(halo_token_ids):.1f}%\")\n",
    "\n",
    "if len(unique_halo_tokens) / len(halo_token_ids) > 0.5:\n",
    "    print(f\"\\n  ✓ HIGH COVERAGE: GB2312 byte pairs hit majority of halo tokens!\")\n",
    "    print(f\"    This strongly suggests halo tokens come from GB2312 text.\")\n",
    "elif len(unique_halo_tokens) / len(halo_token_ids) > 0.1:\n",
    "    print(f\"\\n  ≈ MODERATE COVERAGE: GB2312 hits some halo tokens\")\n",
    "else:\n",
    "    print(f\"\\n  ✗ LOW COVERAGE: GB2312 rarely produces halo tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "**If halo enrichment > 28x (20% higher than random bytes):**\n",
    "- ✓ **Smoking gun**: GB2312-encoded Chinese text is specifically overrepresented\n",
    "- Qwen's training data likely included pre-Unicode Chinese corpora\n",
    "- Halo tokens are artifacts of legacy encoding in training data\n",
    "\n",
    "**If halo enrichment ≈ 23x (similar to random bytes):**\n",
    "- ≈ **Inconclusive**: GB2312 behaves like general binary noise\n",
    "- Halo tokens come from various non-UTF-8 sources, not specifically GB2312\n",
    "\n",
    "**If halo enrichment < 18x (lower than random bytes):**\n",
    "- ✗ **Evidence against**: GB2312 is less halo-enriched than random noise\n",
    "- Qwen likely did not train on significant GB2312 text\n",
    "\n",
    "**High halo token coverage (>50%):**\n",
    "- Would indicate that most halo tokens are specifically reachable via GB2312 byte pairs\n",
    "- Strong evidence that legacy Chinese encoding is the primary source of halo tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
