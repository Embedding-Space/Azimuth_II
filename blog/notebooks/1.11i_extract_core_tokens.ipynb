{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Core Tokens: The Crystallized Solid\n",
    "\n",
    "From 1.11h we discovered that 124 unique vectors (2,211 tokens) are packed within 2 ULP of the Big One, with one outlier at 32 ULP.\n",
    "\n",
    "This notebook:\n",
    "- Identifies which tokens belong to the crystallized core (L∞ ≤ 2 ULP from Big One)\n",
    "- Saves them to a new safetensors file compatible with existing analysis notebooks\n",
    "- Creates `1.11i_core_cluster_tokens.safetensors` (drop-in replacement for `1.4h_cluster_tokens.safetensors`)\n",
    "\n",
    "With this cleaned dataset, we can re-run:\n",
    "- 1.11g (power-of-two quantization - should be perfect now)\n",
    "- 1.11c (visualization - cleaner structure)\n",
    "- Any other analysis that was polluted by the outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "CLUSTER_TOKENS_PATH = '../tensors/Qwen3-4B-Instruct-2507/1.4h_cluster_tokens.safetensors'\n",
    "GAMMA_PATH = '../tensors/Qwen3-4B-Instruct-2507/W.safetensors'\n",
    "OUTPUT_PATH = '../tensors/Qwen3-4B-Instruct-2507/1.11i_core_cluster_tokens.safetensors'\n",
    "\n",
    "# Core boundary (from 1.11h analysis)\n",
    "MAX_CORE_RADIUS_ULP = 3.0  # Include everything within 3 ULP of Big One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file, save_file\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2212 cluster token IDs from 1.4h\n"
     ]
    }
   ],
   "source": [
    "# Load cluster token IDs\n",
    "cluster_data = load_file(CLUSTER_TOKENS_PATH)\n",
    "cluster_token_ids = cluster_data['cluster_token_ids'].to(device)\n",
    "\n",
    "print(f'Loaded {len(cluster_token_ids)} cluster token IDs from 1.4h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gamma matrix: torch.Size([151936, 2560])\n",
      "Precision: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Load gamma matrix in bfloat16\n",
    "gamma_data = load_file(GAMMA_PATH)\n",
    "W = gamma_data['W'].to(torch.bfloat16).to(device)\n",
    "\n",
    "print(f'Loaded gamma matrix: {W.shape}')\n",
    "print(f'Precision: {W.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2212 vectors of dimension 2560\n"
     ]
    }
   ],
   "source": [
    "# Extract cluster vectors\n",
    "cluster_vectors = W[cluster_token_ids]\n",
    "\n",
    "print(f'Extracted {cluster_vectors.shape[0]} vectors of dimension {cluster_vectors.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Unique Vectors and Big One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 125 unique vectors\n",
      "Population range: 1 to 814\n"
     ]
    }
   ],
   "source": [
    "# Find unique vectors and their populations\n",
    "unique_vectors, inverse_indices = torch.unique(cluster_vectors.to('cpu'), dim=0, return_inverse=True)\n",
    "unique_vectors = unique_vectors.to(device)\n",
    "inverse_indices = inverse_indices.to(device)\n",
    "\n",
    "populations = torch.bincount(inverse_indices)\n",
    "\n",
    "print(f'Found {len(unique_vectors)} unique vectors')\n",
    "print(f'Population range: {populations.min().item()} to {populations.max().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Big One:\n",
      "  Index: 60\n",
      "  Population: 814 tokens\n",
      "  Vector norm: 0.370917\n"
     ]
    }
   ],
   "source": [
    "# Find the Big One (814-token black hole)\n",
    "big_one_idx = torch.argmax(populations).item()\n",
    "big_one_population = populations[big_one_idx].item()\n",
    "big_one_vector = unique_vectors[big_one_idx]\n",
    "\n",
    "print(f'\\nThe Big One:')\n",
    "print(f'  Index: {big_one_idx}')\n",
    "print(f'  Population: {big_one_population} tokens')\n",
    "print(f'  Vector norm: {torch.norm(big_one_vector.to(torch.float32)).item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute L∞ Distances from Big One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ULP at Big One scale: 1.525879e-05\n"
     ]
    }
   ],
   "source": [
    "def bfloat16_ulp(x):\n",
    "    \"\"\"Compute ULP for bfloat16 at value x.\"\"\"\n",
    "    if x == 0:\n",
    "        return 2**(-133)\n",
    "    exponent = int(np.floor(np.log2(np.abs(x))))\n",
    "    return 2**(exponent - 7)\n",
    "\n",
    "# Compute ULP at Big One scale\n",
    "big_one_f32 = big_one_vector.to(torch.float32).cpu().numpy()\n",
    "typical_value = np.median(np.abs(big_one_f32[big_one_f32 != 0]))\n",
    "ulp_at_origin = bfloat16_ulp(typical_value)\n",
    "\n",
    "print(f'ULP at Big One scale: {ulp_at_origin:.6e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distances in ULP units:\n",
      "  Range: [0.00, 32.00]\n",
      "  Mean: 1.61\n"
     ]
    }
   ],
   "source": [
    "# Compute L∞ distance from Big One to all unique vectors\n",
    "differences = unique_vectors - big_one_vector\n",
    "linf_distances = torch.max(torch.abs(differences), dim=1)[0]\n",
    "\n",
    "# Convert to ULP units\n",
    "linf_distances_f32 = linf_distances.to(torch.float32).cpu().numpy()\n",
    "distances_in_ulp = linf_distances_f32 / ulp_at_origin\n",
    "\n",
    "print(f'\\nDistances in ULP units:')\n",
    "print(f'  Range: [{distances_in_ulp.min():.2f}, {distances_in_ulp.max():.2f}]')\n",
    "print(f'  Mean: {distances_in_ulp.mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Core Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core boundary: L∞ ≤ 3.0 ULP\n",
      "\n",
      "Unique vectors:\n",
      "  Core: 122\n",
      "  Halo: 3\n",
      "  Total: 125\n"
     ]
    }
   ],
   "source": [
    "# Mask: True for unique vectors in the core\n",
    "core_vector_mask = distances_in_ulp <= MAX_CORE_RADIUS_ULP\n",
    "\n",
    "n_core_vectors = core_vector_mask.sum()\n",
    "n_halo_vectors = (~core_vector_mask).sum()\n",
    "\n",
    "print(f'Core boundary: L∞ ≤ {MAX_CORE_RADIUS_ULP} ULP')\n",
    "print(f'\\nUnique vectors:')\n",
    "print(f'  Core: {n_core_vectors}')\n",
    "print(f'  Halo: {n_halo_vectors}')\n",
    "print(f'  Total: {len(unique_vectors)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map to Original Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:\n",
      "  Core: 2206\n",
      "  Halo: 6\n",
      "  Total: 2212\n",
      "\n",
      "Core token population: 2206 / 2212 (99.73%)\n"
     ]
    }
   ],
   "source": [
    "# For each of the 2,212 original tokens, check if its unique vector is in the core\n",
    "# inverse_indices[i] tells us which unique vector token i maps to\n",
    "\n",
    "# Convert core_vector_mask to tensor\n",
    "core_vector_mask_tensor = torch.tensor(core_vector_mask, dtype=torch.bool).to(device)\n",
    "\n",
    "# For each token, check if its unique vector is in the core\n",
    "token_in_core = core_vector_mask_tensor[inverse_indices]\n",
    "\n",
    "# Extract core token IDs\n",
    "core_token_ids = cluster_token_ids[token_in_core]\n",
    "halo_token_ids = cluster_token_ids[~token_in_core]\n",
    "\n",
    "print(f'\\nTokens:')\n",
    "print(f'  Core: {len(core_token_ids)}')\n",
    "print(f'  Halo: {len(halo_token_ids)}')\n",
    "print(f'  Total: {len(cluster_token_ids)}')\n",
    "\n",
    "print(f'\\nCore token population: {len(core_token_ids)} / {len(cluster_token_ids)} '\n",
    "      f'({100*len(core_token_ids)/len(cluster_token_ids):.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Core Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population verification:\n",
      "  Core vectors population sum: 2206\n",
      "  Halo vectors population sum: 6\n",
      "  Total: 2212\n",
      "\n",
      "  Core: 2206 tokens across 122 unique vectors\n",
      "  Halo: 6 tokens across 3 unique vectors\n"
     ]
    }
   ],
   "source": [
    "# Count tokens in core vs halo by population\n",
    "core_populations = populations[core_vector_mask_tensor.cpu().numpy()]\n",
    "halo_populations = populations[~core_vector_mask_tensor.cpu().numpy()]\n",
    "\n",
    "print(f'Population verification:')\n",
    "print(f'  Core vectors population sum: {core_populations.sum().item()}')\n",
    "print(f'  Halo vectors population sum: {halo_populations.sum().item()}')\n",
    "print(f'  Total: {populations.sum().item()}')\n",
    "print()\n",
    "print(f'  Core: {core_populations.sum().item()} tokens across {len(core_populations)} unique vectors')\n",
    "print(f'  Halo: {halo_populations.sum().item()} tokens across {len(halo_populations)} unique vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Halo Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halo outlier token IDs: [83971, 136755, 136831, 138068, 138072, 139278]\n",
      "Halo outlier distances (ULP): [32.0, 4.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "if len(halo_token_ids) > 0:\n",
    "    print(f'Halo outlier token IDs: {halo_token_ids.cpu().numpy().tolist()}')\n",
    "    print(f'Halo outlier distances (ULP): {distances_in_ulp[~core_vector_mask].tolist()}')\n",
    "else:\n",
    "    print('No halo outliers (all tokens in core)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Core Cluster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving core cluster data...\n",
      "✓ Saved to ../tensors/Qwen3-4B-Instruct-2507/1.11i_core_cluster_tokens.safetensors\n",
      "  Size: 17.6 KB\n",
      "\n",
      "Saved data:\n",
      "  cluster_token_ids: 2,206 core tokens\n",
      "  max_core_radius_ulp: 3.0 (boundary)\n",
      "  n_core_tokens: 2206\n",
      "  n_halo_tokens: 6\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nSaving core cluster data...')\n",
    "\n",
    "# Save in same format as 1.4h for compatibility\n",
    "save_file({\n",
    "    'cluster_token_ids': core_token_ids.cpu(),\n",
    "    'max_core_radius_ulp': torch.tensor([MAX_CORE_RADIUS_ULP], dtype=torch.float32),\n",
    "    'n_core_tokens': torch.tensor([len(core_token_ids)], dtype=torch.int64),\n",
    "    'n_halo_tokens': torch.tensor([len(halo_token_ids)], dtype=torch.int64),\n",
    "}, OUTPUT_PATH)\n",
    "\n",
    "print(f'✓ Saved to {OUTPUT_PATH}')\n",
    "print(f'  Size: {Path(OUTPUT_PATH).stat().st_size / 1024:.1f} KB')\n",
    "print()\n",
    "print(f'Saved data:')\n",
    "print(f'  cluster_token_ids: {len(core_token_ids):,} core tokens')\n",
    "print(f'  max_core_radius_ulp: {MAX_CORE_RADIUS_ULP} (boundary)')\n",
    "print(f'  n_core_tokens: {len(core_token_ids)}')\n",
    "print(f'  n_halo_tokens: {len(halo_token_ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying saved data...\n",
      "✓ Verification passed\n",
      "  Token IDs: 2,206\n",
      "  Core radius: 3.0 ULP\n",
      "  Core tokens: 2206\n",
      "  Halo tokens: 6\n",
      "\n",
      "All checks passed! Core cluster data is ready.\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nVerifying saved data...')\n",
    "\n",
    "# Load back\n",
    "verification = load_file(OUTPUT_PATH)\n",
    "loaded_ids = verification['cluster_token_ids']\n",
    "loaded_radius = verification['max_core_radius_ulp']\n",
    "loaded_n_core = verification['n_core_tokens']\n",
    "loaded_n_halo = verification['n_halo_tokens']\n",
    "\n",
    "# Verify\n",
    "assert len(loaded_ids) == len(core_token_ids), 'Count mismatch'\n",
    "assert torch.all(loaded_ids == core_token_ids.cpu()), 'Token IDs do not match'\n",
    "assert loaded_n_core.item() == len(core_token_ids), 'Core count mismatch'\n",
    "assert loaded_n_halo.item() == len(halo_token_ids), 'Halo count mismatch'\n",
    "\n",
    "print(f'✓ Verification passed')\n",
    "print(f'  Token IDs: {len(loaded_ids):,}')\n",
    "print(f'  Core radius: {loaded_radius.item()} ULP')\n",
    "print(f'  Core tokens: {loaded_n_core.item()}')\n",
    "print(f'  Halo tokens: {loaded_n_halo.item()}')\n",
    "print()\n",
    "print(f'All checks passed! Core cluster data is ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CORE CLUSTER EXTRACTION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Original cluster (1.4h): 2212 tokens\n",
      "Core (L∞ ≤ 3.0 ULP): 2206 tokens (122 unique vectors)\n",
      "Halo (L∞ > 3.0 ULP): 6 tokens (3 unique vectors)\n",
      "\n",
      "Saved to: ../tensors/Qwen3-4B-Instruct-2507/1.11i_core_cluster_tokens.safetensors\n",
      "\n",
      "This file is a drop-in replacement for 1.4h_cluster_tokens.safetensors\n",
      "Use it to re-run analysis on the crystallized core only:\n",
      "  - 1.11g (power-of-two quantization proof)\n",
      "  - 1.11c (cluster visualization)\n",
      "  - 1.11d (dimensional diversity)\n",
      "  - Any other notebook that loads cluster_token_ids\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('CORE CLUSTER EXTRACTION COMPLETE')\n",
    "print('='*70)\n",
    "print()\n",
    "print(f'Original cluster (1.4h): {len(cluster_token_ids)} tokens')\n",
    "print(f'Core (L∞ ≤ {MAX_CORE_RADIUS_ULP} ULP): {len(core_token_ids)} tokens ({len(core_populations)} unique vectors)')\n",
    "print(f'Halo (L∞ > {MAX_CORE_RADIUS_ULP} ULP): {len(halo_token_ids)} tokens ({len(halo_populations)} unique vectors)')\n",
    "print()\n",
    "print(f'Saved to: {OUTPUT_PATH}')\n",
    "print()\n",
    "print(f'This file is a drop-in replacement for 1.4h_cluster_tokens.safetensors')\n",
    "print(f'Use it to re-run analysis on the crystallized core only:')\n",
    "print(f'  - 1.11g (power-of-two quantization proof)')\n",
    "print(f'  - 1.11c (cluster visualization)')\n",
    "print(f'  - 1.11d (dimensional diversity)')\n",
    "print(f'  - Any other notebook that loads cluster_token_ids')\n",
    "print()\n",
    "print('='*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
