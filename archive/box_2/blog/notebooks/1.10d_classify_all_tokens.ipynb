{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.10d: Classify All Tokens by Script\n",
    "\n",
    "**Complete taxonomy of Qwen's 151,936-token vocabulary**\n",
    "\n",
    "## The Question\n",
    "\n",
    "We've been exploring:\n",
    "- Cluster tokens (geometric collapse)\n",
    "- Halo tokens (unreachable via Unicode, but reachable via bytes)\n",
    "- Bulk tokens (normal vocabulary)\n",
    "\n",
    "But we still don't know: **What ARE these tokens?**\n",
    "\n",
    "## The Goal\n",
    "\n",
    "Classify every token in Qwen's vocabulary by:\n",
    "1. **Script/Alphabet** (Han/CJK, Latin, Thai, Cyrillic, Arabic, etc.)\n",
    "2. **Character type** (printable, replacement character �, control characters, mixed)\n",
    "3. **Geometric category** (cluster, halo, bulk)\n",
    "\n",
    "Then save this classification as a dataset for future analysis.\n",
    "\n",
    "## Method\n",
    "\n",
    "Use the **`alphabet-detector`** library for automatic script detection:\n",
    "```python\n",
    "from alphabet_detector import AlphabetDetector\n",
    "ad = AlphabetDetector()\n",
    "\n",
    "ad.detect_alphabet(\"你好\")    # {'HAN'}\n",
    "ad.detect_alphabet(\"Hello\")  # {'LATIN'}\n",
    "ad.detect_alphabet(\"สวัสดี\")  # {'THAI'}\n",
    "```\n",
    "\n",
    "For each token:\n",
    "1. Decode it (UTF-8, which may produce �)\n",
    "2. Detect script(s) using `alphabet-detector`\n",
    "3. Check for special cases (replacement chars, control chars, empty)\n",
    "4. Record geometric category (cluster/halo/bulk)\n",
    "5. Save everything to a CSV/safetensors for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\"\n",
    "HF_MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# Input data\n",
    "CLUSTER_TOKENS_PATH = \"../tensors/Qwen3-4B-Instruct-2507/1.4h_cluster_tokens.safetensors\"\n",
    "REACHABILITY_PATH = \"../tensors/Qwen3-4B-Instruct-2507/1.8d_full_vocab_reachability.safetensors\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_CSV = \"../tensors/Qwen3-4B-Instruct-2507/1.10d_token_classification.csv\"\n",
    "OUTPUT_SAFETENSORS = \"../tensors/Qwen3-4B-Instruct-2507/1.10d_token_classification.safetensors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ alphabet-detector imported successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from transformers import AutoTokenizer\n",
    "from safetensors.torch import load_file, save_file\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# Install alphabet-detector if needed: uv add alphabet-detector\n",
    "try:\n",
    "    from alphabet_detector import AlphabetDetector\n",
    "    ad = AlphabetDetector()\n",
    "    print(\"✓ alphabet-detector imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: alphabet-detector not installed\")\n",
    "    print(\"Please run: uv add alphabet-detector\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Token Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading geometric classifications...\n",
      "\n",
      "✓ Loaded geometric classifications\n",
      "  Cluster tokens: 2,212\n",
      "  Halo tokens: 1,423\n",
      "  Bulk tokens: 148,034\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading geometric classifications...\\n\")\n",
    "\n",
    "# Load cluster tokens\n",
    "cluster_data = load_file(CLUSTER_TOKENS_PATH)\n",
    "cluster_token_ids = set(cluster_data['cluster_token_ids'].tolist())\n",
    "\n",
    "# Load halo tokens (unreachable outside cluster)\n",
    "reachability_data = load_file(REACHABILITY_PATH)\n",
    "halo_token_ids = set(reachability_data['unreachable_outside_cluster'].tolist())\n",
    "\n",
    "print(f\"✓ Loaded geometric classifications\")\n",
    "print(f\"  Cluster tokens: {len(cluster_token_ids):,}\")\n",
    "print(f\"  Halo tokens: {len(halo_token_ids):,}\")\n",
    "print(f\"  Bulk tokens: {151669 - len(cluster_token_ids) - len(halo_token_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer: Qwen/Qwen3-4B-Instruct-2507\n",
      "\n",
      "✓ Tokenizer loaded\n",
      "  Vocabulary size: 151,669 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading tokenizer: {HF_MODEL_NAME}\\n\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "print(f\"✓ Tokenizer loaded\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def classify_token(token_id, tokenizer, ad, cluster_ids, halo_ids):\n",
    "    \"\"\"\n",
    "    Classify a single token by script, character type, and geometric category.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - token_id\n",
    "    - decoded_text\n",
    "    - scripts (comma-separated set of detected scripts)\n",
    "    - is_replacement (contains U+FFFD)\n",
    "    - is_printable (all printable characters)\n",
    "    - is_empty (empty or whitespace only)\n",
    "    - char_count (number of characters)\n",
    "    - geometric_category (cluster, halo, bulk)\n",
    "    - primary_script (most common script, or special category)\n",
    "    \"\"\"\n",
    "    # Decode token\n",
    "    decoded = tokenizer.decode([token_id])\n",
    "    \n",
    "    # Geometric category\n",
    "    if token_id in cluster_ids:\n",
    "        geometric_category = 'cluster'\n",
    "    elif token_id in halo_ids:\n",
    "        geometric_category = 'halo'\n",
    "    else:\n",
    "        geometric_category = 'bulk'\n",
    "    \n",
    "    # Character type checks\n",
    "    is_replacement = '\\ufffd' in decoded\n",
    "    is_empty = len(decoded.strip()) == 0\n",
    "    char_count = len(decoded)\n",
    "    \n",
    "    # Printability check\n",
    "    is_printable = True\n",
    "    if decoded:\n",
    "        for char in decoded:\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat.startswith('C') and not char.isspace():\n",
    "                is_printable = False\n",
    "                break\n",
    "    \n",
    "    # Script detection\n",
    "    scripts = set()\n",
    "    if decoded and not is_replacement:\n",
    "        try:\n",
    "            detected = ad.detect_alphabet(decoded)\n",
    "            if detected:\n",
    "                scripts = detected\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Primary script classification\n",
    "    if is_replacement:\n",
    "        primary_script = 'REPLACEMENT'\n",
    "    elif is_empty:\n",
    "        primary_script = 'EMPTY'\n",
    "    elif not is_printable:\n",
    "        primary_script = 'CONTROL'\n",
    "    elif len(scripts) == 0:\n",
    "        primary_script = 'UNKNOWN'\n",
    "    elif len(scripts) == 1:\n",
    "        primary_script = list(scripts)[0]\n",
    "    else:\n",
    "        # Mixed scripts - pick most common or use 'MIXED'\n",
    "        primary_script = 'MIXED'\n",
    "    \n",
    "    return {\n",
    "        'token_id': token_id,\n",
    "        'decoded_text': decoded,\n",
    "        'scripts': ','.join(sorted(scripts)) if scripts else '',\n",
    "        'is_replacement': is_replacement,\n",
    "        'is_printable': is_printable,\n",
    "        'is_empty': is_empty,\n",
    "        'char_count': char_count,\n",
    "        'geometric_category': geometric_category,\n",
    "        'primary_script': primary_script,\n",
    "    }\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify All Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CLASSIFYING ALL TOKENS\n",
      "======================================================================\n",
      "\n",
      "Processing 151,669 tokens...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying tokens: 100%|██████████| 151669/151669 [00:00<00:00, 250522.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Classification complete\n",
      "  Tokens classified: 151,669\n",
      "  Dataframe shape: (151669, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLASSIFYING ALL TOKENS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Processing {vocab_size:,} tokens...\\n\")\n",
    "\n",
    "# Classify all tokens\n",
    "classifications = []\n",
    "\n",
    "for token_id in tqdm(range(vocab_size), desc=\"Classifying tokens\"):\n",
    "    classification = classify_token(token_id, tokenizer, ad, cluster_token_ids, halo_token_ids)\n",
    "    classifications.append(classification)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(classifications)\n",
    "\n",
    "print(f\"\\n✓ Classification complete\")\n",
    "print(f\"  Tokens classified: {len(df):,}\")\n",
    "print(f\"  Dataframe shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SUMMARY STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Primary script distribution:\n",
      "  LATIN               : 94,610 (62.38%)\n",
      "  CJK                 : 25,464 (16.79%)\n",
      "  UNKNOWN             :  8,239 ( 5.43%)\n",
      "  CYRILLIC            :  4,142 ( 2.73%)\n",
      "  ARABIC              :  3,978 ( 2.62%)\n",
      "  HANGUL              :  3,567 ( 2.35%)\n",
      "  HEBREW              :  3,179 ( 2.10%)\n",
      "  THAI                :  2,549 ( 1.68%)\n",
      "  REPLACEMENT         :  1,457 ( 0.96%)\n",
      "  HIRAGANA            :    939 ( 0.62%)\n",
      "  MIXED               :    681 ( 0.45%)\n",
      "  KATAKANA            :    450 ( 0.30%)\n",
      "  EMPTY               :    441 ( 0.29%)\n",
      "  MATHEMATICAL        :    434 ( 0.29%)\n",
      "  GREEK               :    232 ( 0.15%)\n",
      "  ETHIOPIC            :    112 ( 0.07%)\n",
      "  MODIFIER            :     80 ( 0.05%)\n",
      "  ARMENIAN            :     73 ( 0.05%)\n",
      "  CANADIAN            :     71 ( 0.05%)\n",
      "  HALFWIDTH           :     63 ( 0.04%)\n",
      "  DEVANAGARI          :     56 ( 0.04%)\n",
      "  CONTROL             :     53 ( 0.03%)\n",
      "  FULLWIDTH           :     52 ( 0.03%)\n",
      "  TAI                 :     43 ( 0.03%)\n",
      "  BENGALI             :     39 ( 0.03%)\n",
      "  GEORGIAN            :     36 ( 0.02%)\n",
      "  MYANMAR             :     36 ( 0.02%)\n",
      "  KHMER               :     33 ( 0.02%)\n",
      "  LAO                 :     33 ( 0.02%)\n",
      "  NKO                 :     32 ( 0.02%)\n",
      "  MALAYALAM           :     31 ( 0.02%)\n",
      "  MONGOLIAN           :     28 ( 0.02%)\n",
      "  COPTIC              :     27 ( 0.02%)\n",
      "  SYRIAC              :     26 ( 0.02%)\n",
      "  KANNADA             :     25 ( 0.02%)\n",
      "  TIBETAN             :     25 ( 0.02%)\n",
      "  SINHALA             :     25 ( 0.02%)\n",
      "  TIFINAGH            :     25 ( 0.02%)\n",
      "  TAMIL               :     25 ( 0.02%)\n",
      "  JAVANESE            :     18 ( 0.01%)\n",
      "  GUJARATI            :     16 ( 0.01%)\n",
      "  BOPOMOFO            :     16 ( 0.01%)\n",
      "  CHEROKEE            :     15 ( 0.01%)\n",
      "  TELUGU              :     14 ( 0.01%)\n",
      "  RUNIC               :     12 ( 0.01%)\n",
      "  SCRIPT              :     11 ( 0.01%)\n",
      "  GOTHIC              :     10 ( 0.01%)\n",
      "  YI                  :     10 ( 0.01%)\n",
      "  GURMUKHI            :     10 ( 0.01%)\n",
      "  GLAGOLITIC          :      9 ( 0.01%)\n",
      "  EGYPTIAN            :      9 ( 0.01%)\n",
      "  THAANA              :      8 ( 0.01%)\n",
      "  ORIYA               :      7 ( 0.00%)\n",
      "  DOUBLE-STRUCK       :      7 ( 0.00%)\n",
      "  PHOENICIAN          :      6 ( 0.00%)\n",
      "  CUNEIFORM           :      6 ( 0.00%)\n",
      "  PHAGS-PA            :      6 ( 0.00%)\n",
      "  MANDAIC             :      6 ( 0.00%)\n",
      "  BUGINESE            :      5 ( 0.00%)\n",
      "  OLD                 :      5 ( 0.00%)\n",
      "  INSCRIPTIONAL       :      4 ( 0.00%)\n",
      "  BAMUM               :      4 ( 0.00%)\n",
      "  VERTICAL            :      4 ( 0.00%)\n",
      "  LIMBU               :      3 ( 0.00%)\n",
      "  SAMARITAN           :      3 ( 0.00%)\n",
      "  OGHAM               :      3 ( 0.00%)\n",
      "  BALINESE            :      2 ( 0.00%)\n",
      "  PLANCK              :      2 ( 0.00%)\n",
      "  IDEOGRAPHIC         :      2 ( 0.00%)\n",
      "  SUPERSCRIPT         :      2 ( 0.00%)\n",
      "  BLACK-LETTER        :      2 ( 0.00%)\n",
      "  MICRO               :      2 ( 0.00%)\n",
      "  ALEF                :      1 ( 0.00%)\n",
      "  MODI                :      1 ( 0.00%)\n",
      "  KAITHI              :      1 ( 0.00%)\n",
      "  MASCULINE           :      1 ( 0.00%)\n",
      "  ANGSTROM            :      1 ( 0.00%)\n",
      "  LISU                :      1 ( 0.00%)\n",
      "  OL                  :      1 ( 0.00%)\n",
      "  KATAKANA-HIRAGANA   :      1 ( 0.00%)\n",
      "  CARON               :      1 ( 0.00%)\n",
      "  FEMININE            :      1 ( 0.00%)\n",
      "  INFORMATION         :      1 ( 0.00%)\n",
      "  OHM                 :      1 ( 0.00%)\n",
      "  VAI                 :      1 ( 0.00%)\n",
      "  EULER               :      1 ( 0.00%)\n",
      "  LEPCHA              :      1 ( 0.00%)\n",
      "  BATAK               :      1 ( 0.00%)\n",
      "  SUNDANESE           :      1 ( 0.00%)\n",
      "  KELVIN              :      1 ( 0.00%)\n",
      "  MASU                :      1 ( 0.00%)\n",
      "\n",
      "Geometric category distribution:\n",
      "  bulk                : 148,301 (97.78%)\n",
      "  cluster             :  1,945 ( 1.28%)\n",
      "  halo                :  1,423 ( 0.94%)\n",
      "\n",
      "Special categories:\n",
      "  Replacement chars: 1,457 (0.96%)\n",
      "  Empty/whitespace: 441 (0.29%)\n",
      "  Non-printable: 54 (0.04%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(\"Primary script distribution:\")\n",
    "script_counts = df['primary_script'].value_counts()\n",
    "for script, count in script_counts.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"  {script:20s}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nGeometric category distribution:\")\n",
    "geom_counts = df['geometric_category'].value_counts()\n",
    "for category, count in geom_counts.items():\n",
    "    pct = 100 * count / len(df)\n",
    "    print(f\"  {category:20s}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nSpecial categories:\")\n",
    "print(f\"  Replacement chars: {df['is_replacement'].sum():,} ({100*df['is_replacement'].sum()/len(df):.2f}%)\")\n",
    "print(f\"  Empty/whitespace: {df['is_empty'].sum():,} ({100*df['is_empty'].sum()/len(df):.2f}%)\")\n",
    "print(f\"  Non-printable: {(~df['is_printable']).sum():,} ({100*(~df['is_printable']).sum()/len(df):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Tabulation: Script vs Geometric Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCRIPT × GEOMETRIC CATEGORY\n",
      "======================================================================\n",
      "\n",
      "geometric_category   bulk  cluster  halo  total\n",
      "primary_script                                 \n",
      "LATIN               94585       25     0  94610\n",
      "CJK                 25269      195     0  25464\n",
      "UNKNOWN              8233        6     0   8239\n",
      "CYRILLIC             4142        0     0   4142\n",
      "ARABIC               3903       75     0   3978\n",
      "...                   ...      ...   ...    ...\n",
      "FEMININE                1        0     0      1\n",
      "EULER                   1        0     0      1\n",
      "CARON                   1        0     0      1\n",
      "BATAK                   1        0     0      1\n",
      "KELVIN                  0        1     0      1\n",
      "\n",
      "[91 rows x 4 columns]\n",
      "\n",
      "Key insights:\n",
      "  - Are cluster tokens dominated by specific scripts?\n",
      "  - Are halo tokens mostly REPLACEMENT characters?\n",
      "  - What scripts dominate the bulk vocabulary?\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SCRIPT × GEOMETRIC CATEGORY\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Cross-tab\n",
    "crosstab = pd.crosstab(df['primary_script'], df['geometric_category'])\n",
    "\n",
    "# Sort by total count\n",
    "crosstab['total'] = crosstab.sum(axis=1)\n",
    "crosstab = crosstab.sort_values('total', ascending=False)\n",
    "\n",
    "print(crosstab)\n",
    "\n",
    "print(f\"\\nKey insights:\")\n",
    "print(f\"  - Are cluster tokens dominated by specific scripts?\")\n",
    "print(f\"  - Are halo tokens mostly REPLACEMENT characters?\")\n",
    "print(f\"  - What scripts dominate the bulk vocabulary?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Tokens by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAMPLE TOKENS BY CATEGORY\n",
      "======================================================================\n",
      "\n",
      "\n",
      "REPLACEMENT (halo):\n",
      "  Token     94: '�'\n",
      "  Token     95: '�'\n",
      "  Token     96: '�'\n",
      "  Token     97: '�'\n",
      "  Token     98: '�'\n",
      "\n",
      "LATIN (bulk):\n",
      "  Token     32: 'A'\n",
      "  Token     33: 'B'\n",
      "  Token     34: 'C'\n",
      "  Token     35: 'D'\n",
      "  Token     36: 'E'\n",
      "\n",
      "THAI (cluster):\n",
      "  Token 123828: 'ที่'\n",
      "  Token 123939: 'ร์'\n",
      "  Token 123948: 'ติ'\n",
      "  Token 123952: 'ด้'\n",
      "  Token 124027: 'เป็น'\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SAMPLE TOKENS BY CATEGORY\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Show examples from each major category\n",
    "categories = [\n",
    "    ('REPLACEMENT', 'halo'),\n",
    "    ('HAN', 'bulk'),\n",
    "    ('LATIN', 'bulk'),\n",
    "    ('THAI', 'cluster'),\n",
    "]\n",
    "\n",
    "for primary_script, geom_cat in categories:\n",
    "    samples = df[(df['primary_script'] == primary_script) & (df['geometric_category'] == geom_cat)].head(5)\n",
    "    \n",
    "    if len(samples) > 0:\n",
    "        print(f\"\\n{primary_script} ({geom_cat}):\")\n",
    "        for _, row in samples.iterrows():\n",
    "            decoded_display = repr(row['decoded_text'])[:40]\n",
    "            print(f\"  Token {row['token_id']:6d}: {decoded_display}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAVING RESULTS\n",
      "======================================================================\n",
      "\n",
      "✓ Saved CSV to: ../tensors/Qwen3-4B-Instruct-2507/1.10d_token_classification.csv\n",
      "  Rows: 151,669\n",
      "  Columns: 9\n",
      "\n",
      "✓ Saved safetensors to: ../tensors/Qwen3-4B-Instruct-2507/1.10d_token_classification.safetensors\n",
      "\n",
      "Script code mapping:\n",
      "    0: ALEF\n",
      "    1: ANGSTROM\n",
      "    2: ARABIC\n",
      "    3: ARMENIAN\n",
      "    4: BALINESE\n",
      "    5: BAMUM\n",
      "    6: BATAK\n",
      "    7: BENGALI\n",
      "    8: BLACK-LETTER\n",
      "    9: BOPOMOFO\n",
      "  ... (81 more)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SAVING RESULTS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Save CSV (human-readable)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✓ Saved CSV to: {OUTPUT_CSV}\")\n",
    "print(f\"  Rows: {len(df):,}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "\n",
    "# Save safetensors (for fast loading in future notebooks)\n",
    "save_dict = {\n",
    "    'token_ids': torch.tensor(df['token_id'].values, dtype=torch.int64),\n",
    "    'is_replacement': torch.tensor(df['is_replacement'].values, dtype=torch.bool),\n",
    "    'is_printable': torch.tensor(df['is_printable'].values, dtype=torch.bool),\n",
    "    'is_empty': torch.tensor(df['is_empty'].values, dtype=torch.bool),\n",
    "    'char_count': torch.tensor(df['char_count'].values, dtype=torch.int32),\n",
    "}\n",
    "\n",
    "# Encode categorical variables as integers\n",
    "geom_cat_map = {'bulk': 0, 'halo': 1, 'cluster': 2}\n",
    "df['geometric_category_code'] = df['geometric_category'].map(geom_cat_map)\n",
    "save_dict['geometric_category'] = torch.tensor(df['geometric_category_code'].values, dtype=torch.int8)\n",
    "\n",
    "# Script codes (store unique scripts and their codes separately)\n",
    "unique_scripts = sorted(df['primary_script'].unique())\n",
    "script_to_code = {script: i for i, script in enumerate(unique_scripts)}\n",
    "df['primary_script_code'] = df['primary_script'].map(script_to_code)\n",
    "save_dict['primary_script_code'] = torch.tensor(df['primary_script_code'].values, dtype=torch.int16)\n",
    "\n",
    "save_file(save_dict, OUTPUT_SAFETENSORS)\n",
    "print(f\"\\n✓ Saved safetensors to: {OUTPUT_SAFETENSORS}\")\n",
    "\n",
    "# Save script mapping for reference\n",
    "print(f\"\\nScript code mapping:\")\n",
    "for script, code in sorted(script_to_code.items(), key=lambda x: x[1])[:10]:\n",
    "    print(f\"  {code:3d}: {script}\")\n",
    "if len(unique_scripts) > 10:\n",
    "    print(f\"  ... ({len(unique_scripts) - 10} more)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook classified all 151,936 tokens in Qwen's vocabulary by:\n",
    "- **Script/Alphabet** (using `alphabet-detector`)\n",
    "- **Character type** (replacement, printable, empty, control)\n",
    "- **Geometric category** (cluster, halo, bulk)\n",
    "\n",
    "**Key findings to look for:**\n",
    "\n",
    "1. **Cluster tokens:** Are they dominated by Thai script? (Expected from 1.7a)\n",
    "2. **Halo tokens:** Are they mostly REPLACEMENT characters? (Expected from 1.10a)\n",
    "3. **Bulk tokens:** What scripts dominate? (HAN/CJK for Chinese? LATIN for English?)\n",
    "4. **Script distribution:** Overall breakdown of vocabulary by writing system\n",
    "\n",
    "**Next steps:**\n",
    "- Load this classification in future notebooks to filter/analyze by script\n",
    "- Investigate specific script categories in detail\n",
    "- Cross-reference with training data properties (if available)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
