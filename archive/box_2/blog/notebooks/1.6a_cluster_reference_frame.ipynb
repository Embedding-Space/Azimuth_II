{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 1.6a: Cluster-Centric Reference Frame\n",
    "\n",
    "This notebook establishes a coordinate system centered on the cluster and computes reusable geometric infrastructure.\n",
    "\n",
    "## The Question\n",
    "\n",
    "We've identified a tight cluster of 2,212 tokens (1.4g-1.4h) and seen that it appears as a concentrated spike in the radial density profile (1.5c). But what about the remaining ~18,161 non-cluster tokens in the overdensity?\n",
    "\n",
    "Are they:\n",
    "- **Uniform background** (\"ice cube floating in diffuse gas\")\n",
    "- **Thermal halo** (tokens that escaped the cluster)\n",
    "- **Structured** (sub-clusters, filaments, anisotropies)\n",
    "\n",
    "To answer this, we need to view the universe **from the cluster's perspective**. This notebook sets up the infrastructure:\n",
    "\n",
    "1. **Cluster centroid** - the origin of our new coordinate system\n",
    "2. **PCA basis** - consistent axes for all future visualizations\n",
    "\n",
    "These will be reused throughout the 1.6 series to study the structure surrounding the cluster.\n",
    "\n",
    "## Method\n",
    "\n",
    "We'll:\n",
    "1. Load W and compute PCA (save eigenvectors + eigenvalues)\n",
    "2. Load cluster token IDs and compute geometric centroid\n",
    "3. Save everything for reuse\n",
    "4. Verify the saved data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to analyze\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file, save_file\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Detect available device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Load W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W: torch.Size([151936, 2560])\n",
      "  151,936 tokens in 2,560 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Load W\n",
    "tensor_path = Path(f\"../tensors/{MODEL_NAME}/W.safetensors\")\n",
    "W_bf16 = load_file(tensor_path)[\"W\"]\n",
    "W = W_bf16.to(torch.float32).to(device)\n",
    "N, d = W.shape\n",
    "\n",
    "print(f\"Loaded W: {W.shape}\")\n",
    "print(f\"  {N:,} tokens in {d:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Compute PCA Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing PCA...\n",
      "\n",
      "✓ PCA computed\n",
      "\n",
      "Top 10 eigenvalues:\n",
      "  PC1: 0.010487 (0.94% variance)\n",
      "  PC2: 0.003178 (0.28% variance)\n",
      "  PC3: 0.002791 (0.25% variance)\n",
      "  PC4: 0.002616 (0.23% variance)\n",
      "  PC5: 0.001973 (0.18% variance)\n",
      "  PC6: 0.001805 (0.16% variance)\n",
      "  PC7: 0.001609 (0.14% variance)\n",
      "  PC8: 0.001549 (0.14% variance)\n",
      "  PC9: 0.001468 (0.13% variance)\n",
      "  PC10: 0.001389 (0.12% variance)\n",
      "\n",
      "Cumulative variance:\n",
      "  First 10 PCs: 2.58%\n",
      "  First 100 PCs: 10.16%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing PCA...\\n\")\n",
    "\n",
    "# Center the data\n",
    "W_centered = W - W.mean(dim=0)\n",
    "\n",
    "# Covariance matrix\n",
    "cov = (W_centered.T @ W_centered) / N\n",
    "\n",
    "# Move to CPU for eigen decomposition (MPS doesn't support eigh)\n",
    "cov_cpu = cov.cpu()\n",
    "\n",
    "# Eigendecomposition\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(cov_cpu)\n",
    "\n",
    "# Sort descending by eigenvalue\n",
    "idx = torch.argsort(eigenvalues, descending=True)\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "print(f\"✓ PCA computed\")\n",
    "print(f\"\\nTop 10 eigenvalues:\")\n",
    "for i in range(10):\n",
    "    variance_explained = eigenvalues[i] / eigenvalues.sum() * 100\n",
    "    print(f\"  PC{i+1}: {eigenvalues[i].item():.6f} ({variance_explained:.2f}% variance)\")\n",
    "\n",
    "# Compute cumulative variance\n",
    "cumsum = torch.cumsum(eigenvalues, dim=0)\n",
    "total_variance = eigenvalues.sum()\n",
    "variance_10 = cumsum[9] / total_variance * 100\n",
    "variance_100 = cumsum[99] / total_variance * 100\n",
    "\n",
    "print(f\"\\nCumulative variance:\")\n",
    "print(f\"  First 10 PCs: {variance_10:.2f}%\")\n",
    "print(f\"  First 100 PCs: {variance_100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Load Cluster Tokens and Compute Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading cluster tokens...\n",
      "\n",
      "✓ Loaded 2,212 cluster tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading cluster tokens...\\n\")\n",
    "\n",
    "# Load cluster token IDs\n",
    "cluster_path = Path(f\"../tensors/{MODEL_NAME}/1.4h_cluster_tokens.safetensors\")\n",
    "cluster_data = load_file(cluster_path)\n",
    "cluster_token_ids = cluster_data['cluster_token_ids']\n",
    "\n",
    "print(f\"✓ Loaded {len(cluster_token_ids):,} cluster tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing cluster centroid...\n",
      "\n",
      "✓ Cluster centroid computed\n",
      "  Shape: torch.Size([2560])\n",
      "  Norm: 0.370917\n",
      "\n",
      "  First 10 components: [ 0.00607304  0.01324463  0.01177979  0.03662109  0.01586914 -0.01257324\n",
      "  0.00659181  0.00830075 -0.02148438 -0.05200195]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing cluster centroid...\\n\")\n",
    "\n",
    "# Get cluster vectors\n",
    "cluster_vecs = W[cluster_token_ids]\n",
    "\n",
    "# Compute geometric centroid (unweighted mean)\n",
    "cluster_centroid = cluster_vecs.mean(dim=0)\n",
    "\n",
    "print(f\"✓ Cluster centroid computed\")\n",
    "print(f\"  Shape: {cluster_centroid.shape}\")\n",
    "print(f\"  Norm: {torch.linalg.vector_norm(cluster_centroid).item():.6f}\")\n",
    "print(f\"\\n  First 10 components: {cluster_centroid.cpu().numpy()[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Save Reference Frame Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving reference frame data...\n",
      "\n",
      "✓ Saved to ../tensors/Qwen3-4B-Instruct-2507/1.6a_cluster_reference_frame.safetensors\n",
      "  Size: 25.0 MB\n",
      "\n",
      "Saved data:\n",
      "  cluster_centroid: torch.Size([2560]) (geometric center of 2,212 cluster tokens)\n",
      "  W_eigenvalues: torch.Size([2560]) (PCA eigenvalues for full vocabulary)\n",
      "  W_eigenvectors: torch.Size([2560, 2560]) (PCA basis vectors for full vocabulary)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving reference frame data...\\n\")\n",
    "\n",
    "# Prepare output\n",
    "output_dir = Path(f\"../tensors/{MODEL_NAME}\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"1.6a_cluster_reference_frame.safetensors\"\n",
    "\n",
    "# Save\n",
    "save_file({\n",
    "    'cluster_centroid': cluster_centroid.cpu(),\n",
    "    'W_eigenvalues': eigenvalues,\n",
    "    'W_eigenvectors': eigenvectors,\n",
    "}, output_path)\n",
    "\n",
    "print(f\"✓ Saved to {output_path}\")\n",
    "print(f\"  Size: {output_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "print()\n",
    "print(f\"Saved data:\")\n",
    "print(f\"  cluster_centroid: {cluster_centroid.shape} (geometric center of 2,212 cluster tokens)\")\n",
    "print(f\"  W_eigenvalues: {eigenvalues.shape} (PCA eigenvalues for full vocabulary)\")\n",
    "print(f\"  W_eigenvectors: {eigenvectors.shape} (PCA basis vectors for full vocabulary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Verify Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying saved data...\n",
      "\n",
      "✓ Verification passed\n",
      "  Centroid: torch.Size([2560])\n",
      "  Eigenvalues: torch.Size([2560])\n",
      "  Eigenvectors: torch.Size([2560, 2560])\n",
      "\n",
      "All checks passed! Reference frame is ready for use.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerifying saved data...\\n\")\n",
    "\n",
    "# Load back\n",
    "verification = load_file(output_path)\n",
    "loaded_centroid = verification['cluster_centroid']\n",
    "loaded_eigenvalues = verification['W_eigenvalues']\n",
    "loaded_eigenvectors = verification['W_eigenvectors']\n",
    "\n",
    "# Verify shapes\n",
    "assert loaded_centroid.shape == (d,), f\"Centroid shape mismatch: {loaded_centroid.shape}\"\n",
    "assert loaded_eigenvalues.shape == (d,), f\"Eigenvalues shape mismatch: {loaded_eigenvalues.shape}\"\n",
    "assert loaded_eigenvectors.shape == (d, d), f\"Eigenvectors shape mismatch: {loaded_eigenvectors.shape}\"\n",
    "\n",
    "# Verify values match\n",
    "assert torch.allclose(loaded_centroid, cluster_centroid.cpu()), \"Centroid values don't match\"\n",
    "assert torch.allclose(loaded_eigenvalues, eigenvalues), \"Eigenvalues don't match\"\n",
    "assert torch.allclose(loaded_eigenvectors, eigenvectors), \"Eigenvectors don't match\"\n",
    "\n",
    "print(f\"✓ Verification passed\")\n",
    "print(f\"  Centroid: {loaded_centroid.shape}\")\n",
    "print(f\"  Eigenvalues: {loaded_eigenvalues.shape}\")\n",
    "print(f\"  Eigenvectors: {loaded_eigenvectors.shape}\")\n",
    "print()\n",
    "print(f\"All checks passed! Reference frame is ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've established a cluster-centric reference frame for studying the surrounding token distribution:\n",
    "\n",
    "**Saved data:**\n",
    "- `cluster_centroid` (2560,) - geometric center of the 2,212-token cluster\n",
    "- `W_eigenvalues` (2560,) - principal component variances\n",
    "- `W_eigenvectors` (2560, 2560) - principal component directions\n",
    "\n",
    "**Next steps:**\n",
    "- **1.6b:** View universe from cluster perspective (Mollweide + polar projections)\n",
    "- **1.6c:** Test isotropy - is the surrounding distribution uniform?\n",
    "- **1.6d:** Compare cluster-centric view to global view - is the cluster special?\n",
    "\n",
    "The cluster centroid becomes our new origin. Any anisotropies or structure in the surrounding tokens will be visible as deviations from spherical symmetry."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
