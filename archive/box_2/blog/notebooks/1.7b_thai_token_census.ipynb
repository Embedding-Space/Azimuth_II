{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7b: Thai Token Census\n",
    "\n",
    "This notebook counts all Thai tokens in the full vocabulary to contextualize the cluster.\n",
    "\n",
    "## The Question\n",
    "\n",
    "We found that 71.4% of the cluster (1,579 tokens) are Thai script. But how does this compare to the full vocabulary?\n",
    "\n",
    "**Is the cluster unusual, or is Thai just heavily represented in the tokenizer?**\n",
    "\n",
    "Possible scenarios:\n",
    "1. **Cluster is anomalous:** Thai is ~1-2% of full vocabulary → 71% in cluster is extreme concentration\n",
    "2. **Thai is overrepresented:** Thai is ~20-30% of vocabulary → cluster reflects overall distribution\n",
    "3. **Cluster captures most Thai:** Thai is ~1% of vocabulary → cluster contains >50% of all Thai tokens\n",
    "\n",
    "## Method\n",
    "\n",
    "We'll:\n",
    "1. Decode the entire vocabulary (all 151,936 tokens)\n",
    "2. Classify each token by script (reuse 1.7a's classifier)\n",
    "3. Count Thai tokens across full vocabulary\n",
    "4. Compare cluster Thai tokens to full vocabulary Thai tokens\n",
    "5. Compute percentages and ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to analyze\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\"\n",
    "HF_MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from Qwen/Qwen3-4B-Instruct-2507...\n",
      "\n",
      "✓ Tokenizer loaded\n",
      "  Vocabulary size: 151,669 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tokenizer from {HF_MODEL_NAME}...\\n\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
    "vocab_size = len(tokenizer)\n",
    "print(f\"✓ Tokenizer loaded\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cluster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 2,212 cluster token IDs\n"
     ]
    }
   ],
   "source": [
    "# Load cluster tokens from 1.4h\n",
    "tensor_path = Path(f\"../tensors/{MODEL_NAME}/1.4h_cluster_tokens.safetensors\")\n",
    "data = load_file(tensor_path)\n",
    "cluster_token_ids = set(data['cluster_token_ids'].tolist())\n",
    "\n",
    "print(f\"\\nLoaded {len(cluster_token_ids):,} cluster token IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function defined: classify_script(text)\n"
     ]
    }
   ],
   "source": [
    "def classify_script(text):\n",
    "    \"\"\"\n",
    "    Classify text by dominant script using Unicode properties.\n",
    "    Returns primary script name (e.g., 'Thai', 'Han', 'Latin').\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return 'Empty/Whitespace'\n",
    "    \n",
    "    # Count characters by script\n",
    "    script_counts = Counter()\n",
    "    \n",
    "    for char in text:\n",
    "        # Skip whitespace and control characters for classification\n",
    "        if char.isspace() or unicodedata.category(char).startswith('C'):\n",
    "            continue\n",
    "            \n",
    "        # Get Unicode script name\n",
    "        try:\n",
    "            script = unicodedata.name(char).split()[0]\n",
    "            script_counts[script] += 1\n",
    "        except (ValueError, IndexError):\n",
    "            # Handle characters without names or unnamed characters\n",
    "            category = unicodedata.category(char)\n",
    "            if category.startswith('P'):\n",
    "                script_counts['Punctuation'] += 1\n",
    "            elif category.startswith('S'):\n",
    "                script_counts['Symbol'] += 1\n",
    "            else:\n",
    "                script_counts['Other'] += 1\n",
    "    \n",
    "    if not script_counts:\n",
    "        return 'Empty/Whitespace'\n",
    "    \n",
    "    # Return most common script\n",
    "    return script_counts.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "print(\"Helper function defined: classify_script(text)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode and Classify All Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoding and classifying all 151,669 tokens...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tokens: 100%|██████████| 151669/151669 [00:00<00:00, 207919.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processed 151,669 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nDecoding and classifying all {vocab_size:,} tokens...\\n\")\n",
    "\n",
    "# Build classification for every token\n",
    "records = []\n",
    "\n",
    "for token_id in tqdm(range(vocab_size), desc=\"Processing tokens\"):\n",
    "    # Decode\n",
    "    decoded = tokenizer.decode([token_id])\n",
    "    \n",
    "    # Classify script\n",
    "    script = classify_script(decoded)\n",
    "    \n",
    "    # Check if in cluster\n",
    "    in_cluster = token_id in cluster_token_ids\n",
    "    \n",
    "    records.append({\n",
    "        'token_id': token_id,\n",
    "        'decoded': decoded,\n",
    "        'script': script,\n",
    "        'in_cluster': in_cluster,\n",
    "    })\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "print(f\"\\n✓ Processed {len(df):,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thai Token Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "THAI TOKEN CENSUS\n",
      "======================================================================\n",
      "\n",
      "Full vocabulary:\n",
      "  Total tokens: 151,669\n",
      "  Thai tokens: 2,571 (1.70%)\n",
      "  Non-Thai tokens: 149,098 (98.30%)\n",
      "\n",
      "Cluster (2,212 tokens):\n",
      "  Thai tokens: 1,579 (81.18%)\n",
      "  Non-Thai tokens: 366 (18.82%)\n",
      "\n",
      "Non-cluster tokens (149,724 tokens):\n",
      "  Thai tokens: 992 (0.66%)\n",
      "  Non-Thai tokens: 148,732 (99.34%)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THAI TOKEN CENSUS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Overall counts\n",
    "total_tokens = len(df)\n",
    "total_thai = (df['script'] == 'THAI').sum()\n",
    "total_cluster = df['in_cluster'].sum()\n",
    "\n",
    "print(f\"\\nFull vocabulary:\")\n",
    "print(f\"  Total tokens: {total_tokens:,}\")\n",
    "print(f\"  Thai tokens: {total_thai:,} ({100*total_thai/total_tokens:.2f}%)\")\n",
    "print(f\"  Non-Thai tokens: {total_tokens - total_thai:,} ({100*(total_tokens-total_thai)/total_tokens:.2f}%)\")\n",
    "\n",
    "# Cluster counts\n",
    "cluster_thai = ((df['script'] == 'THAI') & df['in_cluster']).sum()\n",
    "cluster_non_thai = (df['in_cluster'] & (df['script'] != 'THAI')).sum()\n",
    "\n",
    "print(f\"\\nCluster (2,212 tokens):\")\n",
    "print(f\"  Thai tokens: {cluster_thai:,} ({100*cluster_thai/total_cluster:.2f}%)\")\n",
    "print(f\"  Non-Thai tokens: {cluster_non_thai:,} ({100*cluster_non_thai/total_cluster:.2f}%)\")\n",
    "\n",
    "# Non-cluster counts\n",
    "non_cluster_thai = ((df['script'] == 'THAI') & ~df['in_cluster']).sum()\n",
    "non_cluster_total = (~df['in_cluster']).sum()\n",
    "\n",
    "print(f\"\\nNon-cluster tokens ({non_cluster_total:,} tokens):\")\n",
    "print(f\"  Thai tokens: {non_cluster_thai:,} ({100*non_cluster_thai/non_cluster_total:.2f}%)\")\n",
    "print(f\"  Non-Thai tokens: {non_cluster_total - non_cluster_thai:,} ({100*(non_cluster_total-non_cluster_thai)/non_cluster_total:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Ratios and Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "KEY RATIOS\n",
      "======================================================================\n",
      "\n",
      "Thai token capture rate:\n",
      "  1,579 of 2,571 Thai tokens are in cluster\n",
      "  = 61.4% of all Thai tokens\n",
      "\n",
      "Thai enrichment in cluster:\n",
      "  Cluster: 81.2% Thai\n",
      "  Full vocabulary: 1.70% Thai\n",
      "  Enrichment factor: 47.9×\n",
      "  (Cluster is 47.9× more Thai-dense than expected)\n",
      "\n",
      "Cluster vs non-cluster:\n",
      "  Cluster Thai: 81.2%\n",
      "  Non-cluster Thai: 0.66%\n",
      "  Ratio: 122.5×\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY RATIOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# What fraction of all Thai tokens are in the cluster?\n",
    "thai_capture_rate = 100 * cluster_thai / total_thai\n",
    "print(f\"\\nThai token capture rate:\")\n",
    "print(f\"  {cluster_thai:,} of {total_thai:,} Thai tokens are in cluster\")\n",
    "print(f\"  = {thai_capture_rate:.1f}% of all Thai tokens\")\n",
    "\n",
    "# Enrichment factor\n",
    "thai_pct_cluster = 100 * cluster_thai / total_cluster\n",
    "thai_pct_vocab = 100 * total_thai / total_tokens\n",
    "enrichment = thai_pct_cluster / thai_pct_vocab\n",
    "\n",
    "print(f\"\\nThai enrichment in cluster:\")\n",
    "print(f\"  Cluster: {thai_pct_cluster:.1f}% Thai\")\n",
    "print(f\"  Full vocabulary: {thai_pct_vocab:.2f}% Thai\")\n",
    "print(f\"  Enrichment factor: {enrichment:.1f}×\")\n",
    "print(f\"  (Cluster is {enrichment:.1f}× more Thai-dense than expected)\")\n",
    "\n",
    "# Comparison to non-cluster\n",
    "non_cluster_thai_pct = 100 * non_cluster_thai / non_cluster_total\n",
    "cluster_vs_non_cluster = thai_pct_cluster / non_cluster_thai_pct\n",
    "\n",
    "print(f\"\\nCluster vs non-cluster:\")\n",
    "print(f\"  Cluster Thai: {thai_pct_cluster:.1f}%\")\n",
    "print(f\"  Non-cluster Thai: {non_cluster_thai_pct:.2f}%\")\n",
    "print(f\"  Ratio: {cluster_vs_non_cluster:.1f}×\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCRIPT DISTRIBUTION: CLUSTER vs FULL VOCABULARY\n",
      "======================================================================\n",
      "\n",
      "Script                 Full Vocab      Cluster   Enrichment\n",
      "----------------------------------------------------------------------\n",
      "LATIN                      61.83%         1.3%         0.0×\n",
      "CJK                        17.01%        10.2%         0.6×\n",
      "CYRILLIC                    2.74%         0.0%         0.0×\n",
      "ARABIC                      2.64%         3.9%         1.5×\n",
      "HANGUL                      2.36%         0.0%         0.0×\n",
      "HEBREW                      2.10%         1.3%         0.6×\n",
      "THAI                        1.70%        81.2%        47.9×\n",
      "REPLACEMENT                 0.91%         0.9%         1.0×\n",
      "HIRAGANA                    0.78%         0.0%         0.0×\n",
      "RIGHT                       0.76%         0.0%         0.0×\n",
      "LEFT                        0.54%         0.0%         0.0×\n",
      "KATAKANA                    0.39%         0.0%         0.0×\n",
      "QUOTATION                   0.34%         0.0%         0.0×\n",
      "Empty/Whitespace            0.32%         0.0%         0.0×\n",
      "APOSTROPHE                  0.30%         0.0%         0.0×\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCRIPT DISTRIBUTION: CLUSTER vs FULL VOCABULARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get top 15 scripts from full vocabulary\n",
    "vocab_script_counts = df['script'].value_counts().head(15)\n",
    "cluster_script_counts = df[df['in_cluster']]['script'].value_counts()\n",
    "\n",
    "print(f\"\\n{'Script':<20} {'Full Vocab':>12} {'Cluster':>12} {'Enrichment':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for script in vocab_script_counts.index:\n",
    "    vocab_count = vocab_script_counts[script]\n",
    "    vocab_pct = 100 * vocab_count / total_tokens\n",
    "    \n",
    "    cluster_count = cluster_script_counts.get(script, 0)\n",
    "    cluster_pct = 100 * cluster_count / total_cluster\n",
    "    \n",
    "    if vocab_pct > 0:\n",
    "        enrichment = cluster_pct / vocab_pct\n",
    "    else:\n",
    "        enrichment = 0\n",
    "    \n",
    "    print(f\"{script:<20} {vocab_pct:>11.2f}% {cluster_pct:>11.1f}% {enrichment:>11.1f}×\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "This census tells us whether the cluster is:\n",
    "\n",
    "**Scenario 1: Extreme concentration**\n",
    "- Thai is <5% of full vocabulary\n",
    "- Cluster captures >50% of all Thai tokens\n",
    "- Enrichment factor >10×\n",
    "- **Interpretation:** Cluster is a Thai token graveyard—most Thai tokens are dead\n",
    "\n",
    "**Scenario 2: Representative sample**\n",
    "- Thai is 20-30% of full vocabulary\n",
    "- Cluster captures ~5% of Thai tokens\n",
    "- Enrichment factor ~2-3×\n",
    "- **Interpretation:** Cluster is slightly enriched for Thai, but not dramatically\n",
    "\n",
    "**Scenario 3: Modest overrepresentation**\n",
    "- Thai is 2-5% of full vocabulary\n",
    "- Cluster captures 20-40% of Thai tokens\n",
    "- Enrichment factor 5-8×\n",
    "- **Interpretation:** Thai tokens disproportionately end up in dead cluster\n",
    "\n",
    "The enrichment factor and capture rate will tell us whether this is a Thai-specific phenomenon or a more general vocabulary issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
