{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4h: Extract Cluster Tokens\n",
    "\n",
    "This notebook identifies and saves the token IDs belonging to the dense cluster.\n",
    "\n",
    "## The Question\n",
    "\n",
    "We've identified a giant connected component of 2,212 tokens (1.4g) that forms a tight cluster with diameter ~0.0016, separated by a void from 18,161 ambient singletons.\n",
    "\n",
    "Now we need to **identify these tokens by their vocabulary IDs** so we can:\n",
    "- Save them for future analysis\n",
    "- Decode them to see what text they represent\n",
    "- Visualize them on sky maps to confirm spatial coherence\n",
    "- Analyze their internal structure\n",
    "\n",
    "## Method\n",
    "\n",
    "We'll:\n",
    "1. Rebuild the adjacency graph (same threshold as 1.4g)\n",
    "2. Find connected components\n",
    "3. Extract token IDs for the giant cluster\n",
    "4. Save to disk\n",
    "5. Verify the saved data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to analyze\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# Adjacency threshold (must match 1.4g)\n",
    "ADJACENCY_THRESHOLD = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file, save_file\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.csgraph import connected_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded distance matrix from ../tensors/Qwen3-4B-Instruct-2507/1.4b_overdensity_distances.safetensors\n",
      "  Shape: torch.Size([20373, 20373])\n",
      "  Spike tokens: 20,373\n"
     ]
    }
   ],
   "source": [
    "# Load distances and token IDs from 1.4b\n",
    "tensor_path = Path(f\"../tensors/{MODEL_NAME}/1.4b_overdensity_distances.safetensors\")\n",
    "data = load_file(tensor_path)\n",
    "dists = data['distances']\n",
    "spike_token_ids = data['spike_token_ids']\n",
    "\n",
    "print(f\"Loaded distance matrix from {tensor_path}\")\n",
    "print(f\"  Shape: {dists.shape}\")\n",
    "print(f\"  Spike tokens: {len(spike_token_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Adjacency Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building adjacency matrix with threshold = 0.002...\n",
      "\n",
      "✓ Adjacency matrix created\n",
      "  Nodes: 20,373\n",
      "  Edges: 2,445,366\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBuilding adjacency matrix with threshold = {ADJACENCY_THRESHOLD}...\\n\")\n",
    "\n",
    "# Create binary adjacency matrix\n",
    "adjacency = (dists < ADJACENCY_THRESHOLD).float()\n",
    "adjacency.fill_diagonal_(0)\n",
    "\n",
    "num_edges = adjacency.sum().item() // 2\n",
    "\n",
    "print(f\"✓ Adjacency matrix created\")\n",
    "print(f\"  Nodes: {len(adjacency):,}\")\n",
    "print(f\"  Edges: {int(num_edges):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding connected components...\n",
      "\n",
      "✓ Found 18,162 connected components\n",
      "\n",
      "Largest component: 2,212 tokens\n",
      "Singletons: 18,161\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFinding connected components...\\n\")\n",
    "\n",
    "# Convert to scipy sparse matrix\n",
    "adjacency_sparse = sp.csr_matrix(adjacency.numpy())\n",
    "\n",
    "# Find connected components\n",
    "n_components, labels = connected_components(\n",
    "    csgraph=adjacency_sparse, \n",
    "    directed=False, \n",
    "    return_labels=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Found {n_components:,} connected components\")\n",
    "\n",
    "# Count component sizes\n",
    "component_sizes = Counter(labels)\n",
    "sorted_components = sorted(component_sizes.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nLargest component: {sorted_components[0][1]:,} tokens\")\n",
    "print(f\"Singletons: {sum(1 for _, size in sorted_components if size == 1):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Giant Cluster Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting giant cluster token IDs...\n",
      "\n",
      "Giant cluster ID: 0\n",
      "Giant cluster size: 2,212 tokens\n",
      "\n",
      "✓ Extracted 2,212 cluster token IDs\n",
      "  Token ID range: [124, 151935]\n",
      "  First 20 IDs: [124, 125, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 77150, 80091, 83971, 119346, 119347, 119348, 119349]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nExtracting giant cluster token IDs...\\n\")\n",
    "\n",
    "# Get the giant cluster ID (largest component)\n",
    "giant_cluster_id = sorted_components[0][0]\n",
    "giant_cluster_size = sorted_components[0][1]\n",
    "\n",
    "print(f\"Giant cluster ID: {giant_cluster_id}\")\n",
    "print(f\"Giant cluster size: {giant_cluster_size:,} tokens\")\n",
    "print()\n",
    "\n",
    "# Create boolean mask: True for tokens in the giant cluster\n",
    "cluster_mask = (labels == giant_cluster_id)\n",
    "\n",
    "# Extract token IDs (these are indices into the full 151,936 vocabulary)\n",
    "cluster_token_ids = spike_token_ids[cluster_mask]\n",
    "\n",
    "print(f\"✓ Extracted {len(cluster_token_ids):,} cluster token IDs\")\n",
    "print(f\"  Token ID range: [{cluster_token_ids.min().item()}, {cluster_token_ids.max().item()}]\")\n",
    "print(f\"  First 20 IDs: {cluster_token_ids[:20].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cluster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving cluster data...\n",
      "\n",
      "✓ Saved to ../tensors/Qwen3-4B-Instruct-2507/1.4h_cluster_tokens.safetensors\n",
      "  Size: 37.4 KB\n",
      "\n",
      "Saved data:\n",
      "  cluster_token_ids: 2,212 token IDs in full vocabulary\n",
      "  cluster_mask: 2,212 True values (mask into spike_token_ids)\n",
      "  adjacency_threshold: 0.002 (for reference)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving cluster data...\\n\")\n",
    "\n",
    "# Prepare output\n",
    "output_dir = Path(f\"../tensors/{MODEL_NAME}\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"1.4h_cluster_tokens.safetensors\"\n",
    "\n",
    "# Save both token IDs and mask\n",
    "save_file({\n",
    "    'cluster_token_ids': cluster_token_ids.cpu(),\n",
    "    'cluster_mask': torch.tensor(cluster_mask, dtype=torch.bool),\n",
    "    'adjacency_threshold': torch.tensor([ADJACENCY_THRESHOLD], dtype=torch.float32),\n",
    "}, output_path)\n",
    "\n",
    "print(f\"✓ Saved to {output_path}\")\n",
    "print(f\"  Size: {output_path.stat().st_size / 1024:.1f} KB\")\n",
    "print()\n",
    "print(f\"Saved data:\")\n",
    "print(f\"  cluster_token_ids: {len(cluster_token_ids):,} token IDs in full vocabulary\")\n",
    "print(f\"  cluster_mask: {cluster_mask.sum():,} True values (mask into spike_token_ids)\")\n",
    "print(f\"  adjacency_threshold: {ADJACENCY_THRESHOLD} (for reference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying saved data...\n",
      "\n",
      "✓ Verification passed\n",
      "  Token IDs: 2,212\n",
      "  Mask True count: 2,212\n",
      "  Threshold: 0.0020000000949949026\n",
      "\n",
      "All checks passed! Data is ready for future analysis.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nVerifying saved data...\\n\")\n",
    "\n",
    "# Load back\n",
    "verification = load_file(output_path)\n",
    "loaded_ids = verification['cluster_token_ids']\n",
    "loaded_mask = verification['cluster_mask']\n",
    "loaded_threshold = verification['adjacency_threshold']\n",
    "\n",
    "# Verify counts match\n",
    "assert len(loaded_ids) == giant_cluster_size, f\"Count mismatch: {len(loaded_ids)} != {giant_cluster_size}\"\n",
    "assert loaded_mask.sum().item() == giant_cluster_size, f\"Mask count mismatch\"\n",
    "assert torch.all(loaded_ids == cluster_token_ids), \"Token IDs don't match\"\n",
    "\n",
    "# Use approximate equality for float comparison\n",
    "assert torch.allclose(loaded_threshold, torch.tensor([ADJACENCY_THRESHOLD], dtype=torch.float32)), \\\n",
    "    f\"Threshold doesn't match: {loaded_threshold.item()} != {ADJACENCY_THRESHOLD}\"\n",
    "\n",
    "print(f\"✓ Verification passed\")\n",
    "print(f\"  Token IDs: {len(loaded_ids):,}\")\n",
    "print(f\"  Mask True count: {loaded_mask.sum().item():,}\")\n",
    "print(f\"  Threshold: {loaded_threshold.item()}\")\n",
    "print()\n",
    "print(f\"All checks passed! Data is ready for future analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've successfully identified and saved the dense cluster tokens:\n",
    "\n",
    "- **Cluster size**: 2,212 tokens\n",
    "- **Diameter**: ~0.0016 (from 1.4g)\n",
    "- **Void boundary**: ~0.0036 (from 1.4g)\n",
    "- **Saved to**: `../tensors/{MODEL_NAME}/1.4h_cluster_tokens.safetensors`\n",
    "\n",
    "**What's included:**\n",
    "- `cluster_token_ids`: Token IDs in the full 151,936 vocabulary\n",
    "- `cluster_mask`: Boolean mask (True for cluster tokens) into the 20,373 spike tokens\n",
    "- `adjacency_threshold`: The 0.002 threshold used for clustering\n",
    "\n",
    "**Next steps:**\n",
    "- Decode these tokens to see what text they represent\n",
    "- Visualize them on sky maps (1.3a/1.3b) with different colors\n",
    "- Analyze their internal structure (are the black holes within this cluster?)\n",
    "- Compare to other models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
