{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1.4a: Identify Spike Tokens\n",
    "\n",
    "This notebook identifies which tokens belong to the low-norm overdensity (\"the spike\") by filtering based on spherical coordinates.\n",
    "\n",
    "## The Question\n",
    "\n",
    "We've established that there's a concentrated overdensity of tokens near the origin at specific coordinates. But **which tokens** are in this cluster?\n",
    "\n",
    "Once we have the token IDs, we can:\n",
    "- Decode them to see what text they represent\n",
    "- Look for patterns (language scripts, special characters, rare tokens)\n",
    "- Understand why these tokens are clustered together\n",
    "\n",
    "## Method\n",
    "\n",
    "We'll:\n",
    "1. Compute spherical coordinates (same as 1.3a/1.3b)\n",
    "2. Define a bounding box around the overdensity in (latitude, longitude, radius) space\n",
    "3. Filter tokens that fall within this box\n",
    "4. Extract their token IDs and decode them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to analyze\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\"\n",
    "MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"  # For tokenizer\n",
    "\n",
    "# PCA basis selection (match 1.3a/1.3b)\n",
    "NORTH_PC = 2\n",
    "MERIDIAN_PC = 1\n",
    "EQUINOX_PC = 3\n",
    "\n",
    "# Overdensity bounding box (adjust based on visual inspection)\n",
    "LAT_MIN = -15    # degrees\n",
    "LAT_MAX = 5      # degrees\n",
    "LON_MIN = -10    # degrees\n",
    "LON_MAX = 20     # degrees\n",
    "R_MIN = 0.2      # distance units\n",
    "R_MAX = 0.5      # distance units\n",
    "\n",
    "# How many example tokens to decode\n",
    "NUM_EXAMPLES = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Load W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W from ../tensors/Qwen3-4B-Instruct-2507/W.safetensors\n",
      "  Shape: torch.Size([151936, 2560])\n",
      "  Dtype: torch.bfloat16\n",
      "\n",
      "Token space: 151,936 tokens in 2,560 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Load W in bfloat16\n",
    "tensor_path = Path(f\"../tensors/{MODEL_NAME}/W.safetensors\")\n",
    "W_bf16 = load_file(tensor_path)[\"W\"]\n",
    "\n",
    "print(f\"Loaded W from {tensor_path}\")\n",
    "print(f\"  Shape: {W_bf16.shape}\")\n",
    "print(f\"  Dtype: {W_bf16.dtype}\")\n",
    "\n",
    "# Convert to float32\n",
    "W = W_bf16.to(torch.float32)\n",
    "\n",
    "N, d = W.shape\n",
    "print(f\"\\nToken space: {N:,} tokens in {d:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Compute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PCA...\n",
      "✓ PCA computed\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing PCA...\")\n",
    "\n",
    "W_centered = W - W.mean(dim=0)\n",
    "cov = (W_centered.T @ W_centered) / N\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
    "\n",
    "# Sort descending\n",
    "idx = torch.argsort(eigenvalues, descending=True)\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "print(\"✓ PCA computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Define Spherical Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spherical basis defined:\n",
      "  North: PC2\n",
      "  Meridian: PC1\n",
      "  Equinox: PC3\n"
     ]
    }
   ],
   "source": [
    "def get_pc_vector(pcs, index):\n",
    "    \"\"\"Get PC vector by index (1-indexed), with sign flip for negative indices.\"\"\"\n",
    "    pc_num = abs(index) - 1\n",
    "    vector = pcs[:, pc_num].clone()\n",
    "    if index < 0:\n",
    "        vector = -vector\n",
    "    return vector\n",
    "\n",
    "north = get_pc_vector(eigenvectors, NORTH_PC)\n",
    "meridian = get_pc_vector(eigenvectors, MERIDIAN_PC)\n",
    "equinox = get_pc_vector(eigenvectors, EQUINOX_PC)\n",
    "\n",
    "print(\"Spherical basis defined:\")\n",
    "print(f\"  North: PC{NORTH_PC}\")\n",
    "print(f\"  Meridian: PC{MERIDIAN_PC}\")\n",
    "print(f\"  Equinox: PC{EQUINOX_PC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Project to Spherical Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projecting to spherical coordinates...\n",
      "\n",
      "✓ Spherical coordinates computed\n"
     ]
    }
   ],
   "source": [
    "print(\"Projecting to spherical coordinates...\\n\")\n",
    "\n",
    "# Project onto basis\n",
    "x = W @ meridian\n",
    "y = W @ equinox\n",
    "z = W @ north\n",
    "\n",
    "# Spherical coordinates\n",
    "r = torch.sqrt(x**2 + y**2 + z**2)\n",
    "lat_rad = torch.asin(torch.clamp(z / r, -1, 1))\n",
    "lat_deg = torch.rad2deg(lat_rad)\n",
    "lon_rad = torch.atan2(y, x)\n",
    "lon_deg = torch.rad2deg(lon_rad)\n",
    "\n",
    "print(\"✓ Spherical coordinates computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Filter by Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering tokens in bounding box:\n",
      "  Latitude: [-15°, 5°]\n",
      "  Longitude: [-10°, 20°]\n",
      "  Radius: [0.2, 0.5]\n",
      "\n",
      "✓ Found 20,373 tokens in overdensity\n",
      "  (13.41% of vocabulary)\n",
      "\n",
      "First 20 token IDs: [124, 125, 127, 128, 129, 138, 140, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Filtering tokens in bounding box:\")\n",
    "print(f\"  Latitude: [{LAT_MIN}°, {LAT_MAX}°]\")\n",
    "print(f\"  Longitude: [{LON_MIN}°, {LON_MAX}°]\")\n",
    "print(f\"  Radius: [{R_MIN}, {R_MAX}]\")\n",
    "print()\n",
    "\n",
    "# Create mask\n",
    "mask = (\n",
    "    (lat_deg >= LAT_MIN) & (lat_deg <= LAT_MAX) &\n",
    "    (lon_deg >= LON_MIN) & (lon_deg <= LON_MAX) &\n",
    "    (r >= R_MIN) & (r <= R_MAX)\n",
    ")\n",
    "\n",
    "# Extract token IDs\n",
    "spike_token_ids = torch.where(mask)[0]\n",
    "\n",
    "print(f\"✓ Found {len(spike_token_ids):,} tokens in overdensity\")\n",
    "print(f\"  ({len(spike_token_ids)/N*100:.2f}% of vocabulary)\")\n",
    "print()\n",
    "print(f\"First 20 token IDs: {spike_token_ids[:20].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: Qwen/Qwen3-4B-Instruct-2507\n",
      "\n",
      "✓ Tokenizer loaded (vocab size: 151,669)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tokenizer: {MODEL_ID}\\n\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "print(f\"✓ Tokenizer loaded (vocab size: {len(tokenizer):,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Examine Sample Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoding first 50 spike tokens:\n",
      "\n",
      "Token ID   Repr                           Description\n",
      "----------------------------------------------------------------------\n",
      "124        '�'                            Other Unicode\n",
      "125        '�'                            Other Unicode\n",
      "127        '�'                            Other Unicode\n",
      "128        '�'                            Other Unicode\n",
      "129        '�'                            Other Unicode\n",
      "138        '�'                            Other Unicode\n",
      "140        '�'                            Other Unicode\n",
      "175        '�'                            Other Unicode\n",
      "176        '�'                            Other Unicode\n",
      "177        '�'                            Other Unicode\n",
      "178        '�'                            Other Unicode\n",
      "179        '�'                            Other Unicode\n",
      "180        '�'                            Other Unicode\n",
      "181        '�'                            Other Unicode\n",
      "182        '�'                            Other Unicode\n",
      "183        '�'                            Other Unicode\n",
      "184        '�'                            Other Unicode\n",
      "185        '�'                            Other Unicode\n",
      "186        '�'                            Other Unicode\n",
      "187        '�'                            Other Unicode\n",
      "200        '\\x0c'                         (whitespace/empty)\n",
      "561        'tring'                        ASCII\n",
      "710        'alse'                         ASCII\n",
      "885        '========'                     ASCII\n",
      "985        'indow'                        ASCII\n",
      "1275       'rror'                         ASCII\n",
      "1277       '�'                            Other Unicode\n",
      "1286       'olumn'                        ASCII\n",
      "1428       '();\\n\\n'                      ASCII\n",
      "1485       'clud'                         ASCII\n",
      "1491       'indows'                       ASCII\n",
      "1540       '�'                            Other Unicode\n",
      "1545       'icense'                       ASCII\n",
      "1559       'itial'                        ASCII\n",
      "1680       'amespace'                     ASCII\n",
      "1689       '***************************   ASCII\n",
      "1738       'olean'                        ASCII\n",
      "1768       'vice'                         ASCII\n",
      "2006       'idget'                        ASCII\n",
      "2029       '.Forms'                       ASCII\n",
      "2065       'tings'                        ASCII\n",
      "2094       'ailable'                      ASCII\n",
      "2206       'andom'                        ASCII\n",
      "2262       '({\\n'                         ASCII\n",
      "2344       'ialog'                        ASCII\n",
      "2366       'quals'                        ASCII\n",
      "2443       'ogle'                         ASCII\n",
      "2457       '++)'                          ASCII\n",
      "2483       'gress'                        ASCII\n",
      "2495       '!--'                          ASCII\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nDecoding first {NUM_EXAMPLES} spike tokens:\\n\")\n",
    "print(f\"{'Token ID':<10} {'Repr':<30} {'Description'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, tid in enumerate(spike_token_ids[:NUM_EXAMPLES]):\n",
    "    tid_int = tid.item()\n",
    "    \n",
    "    # Decode token\n",
    "    try:\n",
    "        token_str = tokenizer.decode([tid_int])\n",
    "        token_repr = repr(token_str)[:28]  # Truncate for display\n",
    "        \n",
    "        # Classify\n",
    "        if token_str.strip() == '':\n",
    "            desc = \"(whitespace/empty)\"\n",
    "        elif all(ord(c) < 128 for c in token_str):\n",
    "            desc = \"ASCII\"\n",
    "        elif any('\\u0e00' <= c <= '\\u0e7f' for c in token_str):\n",
    "            desc = \"Thai\"\n",
    "        elif any('\\u4e00' <= c <= '\\u9fff' for c in token_str):\n",
    "            desc = \"CJK\"\n",
    "        else:\n",
    "            desc = \"Other Unicode\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        token_repr = f\"(decode error: {e})\"\n",
    "        desc = \"Error\"\n",
    "    \n",
    "    print(f\"{tid_int:<10} {token_repr:<30} {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Character Type Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing character types across all spike tokens...\n",
      "\n",
      "Category             Count      Percentage\n",
      "--------------------------------------------------\n",
      "ASCII                9337        45.83%\n",
      "Other Unicode        6848        33.61%\n",
      "Thai                 1666         8.18%\n",
      "CJK                  1600         7.85%\n",
      "Whitespace/Empty     362          1.78%\n",
      "Arabic               329          1.61%\n",
      "Cyrillic             231          1.13%\n",
      "\n",
      "Total spike tokens: 20,373\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnalyzing character types across all spike tokens...\\n\")\n",
    "\n",
    "categories = []\n",
    "\n",
    "for tid in spike_token_ids:\n",
    "    tid_int = tid.item()\n",
    "    try:\n",
    "        token_str = tokenizer.decode([tid_int])\n",
    "        \n",
    "        if token_str.strip() == '':\n",
    "            categories.append(\"Whitespace/Empty\")\n",
    "        elif all(ord(c) < 128 for c in token_str):\n",
    "            categories.append(\"ASCII\")\n",
    "        elif any('\\u0e00' <= c <= '\\u0e7f' for c in token_str):\n",
    "            categories.append(\"Thai\")\n",
    "        elif any('\\u4e00' <= c <= '\\u9fff' for c in token_str):\n",
    "            categories.append(\"CJK\")\n",
    "        elif any('\\u0600' <= c <= '\\u06ff' for c in token_str):\n",
    "            categories.append(\"Arabic\")\n",
    "        elif any('\\u0400' <= c <= '\\u04ff' for c in token_str):\n",
    "            categories.append(\"Cyrillic\")\n",
    "        else:\n",
    "            categories.append(\"Other Unicode\")\n",
    "    except:\n",
    "        categories.append(\"Decode Error\")\n",
    "\n",
    "# Count and display\n",
    "category_counts = Counter(categories)\n",
    "\n",
    "print(f\"{'Category':<20} {'Count':<10} {'Percentage'}\")\n",
    "print(\"-\" * 50)\n",
    "for category, count in category_counts.most_common():\n",
    "    pct = count / len(spike_token_ids) * 100\n",
    "    print(f\"{category:<20} {count:<10} {pct:>6.2f}%\")\n",
    "\n",
    "print()\n",
    "print(f\"Total spike tokens: {len(spike_token_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've identified the tokens in the low-norm overdensity by filtering based on spherical coordinates.\n",
    "\n",
    "**Key observations to look for:**\n",
    "- Are these tokens from a specific language or script?\n",
    "- Are they rare/unused tokens?\n",
    "- Do they have something in common semantically?\n",
    "\n",
    "This bridges from geometric analysis (\"there's a cluster\") to semantic analysis (\"the cluster contains Thai script tokens\")."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
