{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4b: Pairwise Distances\n",
    "\n",
    "This notebook computes pairwise L2 distances between tokens in the overdensity to reveal internal structure.\n",
    "\n",
    "## The Question\n",
    "\n",
    "We identified ~20,000 tokens in a spatial bounding box (1.4a), but this includes both:\n",
    "- **The tight cluster** we're interested in (the \"spike\")\n",
    "- **Normal tokens** that happen to pass through that region\n",
    "\n",
    "To separate them, we need to look at **density**—not just where tokens are, but how close they are to each other.\n",
    "\n",
    "## Method\n",
    "\n",
    "We'll:\n",
    "1. Extract vectors for all tokens in the bounded region\n",
    "2. Compute pairwise L2 distances between them\n",
    "3. Save the distance matrix for further analysis\n",
    "\n",
    "This distance matrix will reveal tokens that are extremely close together (or identical), indicating a truly dense cluster rather than normal token separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to analyze\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# PCA basis (must match 1.4a)\n",
    "NORTH_PC = 2\n",
    "MERIDIAN_PC = 1\n",
    "EQUINOX_PC = 3\n",
    "\n",
    "# Bounding box (must match 1.4a)\n",
    "LAT_MIN = -15\n",
    "LAT_MAX = 5\n",
    "LON_MIN = -10\n",
    "LON_MAX = 20\n",
    "R_MIN = 0.2\n",
    "R_MAX = 0.5\n",
    "\n",
    "# Distance computation\n",
    "CHUNK_SIZE = 40  # Process this many rows at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file, save_file\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Detect available device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load W and Compute Spherical Coordinates\n",
    "\n",
    "Same as 1.4a—we need to identify the same tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W: torch.Size([151936, 2560])\n"
     ]
    }
   ],
   "source": [
    "# Load W\n",
    "tensor_path = Path(f\"../tensors/{MODEL_NAME}/W.safetensors\")\n",
    "W_bf16 = load_file(tensor_path)[\"W\"]\n",
    "W = W_bf16.to(torch.float32)\n",
    "N, d = W.shape\n",
    "\n",
    "print(f\"Loaded W: {W.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PCA...\n",
      "✓ PCA computed\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "print(\"Computing PCA...\")\n",
    "W_centered = W - W.mean(dim=0)\n",
    "cov = (W_centered.T @ W_centered) / N\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
    "idx = torch.argsort(eigenvalues, descending=True)\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "print(\"✓ PCA computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basis\n",
    "def get_pc_vector(pcs, index):\n",
    "    pc_num = abs(index) - 1\n",
    "    vector = pcs[:, pc_num].clone()\n",
    "    if index < 0:\n",
    "        vector = -vector\n",
    "    return vector\n",
    "\n",
    "north = get_pc_vector(eigenvectors, NORTH_PC)\n",
    "meridian = get_pc_vector(eigenvectors, MERIDIAN_PC)\n",
    "equinox = get_pc_vector(eigenvectors, EQUINOX_PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing spherical coordinates...\n",
      "✓ Spherical coordinates computed\n"
     ]
    }
   ],
   "source": [
    "# Spherical coordinates\n",
    "print(\"Computing spherical coordinates...\")\n",
    "x = W @ meridian\n",
    "y = W @ equinox\n",
    "z = W @ north\n",
    "\n",
    "r = torch.sqrt(x**2 + y**2 + z**2)\n",
    "lat_rad = torch.asin(torch.clamp(z / r, -1, 1))\n",
    "lat_deg = torch.rad2deg(lat_rad)\n",
    "lon_rad = torch.atan2(y, x)\n",
    "lon_deg = torch.rad2deg(lon_rad)\n",
    "print(\"✓ Spherical coordinates computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Found 20,373 tokens in bounding box\n"
     ]
    }
   ],
   "source": [
    "# Filter by bounding box\n",
    "mask = (\n",
    "    (lat_deg >= LAT_MIN) & (lat_deg <= LAT_MAX) &\n",
    "    (lon_deg >= LON_MIN) & (lon_deg <= LON_MAX) &\n",
    "    (r >= R_MIN) & (r <= R_MAX)\n",
    ")\n",
    "\n",
    "spike_token_ids = torch.where(mask)[0]\n",
    "\n",
    "print(f\"\\n✓ Found {len(spike_token_ids):,} tokens in bounding box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Spike Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike vectors: torch.Size([20373, 2560])\n",
      "  20,373 tokens\n",
      "  2,560 dimensions\n",
      "  Device: mps:0\n"
     ]
    }
   ],
   "source": [
    "# Extract vectors for spike tokens and move to device\n",
    "spike_vecs = W[spike_token_ids].to(device)\n",
    "\n",
    "print(f\"Spike vectors: {spike_vecs.shape}\")\n",
    "print(f\"  {len(spike_token_ids):,} tokens\")\n",
    "print(f\"  {d:,} dimensions\")\n",
    "print(f\"  Device: {spike_vecs.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Pairwise Distances\n",
    "\n",
    "We'll compute exact L2 distances using chunked processing to manage memory. The distance matrix will be NxN where N is the number of spike tokens (~20,000).\n",
    "\n",
    "This computation is expensive but necessary—we don't yet know that there are duplicate vectors, so we need to examine all pairwise distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing exact pairwise L2 distances...\n",
      "\n",
      "Using chunked algorithm with chunk_size=40\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769303562a8c4bd38cfa29db1be0b51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing distances:   0%|          | 0/510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Distance matrix computed\n",
      "  Shape: torch.Size([20373, 20373])\n",
      "  Memory: 1.55 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Computing exact pairwise L2 distances...\\n\")\n",
    "print(f\"Using chunked algorithm with chunk_size={CHUNK_SIZE}\\n\")\n",
    "\n",
    "# Exact distance computation using chunked processing\n",
    "N_spike = len(spike_vecs)\n",
    "dists = torch.zeros((N_spike, N_spike), dtype=torch.float32, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, N_spike, CHUNK_SIZE), desc=\"Computing distances\"):\n",
    "        end_i = min(i + CHUNK_SIZE, N_spike)\n",
    "        chunk = spike_vecs[i:end_i]\n",
    "        \n",
    "        # Compute differences: (chunk_size, N_spike, d)\n",
    "        diffs = chunk.unsqueeze(1) - spike_vecs.unsqueeze(0)\n",
    "        \n",
    "        # Compute L2 norm along dimension 2, move to CPU\n",
    "        dists[i:end_i] = torch.linalg.vector_norm(diffs, ord=2, dim=2).cpu()\n",
    "\n",
    "print(f\"\\n✓ Distance matrix computed\")\n",
    "print(f\"  Shape: {dists.shape}\")\n",
    "print(f\"  Memory: {dists.element_size() * dists.nelement() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance statistics (excluding diagonal):\n",
      "\n",
      "  Number of pairs: 207,519,378\n",
      "  Min: 0.0000000000\n",
      "  Median: 1.2463527918\n",
      "  Mean: 1.1983120441\n",
      "  Max: 2.0540900230\n",
      "\n",
      "Exact zeros (off-diagonal): 651,034\n",
      "  ⚠️  Found 651,034 token pairs with distance = 0\n",
      "     Multiple tokens occupy the same point in space!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Distance statistics (excluding diagonal):\\n\")\n",
    "\n",
    "# Get upper triangle (excluding diagonal)\n",
    "upper_tri_mask = torch.triu(torch.ones_like(dists, dtype=torch.bool), diagonal=1)\n",
    "dists_upper = dists[upper_tri_mask]\n",
    "\n",
    "print(f\"  Number of pairs: {len(dists_upper):,}\")\n",
    "print(f\"  Min: {dists_upper.min():.10f}\")\n",
    "print(f\"  Median: {dists_upper.median():.10f}\")\n",
    "print(f\"  Mean: {dists_upper.mean():.10f}\")\n",
    "print(f\"  Max: {dists_upper.max():.10f}\")\n",
    "print()\n",
    "\n",
    "# Count exact zeros\n",
    "n_zeros = (dists_upper == 0).sum().item()\n",
    "print(f\"Exact zeros (off-diagonal): {n_zeros:,}\")\n",
    "if n_zeros > 0:\n",
    "    print(f\"  ⚠️  Found {n_zeros:,} token pairs with distance = 0\")\n",
    "    print(f\"     Multiple tokens occupy the same point in space!\")\n",
    "else:\n",
    "    print(f\"  ✓ No exact duplicates found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved to ../tensors/Qwen3-4B-Instruct-2507/1.4b_overdensity_distances.safetensors\n",
      "  Size: 1583.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Save distance matrix and metadata\n",
    "output_dir = Path(f\"../tensors/{MODEL_NAME}\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"1.4b_overdensity_distances.safetensors\"\n",
    "\n",
    "save_file({\n",
    "    'distances': dists,\n",
    "    'spike_token_ids': spike_token_ids.cpu(),\n",
    "}, output_path)\n",
    "\n",
    "print(f\"\\n✓ Saved to {output_path}\")\n",
    "print(f\"  Size: {output_path.stat().st_size / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've computed the full pairwise distance matrix for all ~20,000 tokens in the overdensity. This matrix reveals:\n",
    "\n",
    "- **Minimum distances:** How close are the nearest neighbors?\n",
    "- **Exact zeros:** Are there duplicate vectors (multiple tokens at the same point)?\n",
    "- **Distance distribution:** Is there a tight cluster separated from normal tokens?\n",
    "\n",
    "The distance matrix is saved for further analysis—we can now visualize the distribution, identify clusters, and separate the true \"spike\" from tokens that just happen to pass through the region."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
