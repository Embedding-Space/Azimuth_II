{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.9a: The Great Gatsby Corpus Preparation\n",
    "\n",
    "**Download and prepare ASCII training corpus for 100k-step encounter hypothesis test**\n",
    "\n",
    "## The Plan\n",
    "\n",
    "We're testing the encounter hypothesis: can unreachable tokens escape the primordial cluster via rare prediction encounters?\n",
    "\n",
    "To maximize opportunities for encounters, we'll train for 100,000 steps on The Great Gatsby corpus.\n",
    "\n",
    "## Corpus Choice\n",
    "\n",
    "**The Great Gatsby** by F. Scott Fitzgerald\n",
    "- Public domain (published 1925)\n",
    "- ~265 KB of pure ASCII text\n",
    "- Rich vocabulary and varied sentence structure\n",
    "- Available from Project Gutenberg\n",
    "- Guarantees ~50 dead tokens (ASCII bytes that never appear)\n",
    "\n",
    "## This Notebook\n",
    "\n",
    "1. Download raw text from Project Gutenberg\n",
    "2. Strip headers/footers\n",
    "3. Convert to pure ASCII (remove UTF-8 artifacts)\n",
    "4. Analyze byte statistics\n",
    "5. Save corpus to `blog/data/the_great_gatsby.txt`\n",
    "6. Save token lists to `blog/tensors/Lil_Gatsby/1.9a_token_lists.safetensors`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "GUTENBERG_URL = \"https://www.gutenberg.org/cache/epub/64317/pg64317.txt\"\n",
    "\n",
    "# Outputs\n",
    "CORPUS_OUTPUT = \"../data/the_great_gatsby.txt\"\n",
    "TENSOR_OUTPUT = \"../tensors/Lil_Gatsby/1.9a_token_lists.safetensors\"\n",
    "\n",
    "# Vocabulary\n",
    "VOCAB_SIZE = 128  # ASCII only (0-127)\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download The Great Gatsby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from Project Gutenberg...\n",
      "\n",
      "URL: https://www.gutenberg.org/cache/epub/64317/pg64317.txt\n",
      "\n",
      "✓ Downloaded successfully\n",
      "Raw size: 296,858 characters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Downloading from Project Gutenberg...\\n\")\n",
    "print(f\"URL: {GUTENBERG_URL}\")\n",
    "\n",
    "response = requests.get(GUTENBERG_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "raw_text = response.text\n",
    "\n",
    "print(f\"\\n✓ Downloaded successfully\")\n",
    "print(f\"Raw size: {len(raw_text):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip Project Gutenberg Headers/Footers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found start marker: *** START OF THE PROJECT GUTENBERG EBOOK...\n",
      "Found end marker: *** END OF THE PROJECT GUTENBERG EBOOK...\n",
      "\n",
      "Stripped header/footer:\n",
      "  Clean text: 277,090 characters\n",
      "  Removed: 19,768 characters\n"
     ]
    }
   ],
   "source": [
    "# Find start marker (typical Gutenberg format)\n",
    "start_markers = [\n",
    "    \"*** START OF THE PROJECT GUTENBERG EBOOK\",\n",
    "    \"*** START OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "    \"***START OF THE PROJECT GUTENBERG EBOOK\"\n",
    "]\n",
    "\n",
    "start_idx = 0\n",
    "for marker in start_markers:\n",
    "    idx = raw_text.find(marker)\n",
    "    if idx != -1:\n",
    "        # Skip past the marker line\n",
    "        start_idx = raw_text.find('\\n', idx) + 1\n",
    "        print(f\"Found start marker: {marker[:50]}...\")\n",
    "        break\n",
    "\n",
    "# Find end marker\n",
    "end_markers = [\n",
    "    \"*** END OF THE PROJECT GUTENBERG EBOOK\",\n",
    "    \"*** END OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "    \"***END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "]\n",
    "\n",
    "end_idx = len(raw_text)\n",
    "for marker in end_markers:\n",
    "    idx = raw_text.find(marker)\n",
    "    if idx != -1:\n",
    "        end_idx = idx\n",
    "        print(f\"Found end marker: {marker[:50]}...\")\n",
    "        break\n",
    "\n",
    "# Extract just the novel\n",
    "clean_text = raw_text[start_idx:end_idx].strip()\n",
    "\n",
    "print(f\"\\nStripped header/footer:\")\n",
    "print(f\"  Clean text: {len(clean_text):,} characters\")\n",
    "print(f\"  Removed: {len(raw_text) - len(clean_text):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Pure ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8 encoding analysis:\n",
      "  Total bytes: 286,640\n",
      "  Non-ASCII bytes: 14,335\n",
      "  Non-ASCII byte values: [128, 138, 148, 152, 153, 156, 157, 166, 167, 169, 170, 180, 195, 226]\n",
      "\n",
      "⚠️  Text contains non-ASCII characters (UTF-8 encoding artifacts)\n",
      "Converting to pure ASCII by stripping...\n",
      "\n",
      "✓ Converted to pure ASCII\n",
      "  Final size: 272,305 bytes\n",
      "  Characters removed: 14,335\n"
     ]
    }
   ],
   "source": [
    "# Try encoding as UTF-8 to see what we're dealing with\n",
    "text_bytes_utf8 = clean_text.encode('utf-8', errors='replace')\n",
    "\n",
    "# Find non-ASCII bytes\n",
    "non_ascii = [b for b in text_bytes_utf8 if b > 127]\n",
    "\n",
    "print(f\"UTF-8 encoding analysis:\")\n",
    "print(f\"  Total bytes: {len(text_bytes_utf8):,}\")\n",
    "print(f\"  Non-ASCII bytes: {len(non_ascii):,}\")\n",
    "\n",
    "if non_ascii:\n",
    "    print(f\"  Non-ASCII byte values: {sorted(set(non_ascii))}\")\n",
    "    print(f\"\\n⚠️  Text contains non-ASCII characters (UTF-8 encoding artifacts)\")\n",
    "    print(f\"Converting to pure ASCII by stripping...\")\n",
    "    \n",
    "    # Strip non-ASCII characters\n",
    "    ascii_text = clean_text.encode('ascii', errors='ignore').decode('ascii')\n",
    "    text_bytes = ascii_text.encode('ascii')\n",
    "    \n",
    "    print(f\"\\n✓ Converted to pure ASCII\")\n",
    "    print(f\"  Final size: {len(text_bytes):,} bytes\")\n",
    "    print(f\"  Characters removed: {len(text_bytes_utf8) - len(text_bytes):,}\")\n",
    "else:\n",
    "    print(f\"\\n✓ Text is already pure ASCII\")\n",
    "    ascii_text = clean_text\n",
    "    text_bytes = ascii_text.encode('ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Byte Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Byte statistics (ASCII 0-127):\n",
      "  Total bytes in corpus: 272,305\n",
      "  Unique bytes present: 78 / 128\n",
      "  Dead bytes: 50 / 128 (39.1%)\n"
     ]
    }
   ],
   "source": [
    "byte_counts = Counter(text_bytes)\n",
    "\n",
    "# Which ASCII bytes appear?\n",
    "present_bytes = set(byte_counts.keys())\n",
    "n_present = len(present_bytes)\n",
    "\n",
    "# Which ASCII bytes are dead?\n",
    "all_ascii = set(range(VOCAB_SIZE))\n",
    "dead_bytes = all_ascii - present_bytes\n",
    "n_dead = len(dead_bytes)\n",
    "\n",
    "print(f\"\\nByte statistics (ASCII 0-{VOCAB_SIZE-1}):\")\n",
    "print(f\"  Total bytes in corpus: {len(text_bytes):,}\")\n",
    "print(f\"  Unique bytes present: {n_present} / {VOCAB_SIZE}\")\n",
    "print(f\"  Dead bytes: {n_dead} / {VOCAB_SIZE} ({100 * n_dead / VOCAB_SIZE:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common bytes:\n",
      "   32 (   ' '): 44,251 (16.25%)\n",
      "  101 (   'e'): 25,012 ( 9.19%)\n",
      "  116 (   't'): 18,098 ( 6.65%)\n",
      "   97 (   'a'): 16,843 ( 6.19%)\n",
      "  111 (   'o'): 15,739 ( 5.78%)\n",
      "  110 (   'n'): 14,065 ( 5.17%)\n",
      "  105 (   'i'): 12,532 ( 4.60%)\n",
      "  115 (   's'): 12,370 ( 4.54%)\n",
      "  104 (   'h'): 12,240 ( 4.49%)\n",
      "  114 (   'r'): 11,342 ( 4.17%)\n"
     ]
    }
   ],
   "source": [
    "# Show most common bytes\n",
    "most_common = byte_counts.most_common(10)\n",
    "\n",
    "print(f\"\\nMost common bytes:\")\n",
    "for byte_val, count in most_common:\n",
    "    char = repr(chr(byte_val)) if 32 <= byte_val < 127 else f\"\\\\x{byte_val:02x}\"\n",
    "    print(f\"  {byte_val:3d} ({char:>6s}): {count:6,} ({100 * count / len(text_bytes):5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Least common bytes:\n",
      "   81 (   'Q'):      4 ( 0.00%)\n",
      "   50 (   '2'):      4 ( 0.00%)\n",
      "   56 (   '8'):      3 ( 0.00%)\n",
      "   88 (   'X'):      2 ( 0.00%)\n",
      "   55 (   '7'):      2 ( 0.00%)\n",
      "   52 (   '4'):      2 ( 0.00%)\n",
      "   91 (   '['):      2 ( 0.00%)\n",
      "   93 (   ']'):      2 ( 0.00%)\n",
      "   36 (   '$'):      2 ( 0.00%)\n",
      "   90 (   'Z'):      1 ( 0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Show least common bytes (that actually appear)\n",
    "least_common = byte_counts.most_common()[-10:]\n",
    "\n",
    "print(f\"\\nLeast common bytes:\")\n",
    "for byte_val, count in least_common:\n",
    "    char = repr(chr(byte_val)) if 32 <= byte_val < 127 else f\"\\\\x{byte_val:02x}\"\n",
    "    print(f\"  {byte_val:3d} ({char:>6s}): {count:6,} ({100 * count / len(text_bytes):5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dead bytes (50 total):\n",
      "  Byte values: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 35, 37, 38, 39, 43, 47, 60, 61, 62, 64, 92, 94, 95, 96, 123, 124, 125, 126, 127]\n",
      "  Characters: \\x00, \\x01, \\x02, \\x03, \\x04, \\x05, \\x06, \\x07, \\x08, \\x09, \\x0b, \\x0c, \\x0e, \\x0f, \\x10, \\x11, \\x12, \\x13, \\x14, \\x15...\n"
     ]
    }
   ],
   "source": [
    "# Show dead bytes\n",
    "if dead_bytes:\n",
    "    print(f\"\\nDead bytes ({len(dead_bytes)} total):\")\n",
    "    print(f\"  Byte values: {sorted(dead_bytes)}\")\n",
    "    \n",
    "    # Show as characters where printable\n",
    "    printable = [repr(chr(b)) if 32 <= b < 127 else f\"\\\\x{b:02x}\" for b in sorted(dead_bytes)]\n",
    "    print(f\"  Characters: {', '.join(printable[:20])}...\" if len(printable) > 20 else f\"  Characters: {', '.join(printable)}\")\n",
    "else:\n",
    "    print(f\"\\nNo dead bytes - all {VOCAB_SIZE} ASCII values appear in corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Corpus to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved corpus to: ../data/the_great_gatsby.txt\n",
      "  File size: 272,305 bytes (265.9 KB)\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if needed\n",
    "corpus_path = Path(CORPUS_OUTPUT)\n",
    "corpus_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write as ASCII text\n",
    "with open(corpus_path, 'w', encoding='ascii') as f:\n",
    "    f.write(ascii_text)\n",
    "\n",
    "print(f\"✓ Saved corpus to: {corpus_path}\")\n",
    "print(f\"  File size: {len(ascii_text):,} bytes ({len(ascii_text) / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Token Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved token lists to: ../tensors/Lil_Gatsby/1.9a_token_lists.safetensors\n",
      "  Dead tokens: 50\n",
      "  Live tokens: 78\n"
     ]
    }
   ],
   "source": [
    "# Create dead/live token ID lists\n",
    "dead_token_ids = torch.tensor(sorted(dead_bytes), dtype=torch.int64)\n",
    "live_token_ids = torch.tensor(sorted(present_bytes), dtype=torch.int64)\n",
    "\n",
    "# Create output directory if needed\n",
    "tensor_path = Path(TENSOR_OUTPUT)\n",
    "tensor_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save to safetensors\n",
    "save_file({\n",
    "    'dead_token_ids': dead_token_ids,\n",
    "    'live_token_ids': live_token_ids,\n",
    "    'vocab_size': torch.tensor(VOCAB_SIZE, dtype=torch.int64),\n",
    "}, tensor_path)\n",
    "\n",
    "print(f\"\\n✓ Saved token lists to: {tensor_path}\")\n",
    "print(f\"  Dead tokens: {len(dead_token_ids)}\")\n",
    "print(f\"  Live tokens: {len(live_token_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "THE GREAT GATSBY CORPUS - PREPARATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Source:\n",
      "  Novel: The Great Gatsby by F. Scott Fitzgerald\n",
      "  URL: https://www.gutenberg.org/cache/epub/64317/pg64317.txt\n",
      "\n",
      "Corpus:\n",
      "  Size: 272,305 bytes (265.9 KB)\n",
      "  Encoding: Pure ASCII (0-127)\n",
      "\n",
      "Vocabulary (128-byte tokenizer):\n",
      "  Live tokens: 78 (60.9%)\n",
      "  Dead tokens: 50 (39.1%)\n",
      "\n",
      "Outputs:\n",
      "  Corpus: ../data/the_great_gatsby.txt\n",
      "  Token lists: ../tensors/Lil_Gatsby/1.9a_token_lists.safetensors\n",
      "\n",
      "Ready for 1.9b: 100k-step training run\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"THE GREAT GATSBY CORPUS - PREPARATION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nSource:\")\n",
    "print(f\"  Novel: The Great Gatsby by F. Scott Fitzgerald\")\n",
    "print(f\"  URL: {GUTENBERG_URL}\")\n",
    "print(f\"\\nCorpus:\")\n",
    "print(f\"  Size: {len(text_bytes):,} bytes ({len(text_bytes) / 1024:.1f} KB)\")\n",
    "print(f\"  Encoding: Pure ASCII (0-{VOCAB_SIZE-1})\")\n",
    "print(f\"\\nVocabulary ({VOCAB_SIZE}-byte tokenizer):\")\n",
    "print(f\"  Live tokens: {len(live_token_ids)} ({100 * len(live_token_ids) / VOCAB_SIZE:.1f}%)\")\n",
    "print(f\"  Dead tokens: {len(dead_token_ids)} ({100 * len(dead_token_ids) / VOCAB_SIZE:.1f}%)\")\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  Corpus: {corpus_path}\")\n",
    "print(f\"  Token lists: {tensor_path}\")\n",
    "print(f\"\\nReady for 1.9b: 100k-step training run\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
