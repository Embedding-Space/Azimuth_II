{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.2c: Primordial Snowball Test\n",
    "\n",
    "**Hypothesis:** Qwen's black holes start as a quantized Gaussian snowball at initialization, not a perfect singularity.\n",
    "\n",
    "## Background\n",
    "\n",
    "From 11.2b, we learned that continuous noise injection causes cumulative diffusion (random walk). But Qwen's black holes are *tightly clustered* in a 2ε hypercube.\n",
    "\n",
    "**New hypothesis:** Qwen initializes embeddings with:\n",
    "```python\n",
    "base_vector + Gaussian(0, σ)\n",
    "```\n",
    "\n",
    "where σ is small enough that after bfloat16 quantization, dead tokens collapse to ~60 distinct vectors within a 2ε hypercube.\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "**Initialization:** All tokens start near one random vector, with Gaussian jitter at scale σ ≈ ε\n",
    "\n",
    "**Training:** Standard training, NO per-step noise injection\n",
    "\n",
    "**Success criteria:**\n",
    "- Initial dead tokens should cluster into multiple black holes (C > 1)\n",
    "- Max L∞ between black holes ≈ 2ε (tight snowball)\n",
    "- After training, black holes remain stable or consolidate further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "VOCAB_SIZE = 128      # ASCII tokens\n",
    "HIDDEN_DIM = 64       # Embedding dimension\n",
    "N_LAYER = 2           # Transformer layers\n",
    "N_HEAD = 2            # Attention heads\n",
    "MAX_SEQ_LEN = 128     # Context window\n",
    "\n",
    "# Initialization\n",
    "INIT_MODE = \"snowball\"  # Gaussian cluster, not singularity\n",
    "INIT_SIGMA = 1e-5       # Noise scale (bfloat16 ULP)\n",
    "\n",
    "# Training\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.01\n",
    "BATCH_SIZE = 32\n",
    "NUM_TRAIN_STEPS = 10000\n",
    "\n",
    "# Data\n",
    "CORPUS_PATH = \"../data/training_corpus.txt\"\n",
    "OUTPUT_DIR = \"../data/embeddings_128vocab_primordial_snowball\"\n",
    "OUTPUT_FILE = \"embedding_evolution.safetensors\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, TrainerCallback\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "from collections import Counter\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"Using MPS (Apple Silicon)\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from: ../data/training_corpus.txt\n",
      "\n",
      "✓ Corpus loaded\n",
      "Total bytes: 265,905\n",
      "Vocabulary size: 128\n",
      "Unique bytes in corpus: 77\n",
      "Dead tokens (never appear): 51 (39.8%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading corpus from: {CORPUS_PATH}\\n\")\n",
    "\n",
    "with open(CORPUS_PATH, 'r', encoding='ascii') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "# Convert to bytes and filter to vocab size\n",
    "corpus_bytes = [b for b in corpus_text.encode('ascii') if b < VOCAB_SIZE]\n",
    "\n",
    "print(f\"✓ Corpus loaded\")\n",
    "print(f\"Total bytes: {len(corpus_bytes):,}\")\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Count unique bytes\n",
    "unique_bytes = set(corpus_bytes)\n",
    "dead_tokens = VOCAB_SIZE - len(unique_bytes)\n",
    "\n",
    "print(f\"Unique bytes in corpus: {len(unique_bytes)}\")\n",
    "print(f\"Dead tokens (never appear): {dead_tokens} ({100 * dead_tokens / VOCAB_SIZE:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset created\n",
      "Training examples: 265,777\n"
     ]
    }
   ],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    \"\"\"Dataset for byte-level language modeling.\"\"\"\n",
    "    def __init__(self, byte_sequence, max_seq_len):\n",
    "        self.byte_sequence = byte_sequence\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.byte_sequence) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.byte_sequence[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(chunk[:-1], dtype=torch.long),\n",
    "            'labels': torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "dataset = ByteDataset(corpus_bytes, MAX_SEQ_LEN)\n",
    "print(f\"\\n✓ Dataset created\")\n",
    "print(f\"Training examples: {len(dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Model created (bfloat16, standard LayerNorm)\n",
      "Total parameters: 116,480\n"
     ]
    }
   ],
   "source": [
    "# Create standard GPT-2 config\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Convert to bfloat16\n",
    "model = model.to(torch.bfloat16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n✓ Model created (bfloat16, standard LayerNorm)\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Snowball Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying primordial snowball initialization...\n",
      "  Base vector + Gaussian(0, σ=1.00e-05)\n",
      "\n",
      "✓ Snowball initialization complete\n",
      "  Base vector norm: 8.201394\n",
      "  Initial unique vectors: 21\n",
      "  Initial black holes (C₀): 15\n",
      "  Initial black hole population (P₀): 122\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nApplying primordial snowball initialization...\")\n",
    "print(f\"  Base vector + Gaussian(0, σ={INIT_SIGMA:.2e})\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate one random base vector\n",
    "    base_vector = torch.randn(HIDDEN_DIM)\n",
    "    \n",
    "    # Add small Gaussian jitter to each token\n",
    "    noise = torch.randn(VOCAB_SIZE, HIDDEN_DIM) * INIT_SIGMA\n",
    "    \n",
    "    # Combine: all tokens near base_vector\n",
    "    model.transformer.wte.weight[:] = base_vector + noise\n",
    "\n",
    "# Analyze initial snowball structure\n",
    "initial_embeddings = model.transformer.wte.weight.data.clone().cpu()\n",
    "unique_init, inverse_init, counts_init = torch.unique(\n",
    "    initial_embeddings,\n",
    "    dim=0,\n",
    "    return_inverse=True,\n",
    "    return_counts=True\n",
    ")\n",
    "\n",
    "init_black_holes = (counts_init > 1).sum().item()\n",
    "init_bh_population = counts_init[counts_init > 1].sum().item()\n",
    "\n",
    "print(f\"✓ Snowball initialization complete\")\n",
    "print(f\"  Base vector norm: {base_vector.norm().item():.6f}\")\n",
    "print(f\"  Initial unique vectors: {len(unique_init)}\")\n",
    "print(f\"  Initial black holes (C₀): {init_black_holes}\")\n",
    "print(f\"  Initial black hole population (P₀): {init_bh_population}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-allocate Embedding History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Pre-allocated embedding history\n",
      "  Shape: torch.Size([10001, 128, 64])\n",
      "  Memory: 163.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Pre-allocate tensor for all snapshots\n",
    "embedding_history = torch.zeros(\n",
    "    (NUM_TRAIN_STEPS + 1, VOCAB_SIZE, HIDDEN_DIM),\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Save initial state\n",
    "embedding_history[0] = model.transformer.wte.weight.data.clone().cpu()\n",
    "\n",
    "print(f\"\\n✓ Pre-allocated embedding history\")\n",
    "print(f\"  Shape: {embedding_history.shape}\")\n",
    "print(f\"  Memory: {embedding_history.element_size() * embedding_history.numel() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Callback (No Noise Injection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Callback defined (no noise injection)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingHistoryCallback(TrainerCallback):\n",
    "    \"\"\"Save embeddings to history (no noise injection).\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_history):\n",
    "        self.embedding_history = embedding_history\n",
    "    \n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        step = state.global_step\n",
    "        \n",
    "        # Store in memory (no noise injection)\n",
    "        self.embedding_history[step] = model.transformer.wte.weight.data.clone().cpu()\n",
    "        \n",
    "        # Print progress every 1000 steps\n",
    "        if step % 1000 == 0 and step > 0:\n",
    "            embeddings = self.embedding_history[step]\n",
    "            centroid_norm = embeddings.mean(dim=0).norm().item()\n",
    "            print(f\"[Step {step:5d}] Centroid norm: {centroid_norm:.6f}\")\n",
    "        \n",
    "        return control\n",
    "\n",
    "print(f\"✓ Callback defined (no noise injection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Device: mps\n",
      "  Steps: 10,000\n",
      "  Batch size: 32\n",
      "  Learning rate: 0.001\n",
      "  Weight decay: 0.01\n",
      "  Initial noise: σ = 1.00e-05 (at t=0 only)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_output_primordial_snowball\",\n",
    "    max_steps=NUM_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=100,\n",
    "    save_steps=NUM_TRAIN_STEPS + 1,  # Don't save checkpoints\n",
    "    save_total_limit=0,\n",
    "    seed=RANDOM_SEED,\n",
    "    dataloader_num_workers=0,\n",
    "    use_cpu=(DEVICE == \"cpu\"),\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Steps: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Initial noise: σ = {INIT_SIGMA:.2e} (at t=0 only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting training from primordial snowball...\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jefferyharrell/Projects/Azimuth_II/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 01:38, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.079200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.949600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.910500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.884400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.867700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.858500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.843300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.819200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.783200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.764500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.748300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.731200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.715500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.690700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.659600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.639800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.616700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.605800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.587900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.577700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.568900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.561800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.559200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.556800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.548700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.545100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.543900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.535900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.537100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.528100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.527100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.524400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.523700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.517600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.519700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.519100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.515500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.513200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.506600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.510800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.506500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.510100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.510900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.503400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.504800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.503100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.504400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.499100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>2.497200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.500600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>2.497900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.504500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>2.503100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>2.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.496600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>2.499700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.497800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>2.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.495800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>2.493600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>2.491700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>2.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>2.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.495200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>2.497200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>2.492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>2.496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>2.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.493400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step  1000] Centroid norm: 8.187500\n",
      "[Step  2000] Centroid norm: 8.187500\n",
      "[Step  3000] Centroid norm: 8.187500\n",
      "[Step  4000] Centroid norm: 8.187500\n",
      "[Step  5000] Centroid norm: 8.187500\n",
      "[Step  6000] Centroid norm: 8.187500\n",
      "[Step  7000] Centroid norm: 8.187500\n",
      "[Step  8000] Centroid norm: 8.187500\n",
      "[Step  9000] Centroid norm: 8.187500\n",
      "[Step 10000] Centroid norm: 8.187500\n",
      "\n",
      "================================================================================\n",
      "✓ Training complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[EmbeddingHistoryCallback(embedding_history)],\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting training from primordial snowball...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Training complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Embedding History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving embedding history to ../data/embeddings_128vocab_primordial_snowball/embedding_evolution.safetensors...\n",
      "✓ Saved 163.9 MB\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_path / OUTPUT_FILE\n",
    "\n",
    "print(f\"\\nSaving embedding history to {output_file}...\")\n",
    "save_file({'embedding_history': embedding_history}, output_file)\n",
    "\n",
    "print(f\"✓ Saved {output_file.stat().st_size / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results: Count Black Holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BLACK HOLE ANALYSIS\n",
      "================================================================================\n",
      "Initial state (t=0):\n",
      "  Black hole count (C₀): 15\n",
      "  Black hole population (P₀): 122\n",
      "\n",
      "Final state (t=10,000):\n",
      "  Black hole count (C_f): 2\n",
      "  Black hole population (P_f): 51\n",
      "\n",
      "Dead tokens in corpus: 51\n",
      "================================================================================\n",
      "\n",
      "✓ Multiple black holes present\n",
      "  → Consolidation: 15 → 2 black holes\n"
     ]
    }
   ],
   "source": [
    "# Get final embeddings\n",
    "final_embeddings = embedding_history[-1]\n",
    "\n",
    "# Find unique vectors\n",
    "unique_vectors, inverse_indices = torch.unique(\n",
    "    final_embeddings,\n",
    "    dim=0,\n",
    "    return_inverse=True\n",
    ")\n",
    "\n",
    "# Count populations\n",
    "vector_populations = Counter(inverse_indices.tolist())\n",
    "\n",
    "# Black holes are vectors with population ≥ 2\n",
    "black_holes = {vec_id: pop for vec_id, pop in vector_populations.items() if pop >= 2}\n",
    "\n",
    "C = len(black_holes)  # Black hole count\n",
    "P = sum(black_holes.values())  # Total population\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BLACK HOLE ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Initial state (t=0):\")\n",
    "print(f\"  Black hole count (C₀): {init_black_holes}\")\n",
    "print(f\"  Black hole population (P₀): {init_bh_population}\")\n",
    "print(f\"\\nFinal state (t={NUM_TRAIN_STEPS:,}):\")\n",
    "print(f\"  Black hole count (C_f): {C}\")\n",
    "print(f\"  Black hole population (P_f): {P}\")\n",
    "print(f\"\\nDead tokens in corpus: {dead_tokens}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if C > 1:\n",
    "    print(f\"\\n✓ Multiple black holes present\")\n",
    "    if C < init_black_holes:\n",
    "        print(f\"  → Consolidation: {init_black_holes} → {C} black holes\")\n",
    "    elif C > init_black_holes:\n",
    "        print(f\"  → Fragmentation: {init_black_holes} → {C} black holes\")\n",
    "    else:\n",
    "        print(f\"  → Stable: {C} black holes maintained\")\n",
    "elif C == 1:\n",
    "    print(f\"\\n⚠ Collapsed to single black hole\")\n",
    "    print(f\"  → Initial snowball merged during training\")\n",
    "else:\n",
    "    print(f\"\\n✓ All tokens escaped black holes!\")\n",
    "    print(f\"  → Training dispersed the snowball completely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Hole Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Black hole populations (sorted by size):\n",
      "\n",
      "BH #1: 49 tokens\n",
      "BH #2: 2 tokens\n",
      "\n",
      "Largest: 49 tokens\n",
      "Smallest: 2 tokens\n"
     ]
    }
   ],
   "source": [
    "if C > 0:\n",
    "    sorted_bhs = sorted(black_holes.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nBlack hole populations (sorted by size):\\n\")\n",
    "    for i, (vec_id, pop) in enumerate(sorted_bhs[:10], 1):  # Show top 10\n",
    "        print(f\"BH #{i}: {pop} tokens\")\n",
    "    \n",
    "    if C > 10:\n",
    "        print(f\"... ({C - 10} more black holes)\")\n",
    "    \n",
    "    print(f\"\\nLargest: {sorted_bhs[0][1]} tokens\")\n",
    "    if C > 1:\n",
    "        print(f\"Smallest: {sorted_bhs[-1][1]} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Analysis: Dead Token Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEAD TOKEN SNOWBALL GEOMETRY\n",
      "================================================================================\n",
      "Dead tokens: 51\n",
      "\n",
      "L∞ distances:\n",
      "  Min: 0.000000e+00\n",
      "  Max: 9.765625e-04\n",
      "  Mean: 7.506127e-05\n",
      "\n",
      "Reference scales:\n",
      "  ε (bfloat16 ULP): 1.00e-05\n",
      "  2ε (lattice scale): 2.00e-05\n",
      "  Max L∞ / ε: 97.7×\n",
      "\n",
      "Conclusion:\n",
      "  ✗ Dead tokens scattered widely (max L∞ = 98× ε)\n",
      "  → Does not match Qwen's tight clustering\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Identify dead tokens\n",
    "dead_token_ids = sorted([t for t in range(VOCAB_SIZE) if t not in unique_bytes])\n",
    "\n",
    "# Extract dead token embeddings\n",
    "dead_embeddings_final = final_embeddings[dead_token_ids].to(torch.float32)\n",
    "\n",
    "# Pairwise L∞ distances\n",
    "n_dead = len(dead_token_ids)\n",
    "v1 = dead_embeddings_final.unsqueeze(1)  # [n, 1, d]\n",
    "v2 = dead_embeddings_final.unsqueeze(0)  # [1, n, d]\n",
    "diffs = v1 - v2  # [n, n, d]\n",
    "l_inf_distances = torch.abs(diffs).max(dim=2)[0]  # [n, n]\n",
    "\n",
    "# Mask out diagonal\n",
    "mask = ~torch.eye(n_dead, dtype=torch.bool)\n",
    "l_inf_nonzero = l_inf_distances[mask]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DEAD TOKEN SNOWBALL GEOMETRY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Dead tokens: {n_dead}\")\n",
    "print(f\"\\nL∞ distances:\")\n",
    "print(f\"  Min: {l_inf_nonzero.min().item():.6e}\")\n",
    "print(f\"  Max: {l_inf_nonzero.max().item():.6e}\")\n",
    "print(f\"  Mean: {l_inf_nonzero.mean().item():.6e}\")\n",
    "print(f\"\\nReference scales:\")\n",
    "print(f\"  ε (bfloat16 ULP): {INIT_SIGMA:.2e}\")\n",
    "print(f\"  2ε (lattice scale): {2*INIT_SIGMA:.2e}\")\n",
    "print(f\"  Max L∞ / ε: {l_inf_nonzero.max().item() / INIT_SIGMA:.1f}×\")\n",
    "print(f\"\\nConclusion:\")\n",
    "if l_inf_nonzero.max().item() < 10 * INIT_SIGMA:\n",
    "    print(f\"  ✓ Dead tokens form tight snowball (max L∞ < 10ε)\")\n",
    "    print(f\"  → Matches Qwen's observed black hole geometry\")\n",
    "else:\n",
    "    print(f\"  ✗ Dead tokens scattered widely (max L∞ = {l_inf_nonzero.max().item() / INIT_SIGMA:.0f}× ε)\")\n",
    "    print(f\"  → Does not match Qwen's tight clustering\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENT SUMMARY\n",
      "================================================================================\n",
      "Model: GPT-2 (standard LayerNorm)\n",
      "Training steps: 10,000\n",
      "Dead tokens: 51\n",
      "Initialization: Primordial snowball (σ = 1.00e-05)\n",
      "\n",
      "Results:\n",
      "  Initial black holes: 15\n",
      "  Final black holes: 2\n",
      "  Final black hole population: 51\n",
      "  Max L∞ (dead tokens): 9.765625e-04\n",
      "  Centroid displacement: 0.359375\n",
      "\n",
      "Conclusion:\n",
      "  ✗ HYPOTHESIS CHALLENGED\n",
      "  Initial clustering did not produce stable snowball structure\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "initial_centroid = embedding_history[0].mean(dim=0)\n",
    "final_centroid = final_embeddings.mean(dim=0)\n",
    "displacement = (final_centroid - initial_centroid).norm().item()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EXPERIMENT SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Model: GPT-2 (standard LayerNorm)\")\n",
    "print(f\"Training steps: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"Dead tokens: {dead_tokens}\")\n",
    "print(f\"Initialization: Primordial snowball (σ = {INIT_SIGMA:.2e})\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Initial black holes: {init_black_holes}\")\n",
    "print(f\"  Final black holes: {C}\")\n",
    "print(f\"  Final black hole population: {P}\")\n",
    "print(f\"  Max L∞ (dead tokens): {l_inf_nonzero.max().item():.6e}\")\n",
    "print(f\"  Centroid displacement: {displacement:.6f}\")\n",
    "print(f\"\\nConclusion:\")\n",
    "if C > 1 and l_inf_nonzero.max().item() < 10 * INIT_SIGMA:\n",
    "    print(f\"  ✓ HYPOTHESIS SUPPORTED\")\n",
    "    print(f\"  Primordial snowball initialization reproduces Qwen-like black hole structure\")\n",
    "    print(f\"  Black holes remain stable/consolidate during training\")\n",
    "elif C == 1:\n",
    "    print(f\"  ⚠ PARTIAL SUPPORT\")\n",
    "    print(f\"  Snowball initialization worked, but training merged clusters\")\n",
    "    print(f\"  May need larger initial σ or different training dynamics\")\n",
    "else:\n",
    "    print(f\"  ✗ HYPOTHESIS CHALLENGED\")\n",
    "    print(f\"  Initial clustering did not produce stable snowball structure\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
