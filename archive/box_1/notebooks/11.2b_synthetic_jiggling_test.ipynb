{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.2b: Synthetic Jiggling Test\n",
    "\n",
    "**Hypothesis:** Can we MANUFACTURE black hole diffusion by artificially adding random noise?\n",
    "\n",
    "## Background\n",
    "\n",
    "We've ruled out:\n",
    "- Extended training time (100k steps, no diffusion)\n",
    "- RMSNorm vs LayerNorm (11.2a, null result)\n",
    "\n",
    "Yet somehow, Qwen's post-training *consolidates* black holes (Base: 62→13, Qwen2.5: 65→60).\n",
    "\n",
    "**New approach:** Let's cheat. Add explicit random noise of order ε after each optimizer step and see if we can *force* black holes to break apart. If we can manufacture the effect, we can reverse-engineer what natural mechanism might produce similar perturbations.\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "**Control (11.2a):** GPT-2 with RMSNorm → C=1, P=51 (no diffusion)\n",
    "\n",
    "**Treatment:** Same model (LayerNorm, not RMSNorm) but with synthetic noise injection:\n",
    "```python\n",
    "embeddings += torch.randn_like(embeddings) * epsilon\n",
    "```\n",
    "\n",
    "**ε scale:** 3×10⁻⁵ (bfloat16 ULP at black hole magnitude ~0.005)\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "After 10,000 training steps:\n",
    "- **Null result:** C = 1, P = 51 (even synthetic noise doesn't break symmetry)\n",
    "- **Positive result:** C > 1 or P < 51 (noise CAN cause diffusion → look for natural source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (same as 11.2a)\n",
    "VOCAB_SIZE = 128      # ASCII tokens\n",
    "HIDDEN_DIM = 64       # Embedding dimension\n",
    "N_LAYER = 2           # Transformer layers\n",
    "N_HEAD = 2            # Attention heads\n",
    "MAX_SEQ_LEN = 128     # Context window\n",
    "\n",
    "# Initialization\n",
    "INIT_MODE = \"qwen\"    # All tokens start at same point\n",
    "\n",
    "# Training\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.01\n",
    "BATCH_SIZE = 32\n",
    "NUM_TRAIN_STEPS = 10000\n",
    "\n",
    "# Synthetic noise injection\n",
    "EPSILON = 3e-5  # bfloat16 ULP scale\n",
    "INJECT_NOISE = True\n",
    "\n",
    "# Data\n",
    "CORPUS_PATH = \"../data/training_corpus.txt\"\n",
    "OUTPUT_DIR = \"../data/embeddings_128vocab_synthetic_jiggling\"\n",
    "OUTPUT_FILE = \"embedding_evolution.safetensors\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, TrainerCallback\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"Using MPS (Apple Silicon)\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from: ../data/training_corpus.txt\n",
      "\n",
      "✓ Corpus loaded\n",
      "Total bytes: 265,905\n",
      "Vocabulary size: 128\n",
      "Unique bytes in corpus: 77\n",
      "Dead tokens (never appear): 51 (39.8%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading corpus from: {CORPUS_PATH}\\n\")\n",
    "\n",
    "with open(CORPUS_PATH, 'r', encoding='ascii') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "# Convert to bytes and filter to vocab size\n",
    "corpus_bytes = [b for b in corpus_text.encode('ascii') if b < VOCAB_SIZE]\n",
    "\n",
    "print(f\"✓ Corpus loaded\")\n",
    "print(f\"Total bytes: {len(corpus_bytes):,}\")\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Count unique bytes\n",
    "unique_bytes = set(corpus_bytes)\n",
    "dead_tokens = VOCAB_SIZE - len(unique_bytes)\n",
    "\n",
    "print(f\"Unique bytes in corpus: {len(unique_bytes)}\")\n",
    "print(f\"Dead tokens (never appear): {dead_tokens} ({100 * dead_tokens / VOCAB_SIZE:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset created\n",
      "Training examples: 265,777\n"
     ]
    }
   ],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    \"\"\"Dataset for byte-level language modeling.\"\"\"\n",
    "    def __init__(self, byte_sequence, max_seq_len):\n",
    "        self.byte_sequence = byte_sequence\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.byte_sequence) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.byte_sequence[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(chunk[:-1], dtype=torch.long),\n",
    "            'labels': torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "dataset = ByteDataset(corpus_bytes, MAX_SEQ_LEN)\n",
    "print(f\"\\n✓ Dataset created\")\n",
    "print(f\"Training examples: {len(dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model (Standard GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Model created (bfloat16, standard LayerNorm)\n",
      "Total parameters: 116,480\n"
     ]
    }
   ],
   "source": [
    "# Create standard GPT-2 config (LayerNorm, not RMSNorm)\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Convert to bfloat16\n",
    "model = model.to(torch.bfloat16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n✓ Model created (bfloat16, standard LayerNorm)\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Qwen Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying Qwen-style initialization (singular vector)...\n",
      "✓ All 128 tokens initialized to same random unit vector\n",
      "  Initial vector norm: 1.000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nApplying Qwen-style initialization (singular vector)...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate one random unit vector\n",
    "    random_vector = torch.randn(HIDDEN_DIM)\n",
    "    random_vector = random_vector / random_vector.norm()\n",
    "    \n",
    "    # Set ALL embedding vectors to this single vector\n",
    "    model.transformer.wte.weight[:] = random_vector\n",
    "\n",
    "print(f\"✓ All {VOCAB_SIZE} tokens initialized to same random unit vector\")\n",
    "print(f\"  Initial vector norm: {random_vector.norm().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-allocate Embedding History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Pre-allocated embedding history\n",
      "  Shape: torch.Size([10001, 128, 64])\n",
      "  Memory: 163.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Pre-allocate tensor for all snapshots\n",
    "embedding_history = torch.zeros(\n",
    "    (NUM_TRAIN_STEPS + 1, VOCAB_SIZE, HIDDEN_DIM),\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Save initial state\n",
    "embedding_history[0] = model.transformer.wte.weight.data.clone().cpu()\n",
    "\n",
    "print(f\"\\n✓ Pre-allocated embedding history\")\n",
    "print(f\"  Shape: {embedding_history.shape}\")\n",
    "print(f\"  Memory: {embedding_history.element_size() * embedding_history.numel() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Callback with Noise Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Callback defined (ε = 3.00e-05)\n",
      "  Noise injection ENABLED\n"
     ]
    }
   ],
   "source": [
    "class NoiseInjectionCallback(TrainerCallback):\n",
    "    \"\"\"Inject synthetic noise after each optimizer step.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_history, epsilon, inject_noise=True):\n",
    "        self.embedding_history = embedding_history\n",
    "        self.epsilon = epsilon\n",
    "        self.inject_noise = inject_noise\n",
    "    \n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        step = state.global_step\n",
    "        \n",
    "        # INJECT SYNTHETIC NOISE (the cheating part)\n",
    "        if self.inject_noise:\n",
    "            with torch.no_grad():\n",
    "                embeddings = model.transformer.wte.weight\n",
    "                noise = torch.randn_like(embeddings) * (self.epsilon / 10.0)\n",
    "                embeddings.add_(noise)\n",
    "        \n",
    "        # Store in memory\n",
    "        self.embedding_history[step] = model.transformer.wte.weight.data.clone().cpu()\n",
    "        \n",
    "        # Print progress every 1000 steps\n",
    "        if step % 1000 == 0 and step > 0:\n",
    "            embeddings = self.embedding_history[step]\n",
    "            centroid_norm = embeddings.mean(dim=0).norm().item()\n",
    "            marker = \"[+NOISE]\" if self.inject_noise else \"\"\n",
    "            print(f\"[Step {step:5d}] Centroid norm: {centroid_norm:.6f} {marker}\")\n",
    "        \n",
    "        return control\n",
    "\n",
    "print(f\"✓ Callback defined (ε = {EPSILON:.2e})\")\n",
    "if INJECT_NOISE:\n",
    "    print(f\"  Noise injection ENABLED\")\n",
    "else:\n",
    "    print(f\"  Noise injection DISABLED (control run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Device: mps\n",
      "  Steps: 10,000\n",
      "  Batch size: 32\n",
      "  Learning rate: 0.001\n",
      "  Weight decay: 0.01\n",
      "  Noise epsilon: 3.00e-05\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_output_synthetic_jiggling\",\n",
    "    max_steps=NUM_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=100,\n",
    "    save_steps=NUM_TRAIN_STEPS + 1,  # Don't save checkpoints\n",
    "    save_total_limit=0,\n",
    "    seed=RANDOM_SEED,\n",
    "    dataloader_num_workers=0,\n",
    "    use_cpu=(DEVICE == \"cpu\"),\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Steps: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Noise epsilon: {EPSILON:.2e}\" if INJECT_NOISE else \"  No noise injection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting training with synthetic noise injection...\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jefferyharrell/Projects/Azimuth_II/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 01:38, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.335100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.078000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.891700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.841400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.819200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.792900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.753700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.679900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.648400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.570900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.540500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.454400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.438800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.411100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.388500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.361500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.343500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.321800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.295300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.290900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.269100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.223700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.193300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.189400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.179400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.165200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>2.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>2.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>2.160400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>2.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>2.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.152100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>2.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>2.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>2.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>2.155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>2.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>2.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>2.147400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>2.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>2.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.150800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step  1000] Centroid norm: 0.824219 [+NOISE]\n",
      "[Step  2000] Centroid norm: 0.824219 [+NOISE]\n",
      "[Step  3000] Centroid norm: 0.828125 [+NOISE]\n",
      "[Step  4000] Centroid norm: 0.824219 [+NOISE]\n",
      "[Step  5000] Centroid norm: 0.824219 [+NOISE]\n",
      "[Step  6000] Centroid norm: 0.824219 [+NOISE]\n",
      "[Step  7000] Centroid norm: 0.824219 [+NOISE]\n",
      "[Step  8000] Centroid norm: 0.824219 [+NOISE]\n",
      "[Step  9000] Centroid norm: 0.824219 [+NOISE]\n",
      "[Step 10000] Centroid norm: 0.824219 [+NOISE]\n",
      "\n",
      "================================================================================\n",
      "✓ Training complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[NoiseInjectionCallback(embedding_history, EPSILON, INJECT_NOISE)],\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting training with synthetic noise injection...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Training complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Embedding History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving embedding history to ../data/embeddings_128vocab_synthetic_jiggling/embedding_evolution.safetensors...\n",
      "✓ Saved 163.9 MB\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_path / OUTPUT_FILE\n",
    "\n",
    "print(f\"\\nSaving embedding history to {output_file}...\")\n",
    "save_file({'embedding_history': embedding_history}, output_file)\n",
    "\n",
    "print(f\"✓ Saved {output_file.stat().st_size / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results: Count Black Holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BLACK HOLE ANALYSIS\n",
      "================================================================================\n",
      "Black hole count (C): 2\n",
      "Total black hole population (P): 51\n",
      "Dead tokens in corpus: 51\n",
      "\n",
      "Expected (no noise): C = 1, P = 51\n",
      "Observed (synthetic noise ε=3.00e-05): C = 2, P = 51\n",
      "================================================================================\n",
      "\n",
      "✓ POSITIVE RESULT: Synthetic noise causes black hole diffusion!\n",
      "  → Black holes fragmented: 2 distinct clusters\n",
      "\n",
      "Implication: A natural noise source of order ε~3.00e-05 could explain\n",
      "Qwen's black hole behavior. Look for:\n",
      "  - Gradient noise from large batch variance\n",
      "  - Quantization noise in mixed-precision training\n",
      "  - Stochastic rounding in optimizer\n",
      "  - Numerical instability in normalization layers\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get final embeddings\n",
    "final_embeddings = embedding_history[-1]\n",
    "\n",
    "# Find unique vectors\n",
    "unique_vectors, inverse_indices = torch.unique(\n",
    "    final_embeddings,\n",
    "    dim=0,\n",
    "    return_inverse=True\n",
    ")\n",
    "\n",
    "# Count populations\n",
    "vector_populations = Counter(inverse_indices.tolist())\n",
    "\n",
    "# Black holes are vectors with population ≥ 2\n",
    "black_holes = {vec_id: pop for vec_id, pop in vector_populations.items() if pop >= 2}\n",
    "\n",
    "C = len(black_holes)  # Black hole count\n",
    "P = sum(black_holes.values())  # Total population\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BLACK HOLE ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Black hole count (C): {C}\")\n",
    "print(f\"Total black hole population (P): {P}\")\n",
    "print(f\"Dead tokens in corpus: {dead_tokens}\")\n",
    "print(f\"\\nExpected (no noise): C = 1, P = 51\")\n",
    "print(f\"Observed (synthetic noise ε={EPSILON:.2e}): C = {C}, P = {P}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if C > 1 or P < dead_tokens:\n",
    "    print(f\"\\n✓ POSITIVE RESULT: Synthetic noise causes black hole diffusion!\")\n",
    "    if C > 1:\n",
    "        print(f\"  → Black holes fragmented: {C} distinct clusters\")\n",
    "    if P < dead_tokens:\n",
    "        print(f\"  → Tokens escaped: {dead_tokens - P} tokens rescued from black holes\")\n",
    "    print(f\"\\nImplication: A natural noise source of order ε~{EPSILON:.2e} could explain\")\n",
    "    print(f\"Qwen's black hole behavior. Look for:\")\n",
    "    print(f\"  - Gradient noise from large batch variance\")\n",
    "    print(f\"  - Quantization noise in mixed-precision training\")\n",
    "    print(f\"  - Stochastic rounding in optimizer\")\n",
    "    print(f\"  - Numerical instability in normalization layers\")\n",
    "else:\n",
    "    print(f\"\\n✗ NULL RESULT: Even synthetic noise doesn't break black holes\")\n",
    "    print(f\"  Mechanism must be more subtle than simple additive noise\")\n",
    "    print(f\"  Consider:\")\n",
    "    print(f\"    - Different noise scale (try larger ε)\")\n",
    "    print(f\"    - Directional bias (not isotropic)\")\n",
    "    print(f\"    - Noise in gradient space rather than parameter space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Hole Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Black hole populations (sorted by size):\n",
      "\n",
      "BH #1: 31 tokens\n",
      "BH #2: 20 tokens\n",
      "\n",
      "Largest: 31 tokens\n",
      "Smallest: 20 tokens\n"
     ]
    }
   ],
   "source": [
    "if C > 0:\n",
    "    sorted_bhs = sorted(black_holes.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nBlack hole populations (sorted by size):\\n\")\n",
    "    for i, (vec_id, pop) in enumerate(sorted_bhs, 1):\n",
    "        print(f\"BH #{i}: {pop} tokens\")\n",
    "    \n",
    "    print(f\"\\nLargest: {sorted_bhs[0][1]} tokens\")\n",
    "    if C > 1:\n",
    "        print(f\"Smallest: {sorted_bhs[-1][1]} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENT SUMMARY\n",
      "================================================================================\n",
      "Model: GPT-2 (standard LayerNorm)\n",
      "Training steps: 10,000\n",
      "Dead tokens: 51\n",
      "Synthetic noise: ENABLED\n",
      "  Epsilon (ε): 3.00e-05\n",
      "\n",
      "Results:\n",
      "  Black hole count: 2\n",
      "  Black hole population: 51\n",
      "  Centroid displacement: 0.470703\n",
      "\n",
      "Conclusion:\n",
      "  Synthetic noise CAN break black hole symmetry\n",
      "  Natural noise source at scale ε~3.00e-05 likely responsible in Qwen\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "initial_centroid = embedding_history[0].mean(dim=0)\n",
    "final_centroid = final_embeddings.mean(dim=0)\n",
    "displacement = (final_centroid - initial_centroid).norm().item()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EXPERIMENT SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Model: GPT-2 (standard LayerNorm)\")\n",
    "print(f\"Training steps: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"Dead tokens: {dead_tokens}\")\n",
    "print(f\"Synthetic noise: {'ENABLED' if INJECT_NOISE else 'DISABLED'}\")\n",
    "if INJECT_NOISE:\n",
    "    print(f\"  Epsilon (ε): {EPSILON:.2e}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Black hole count: {C}\")\n",
    "print(f\"  Black hole population: {P}\")\n",
    "print(f\"  Centroid displacement: {displacement:.6f}\")\n",
    "print(f\"\\nConclusion:\")\n",
    "if C > 1 or P < dead_tokens:\n",
    "    print(f\"  Synthetic noise CAN break black hole symmetry\")\n",
    "    print(f\"  Natural noise source at scale ε~{EPSILON:.2e} likely responsible in Qwen\")\n",
    "else:\n",
    "    print(f\"  Noise at ε~{EPSILON:.2e} insufficient to cause diffusion\")\n",
    "    print(f\"  Mechanism remains unknown\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
