{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.2a: RMSNorm Diffusion Test\n",
    "\n",
    "**Hypothesis:** RMSNorm (used in Qwen) causes black hole diffusion during training\n",
    "\n",
    "## Background\n",
    "\n",
    "Our previous experiments (run_1001, 100k steps) showed **zero black hole diffusion** using GPT-2 architecture with LayerNorm.\n",
    "\n",
    "Qwen uses **RMSNorm** instead of LayerNorm. The difference:\n",
    "- **LayerNorm**: `(x - mean(x)) / std(x)` — centers and normalizes\n",
    "- **RMSNorm**: `x / sqrt(mean(x²))` — normalizes only, no centering\n",
    "\n",
    "RMSNorm may create different gradient dynamics that break black hole symmetry.\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "**Control:** GPT-2 with LayerNorm → C=1, P=50 (no diffusion)\n",
    "\n",
    "**Treatment:** Same model but with RMSNorm → if C>1 or P<50, RMSNorm causes diffusion\n",
    "\n",
    "## Implementation\n",
    "\n",
    "We'll create a custom GPT-2 model that replaces all LayerNorm layers with RMSNorm.\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "After 10,000 training steps:\n",
    "- **Null result:** C = 1, P = 51 (same as control)\n",
    "- **Positive result:** C > 1 or P < 51 (RMSNorm causes diffusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (same as 08.2a)\n",
    "VOCAB_SIZE = 128      # ASCII tokens\n",
    "HIDDEN_DIM = 64       # Embedding dimension\n",
    "N_LAYER = 2           # Transformer layers\n",
    "N_HEAD = 2            # Attention heads\n",
    "MAX_SEQ_LEN = 128     # Context window\n",
    "\n",
    "# Initialization\n",
    "INIT_MODE = \"qwen\"    # All tokens start at same point\n",
    "\n",
    "# Training\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.01\n",
    "BATCH_SIZE = 32\n",
    "NUM_TRAIN_STEPS = 10000  # 2× longer than 08.2a to give diffusion more time\n",
    "\n",
    "# Data\n",
    "CORPUS_PATH = \"../data/training_corpus.txt\"\n",
    "OUTPUT_DIR = \"../data/embeddings_128vocab_rmsnorm_test\"\n",
    "OUTPUT_FILE = \"embedding_evolution.safetensors\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, TrainerCallback\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"Using MPS (Apple Silicon)\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define RMSNorm Layer\n",
    "\n",
    "RMSNorm implementation following the Qwen paper reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RMSNorm defined\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\n",
    "    \n",
    "    RMSNorm(x) = x / sqrt(mean(x²) + eps) * scale\n",
    "    \n",
    "    Unlike LayerNorm, this does NOT subtract the mean (no centering).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        \n",
    "        # Compute RMS\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)\n",
    "        \n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "print(\"✓ RMSNorm defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace LayerNorm with RMSNorm\n",
    "\n",
    "Monkey-patch the GPT-2 model to use RMSNorm instead of LayerNorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Replacement function defined\n"
     ]
    }
   ],
   "source": [
    "def replace_layernorm_with_rmsnorm(model):\n",
    "    \"\"\"Recursively replace all LayerNorm layers with RMSNorm.\"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.LayerNorm):\n",
    "            # Replace with RMSNorm\n",
    "            rmsnorm = RMSNorm(\n",
    "                hidden_size=module.normalized_shape[0],\n",
    "                eps=module.eps\n",
    "            )\n",
    "            setattr(model, name, rmsnorm)\n",
    "            print(f\"  Replaced {name}: LayerNorm → RMSNorm\")\n",
    "        else:\n",
    "            # Recurse into child modules\n",
    "            replace_layernorm_with_rmsnorm(module)\n",
    "\n",
    "print(\"✓ Replacement function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from: ../data/training_corpus.txt\n",
      "\n",
      "✓ Corpus loaded\n",
      "Total bytes: 265,905\n",
      "Vocabulary size: 128\n",
      "Unique bytes in corpus: 77\n",
      "Dead tokens (never appear): 51 (39.8%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading corpus from: {CORPUS_PATH}\\n\")\n",
    "\n",
    "with open(CORPUS_PATH, 'r', encoding='ascii') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "# Convert to bytes and filter to vocab size\n",
    "corpus_bytes = [b for b in corpus_text.encode('ascii') if b < VOCAB_SIZE]\n",
    "\n",
    "print(f\"✓ Corpus loaded\")\n",
    "print(f\"Total bytes: {len(corpus_bytes):,}\")\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Count unique bytes\n",
    "unique_bytes = set(corpus_bytes)\n",
    "dead_tokens = VOCAB_SIZE - len(unique_bytes)\n",
    "\n",
    "print(f\"Unique bytes in corpus: {len(unique_bytes)}\")\n",
    "print(f\"Dead tokens (never appear): {dead_tokens} ({100 * dead_tokens / VOCAB_SIZE:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset created\n",
      "Training examples: 265,777\n"
     ]
    }
   ],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    \"\"\"Dataset for byte-level language modeling.\"\"\"\n",
    "    def __init__(self, byte_sequence, max_seq_len):\n",
    "        self.byte_sequence = byte_sequence\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.byte_sequence) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.byte_sequence[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(chunk[:-1], dtype=torch.long),\n",
    "            'labels': torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "dataset = ByteDataset(corpus_bytes, MAX_SEQ_LEN)\n",
    "print(f\"\\n✓ Dataset created\")\n",
    "print(f\"Training examples: {len(dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model with RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing LayerNorm with RMSNorm:\n",
      "  Replaced ln_1: LayerNorm → RMSNorm\n",
      "  Replaced ln_2: LayerNorm → RMSNorm\n",
      "  Replaced ln_1: LayerNorm → RMSNorm\n",
      "  Replaced ln_2: LayerNorm → RMSNorm\n",
      "  Replaced ln_f: LayerNorm → RMSNorm\n",
      "\n",
      "✓ Model created (bfloat16)\n",
      "Total parameters: 116,160\n"
     ]
    }
   ],
   "source": [
    "# Create standard GPT-2 config\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Replace all LayerNorm with RMSNorm\n",
    "print(\"\\nReplacing LayerNorm with RMSNorm:\")\n",
    "replace_layernorm_with_rmsnorm(model)\n",
    "\n",
    "# Convert to bfloat16\n",
    "model = model.to(torch.bfloat16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n✓ Model created (bfloat16)\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Qwen Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying Qwen-style initialization (singular vector)...\n",
      "✓ All 128 tokens initialized to same random unit vector\n",
      "  Initial vector norm: 1.000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nApplying Qwen-style initialization (singular vector)...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate one random unit vector\n",
    "    random_vector = torch.randn(HIDDEN_DIM)\n",
    "    random_vector = random_vector / random_vector.norm()\n",
    "    \n",
    "    # Set ALL embedding vectors to this single vector\n",
    "    model.transformer.wte.weight[:] = random_vector\n",
    "\n",
    "print(f\"✓ All {VOCAB_SIZE} tokens initialized to same random unit vector\")\n",
    "print(f\"  Initial vector norm: {random_vector.norm().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-allocate Embedding History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Pre-allocated embedding history\n",
      "  Shape: torch.Size([10001, 128, 64])\n",
      "  Memory: 163.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Pre-allocate tensor for all snapshots\n",
    "embedding_history = torch.zeros(\n",
    "    (NUM_TRAIN_STEPS + 1, VOCAB_SIZE, HIDDEN_DIM),\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Save initial state\n",
    "embedding_history[0] = model.transformer.wte.weight.data.clone().cpu()\n",
    "\n",
    "print(f\"\\n✓ Pre-allocated embedding history\")\n",
    "print(f\"  Shape: {embedding_history.shape}\")\n",
    "print(f\"  Memory: {embedding_history.element_size() * embedding_history.numel() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Callback defined\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingHistoryCallback(TrainerCallback):\n",
    "    \"\"\"Save embeddings to history tensor (in memory only, write at end).\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_history):\n",
    "        self.embedding_history = embedding_history\n",
    "    \n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        step = state.global_step\n",
    "        \n",
    "        # Store in memory\n",
    "        self.embedding_history[step] = model.transformer.wte.weight.data.clone().cpu()\n",
    "        \n",
    "        # Print progress every 1000 steps\n",
    "        if step % 1000 == 0 and step > 0:\n",
    "            embeddings = self.embedding_history[step]\n",
    "            centroid_norm = embeddings.mean(dim=0).norm().item()\n",
    "            print(f\"[Step {step:5d}] Centroid norm: {centroid_norm:.6f}\")\n",
    "        \n",
    "        return control\n",
    "\n",
    "print(\"✓ Callback defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Device: mps\n",
      "  Steps: 10,000\n",
      "  Batch size: 32\n",
      "  Learning rate: 0.001\n",
      "  Weight decay: 0.01\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_output_rmsnorm\",\n",
    "    max_steps=NUM_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=100,\n",
    "    save_steps=NUM_TRAIN_STEPS + 1,  # Don't save checkpoints\n",
    "    save_total_limit=0,\n",
    "    seed=RANDOM_SEED,\n",
    "    dataloader_num_workers=0,\n",
    "    use_cpu=(DEVICE == \"cpu\"),\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Steps: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trainer and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jefferyharrell/Projects/Azimuth_II/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 01:44, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.344700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.909600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.834900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.809100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.784400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.765300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.735800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.710400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.678800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.644600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.584400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.555900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.529600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.514300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.486500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.470800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.461400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.420500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.392500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.367900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.358700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.345700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.337500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.327600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.321100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.306100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.296800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.279500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.279100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.264700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.242800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.233900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>2.234300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>2.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.240500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>2.238200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>2.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.229300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>2.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>2.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>2.228400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>2.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>2.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>2.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>2.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>2.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>2.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>2.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.228700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step  1000] Centroid norm: 0.808594\n",
      "[Step  2000] Centroid norm: 0.804688\n",
      "[Step  3000] Centroid norm: 0.804688\n",
      "[Step  4000] Centroid norm: 0.804688\n",
      "[Step  5000] Centroid norm: 0.808594\n",
      "[Step  6000] Centroid norm: 0.808594\n",
      "[Step  7000] Centroid norm: 0.808594\n",
      "[Step  8000] Centroid norm: 0.808594\n",
      "[Step  9000] Centroid norm: 0.808594\n",
      "[Step 10000] Centroid norm: 0.808594\n",
      "\n",
      "================================================================================\n",
      "✓ Training complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[EmbeddingHistoryCallback(embedding_history)],\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Training complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Embedding History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving embedding history to ../data/embeddings_128vocab_rmsnorm_test/embedding_evolution.safetensors...\n",
      "✓ Saved 163.9 MB\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "output_file = output_path / OUTPUT_FILE\n",
    "\n",
    "print(f\"\\nSaving embedding history to {output_file}...\")\n",
    "save_file({'embedding_history': embedding_history}, output_file)\n",
    "\n",
    "print(f\"✓ Saved {output_file.stat().st_size / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results: Count Black Holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BLACK HOLE ANALYSIS\n",
      "================================================================================\n",
      "Black hole count (C): 1\n",
      "Total black hole population (P): 51\n",
      "Dead tokens in corpus: 51\n",
      "\n",
      "Expected (LayerNorm control): C = 1, P = 51\n",
      "Observed (RMSNorm test): C = 1, P = 51\n",
      "================================================================================\n",
      "\n",
      "✗ NULL RESULT: No diffusion detected\n",
      "  RMSNorm does not appear to cause black hole breakup\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get final embeddings\n",
    "final_embeddings = embedding_history[-1]\n",
    "\n",
    "# Find unique vectors\n",
    "unique_vectors, inverse_indices = torch.unique(\n",
    "    final_embeddings,\n",
    "    dim=0,\n",
    "    return_inverse=True\n",
    ")\n",
    "\n",
    "# Count populations\n",
    "vector_populations = Counter(inverse_indices.tolist())\n",
    "\n",
    "# Black holes are vectors with population ≥ 2\n",
    "black_holes = {vec_id: pop for vec_id, pop in vector_populations.items() if pop >= 2}\n",
    "\n",
    "C = len(black_holes)  # Black hole count\n",
    "P = sum(black_holes.values())  # Total population\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BLACK HOLE ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Black hole count (C): {C}\")\n",
    "print(f\"Total black hole population (P): {P}\")\n",
    "print(f\"Dead tokens in corpus: {dead_tokens}\")\n",
    "print(f\"\\nExpected (LayerNorm control): C = 1, P = 51\")\n",
    "print(f\"Observed (RMSNorm test): C = {C}, P = {P}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if C > 1 or P < dead_tokens:\n",
    "    print(f\"\\n✓ POSITIVE RESULT: RMSNorm causes black hole diffusion!\")\n",
    "    if C > 1:\n",
    "        print(f\"  → Black holes fragmented: {C} distinct clusters\")\n",
    "    if P < dead_tokens:\n",
    "        print(f\"  → Tokens escaped: {dead_tokens - P} tokens no longer in black holes\")\n",
    "else:\n",
    "    print(f\"\\n✗ NULL RESULT: No diffusion detected\")\n",
    "    print(f\"  RMSNorm does not appear to cause black hole breakup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Hole Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Black hole populations (sorted by size):\n",
      "\n",
      "BH #1: 51 tokens\n",
      "\n",
      "Largest: 51 tokens\n"
     ]
    }
   ],
   "source": [
    "if C > 0:\n",
    "    sorted_bhs = sorted(black_holes.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nBlack hole populations (sorted by size):\\n\")\n",
    "    for i, (vec_id, pop) in enumerate(sorted_bhs, 1):\n",
    "        print(f\"BH #{i}: {pop} tokens\")\n",
    "    \n",
    "    print(f\"\\nLargest: {sorted_bhs[0][1]} tokens\")\n",
    "    if C > 1:\n",
    "        print(f\"Smallest: {sorted_bhs[-1][1]} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENT SUMMARY\n",
      "================================================================================\n",
      "Model: GPT-2 with RMSNorm (not LayerNorm)\n",
      "Training steps: 10,000\n",
      "Dead tokens: 51\n",
      "\n",
      "Results:\n",
      "  Black hole count: 1\n",
      "  Black hole population: 51\n",
      "  Centroid displacement: 0.527344\n",
      "\n",
      "Conclusion:\n",
      "  No evidence of diffusion - mechanism remains unknown\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "initial_centroid = embedding_history[0].mean(dim=0)\n",
    "final_centroid = final_embeddings.mean(dim=0)\n",
    "displacement = (final_centroid - initial_centroid).norm().item()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EXPERIMENT SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Model: GPT-2 with RMSNorm (not LayerNorm)\")\n",
    "print(f\"Training steps: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"Dead tokens: {dead_tokens}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Black hole count: {C}\")\n",
    "print(f\"  Black hole population: {P}\")\n",
    "print(f\"  Centroid displacement: {displacement:.6f}\")\n",
    "print(f\"\\nConclusion:\")\n",
    "if C > 1 or P < dead_tokens:\n",
    "    print(f\"  RMSNorm appears to cause black hole diffusion\")\n",
    "else:\n",
    "    print(f\"  No evidence of diffusion - mechanism remains unknown\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
