{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08.4d: Black Hole Counts (Pre-computed)\n",
    "\n",
    "**Compute black hole counts from distance matrices and save for reuse**\n",
    "\n",
    "This notebook processes pre-computed Chebyshev distance matrices to extract black hole statistics across all training steps. By computing once and saving, we enable fast iteration on fission detection analysis.\n",
    "\n",
    "## What We Compute\n",
    "\n",
    "For each run, at each step:\n",
    "- **Number of black holes** (connected components with population ≥2)\n",
    "- **Total population** in black holes\n",
    "- **Largest black hole size**\n",
    "\n",
    "## Strategy\n",
    "\n",
    "1. **Load** pre-computed Chebyshev distance matrices\n",
    "2. **Build** adjacency graphs (GPU-accelerated)\n",
    "3. **Run** Union-Find to extract connected components (CPU, sequential)\n",
    "4. **Save** results as small safetensors files (~40 KB each)\n",
    "\n",
    "## Performance\n",
    "\n",
    "- Expected: ~7 minutes for 16 runs (one-time cost)\n",
    "- Bottleneck: Union-Find algorithm (inherently sequential, can't be parallelized)\n",
    "\n",
    "## Output\n",
    "\n",
    "For each run, saves `black_hole_counts.safetensors` containing:\n",
    "- `counts`: (10001,) int32 - number of black holes at each step\n",
    "- `populations`: (10001,) int32 - total population in black holes\n",
    "- `largest_bh_size`: (10001,) int32 - size of largest black hole\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directories\n",
    "DATA_DIR = \"../data\"\n",
    "RUN_PATTERN = \"embeddings_128vocab_qweninit_run_*\"\n",
    "DISTANCE_FILE = \"pairwise_distances.safetensors\"\n",
    "DISTANCE_KEY = \"chebyshev_distances\"\n",
    "OUTPUT_FILE = \"black_hole_counts.safetensors\"\n",
    "\n",
    "# Expected dimensions\n",
    "EXPECTED_RUNS = 16\n",
    "EXPECTED_STEPS = 10001\n",
    "VOCAB_SIZE = 128\n",
    "\n",
    "# Black hole threshold: bit-identical (use very small epsilon)\n",
    "BLACK_HOLE_THRESHOLD = 1e-10\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file, save_file\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find All Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 runs:\n",
      "  embeddings_128vocab_qweninit_run_001\n",
      "  embeddings_128vocab_qweninit_run_002\n",
      "  embeddings_128vocab_qweninit_run_003\n",
      "  embeddings_128vocab_qweninit_run_004\n",
      "  embeddings_128vocab_qweninit_run_005\n",
      "  embeddings_128vocab_qweninit_run_006\n",
      "  embeddings_128vocab_qweninit_run_007\n",
      "  embeddings_128vocab_qweninit_run_008\n",
      "  embeddings_128vocab_qweninit_run_009\n",
      "  embeddings_128vocab_qweninit_run_010\n",
      "  embeddings_128vocab_qweninit_run_011\n",
      "  embeddings_128vocab_qweninit_run_012\n",
      "  embeddings_128vocab_qweninit_run_013\n",
      "  embeddings_128vocab_qweninit_run_014\n",
      "  embeddings_128vocab_qweninit_run_015\n",
      "  embeddings_128vocab_qweninit_run_016\n",
      "\n",
      "✓ Found all 16 runs\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(DATA_DIR)\n",
    "run_dirs = sorted(data_dir.glob(RUN_PATTERN))\n",
    "\n",
    "print(f\"Found {len(run_dirs)} runs:\")\n",
    "for run_dir in run_dirs:\n",
    "    print(f\"  {run_dir.name}\")\n",
    "\n",
    "if len(run_dirs) != EXPECTED_RUNS:\n",
    "    print(f\"\\n⚠ WARNING: Expected {EXPECTED_RUNS} runs, found {len(run_dirs)}\")\n",
    "else:\n",
    "    print(f\"\\n✓ Found all {EXPECTED_RUNS} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union-Find for Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Union-Find and black hole detection functions defined\n"
     ]
    }
   ],
   "source": [
    "class UnionFind:\n",
    "    \"\"\"Union-Find data structure for finding connected components.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "        self.rank = [0] * n\n",
    "    \n",
    "    def find(self, x):\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])  # Path compression\n",
    "        return self.parent[x]\n",
    "    \n",
    "    def union(self, x, y):\n",
    "        root_x = self.find(x)\n",
    "        root_y = self.find(y)\n",
    "        \n",
    "        if root_x == root_y:\n",
    "            return\n",
    "        \n",
    "        # Union by rank\n",
    "        if self.rank[root_x] < self.rank[root_y]:\n",
    "            self.parent[root_x] = root_y\n",
    "        elif self.rank[root_x] > self.rank[root_y]:\n",
    "            self.parent[root_y] = root_x\n",
    "        else:\n",
    "            self.parent[root_y] = root_x\n",
    "            self.rank[root_x] += 1\n",
    "    \n",
    "    def get_components(self):\n",
    "        \"\"\"Return list of connected components (each component is a list of indices).\"\"\"\n",
    "        components = defaultdict(list)\n",
    "        for i in range(len(self.parent)):\n",
    "            root = self.find(i)\n",
    "            components[root].append(i)\n",
    "        return list(components.values())\n",
    "\n",
    "\n",
    "def find_black_holes(distance_matrix, threshold):\n",
    "    \"\"\"\n",
    "    Find black holes in a distance matrix using Union-Find.\n",
    "    \n",
    "    Args:\n",
    "        distance_matrix: (n, n) tensor of pairwise Chebyshev distances\n",
    "        threshold: distance threshold for considering tokens identical\n",
    "    \n",
    "    Returns:\n",
    "        black_holes: list of lists, each sublist contains token IDs in a black hole\n",
    "    \"\"\"\n",
    "    n = distance_matrix.shape[0]\n",
    "    uf = UnionFind(n)\n",
    "    \n",
    "    # Build adjacency graph: tokens within threshold are connected\n",
    "    adjacency = (distance_matrix < threshold)\n",
    "    \n",
    "    # Extract upper triangle pairs that are adjacent (avoid redundant checks)\n",
    "    triu_indices = torch.triu_indices(n, n, offset=1)\n",
    "    adjacent_pairs = triu_indices[:, adjacency[triu_indices[0], triu_indices[1]]]\n",
    "    \n",
    "    # Union adjacent pairs\n",
    "    for k in range(adjacent_pairs.shape[1]):\n",
    "        i, j = adjacent_pairs[0, k].item(), adjacent_pairs[1, k].item()\n",
    "        uf.union(i, j)\n",
    "    \n",
    "    # Get connected components with population ≥ 2\n",
    "    components = uf.get_components()\n",
    "    black_holes = [comp for comp in components if len(comp) >= 2]\n",
    "    \n",
    "    return black_holes\n",
    "\n",
    "\n",
    "print(\"✓ Union-Find and black hole detection functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Each Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing runs...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89e207a7e18413d9525c221cd61b514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Runs:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  001: already exists, skipping\n",
      "  002: already exists, skipping\n",
      "  003: already exists, skipping\n",
      "  004: already exists, skipping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87dad9669aad4c168132e89a72724dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  005:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  005: saved 120.2 KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd9283c43b5488ab7d2195ced78c473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  006:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  006: saved 120.2 KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521e7e7369f240e1a1928e8f72243595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  007:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  007: saved 120.2 KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9afe742d3d4a0395e1f4b3c0258581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  008:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  008: saved 120.2 KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa85a56d0664252a36cc4b7ca939df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  009:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  009: saved 120.2 KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4a4761594d403687aba9385174da7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  010:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  010: saved 120.2 KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1d6ba332e4468eb034d6ac8c6fc62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  011:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  011: saved 120.2 KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e61433703064bcea19a3f066cc6b6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  012:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  012: saved 120.2 KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1f98acbfe3425d9e32e841900c3575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  013:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  013: saved 120.2 KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986fe4fd4e5b4f408c0d01a9b1d6bc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  014:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  014: saved 120.2 KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128fa3b87e9c41e2ad9c7d6aebc6c554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  015:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  015: saved 120.2 KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74395d669b464410b9d9fb33e8a0f12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  016:   0%|          | 0/10001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  016: saved 120.2 KB\n",
      "\n",
      "✓ All runs processed\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nProcessing runs...\\n\")\n",
    "\n",
    "for run_dir in tqdm(run_dirs, desc=\"Runs\"):\n",
    "    run_name = run_dir.name.split('_')[-1]\n",
    "    distance_path = run_dir / DISTANCE_FILE\n",
    "    output_path = run_dir / OUTPUT_FILE\n",
    "    \n",
    "    # Skip if already computed\n",
    "    if output_path.exists():\n",
    "        print(f\"  {run_name}: already exists, skipping\")\n",
    "        continue\n",
    "    \n",
    "    if not distance_path.exists():\n",
    "        print(f\"  {run_name}: distance file not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Load distance matrices\n",
    "    data = load_file(distance_path)\n",
    "    chebyshev_distances = data[DISTANCE_KEY]\n",
    "    n_steps = chebyshev_distances.shape[0]\n",
    "    \n",
    "    # Validate dimensions\n",
    "    expected_shape = (EXPECTED_STEPS, VOCAB_SIZE, VOCAB_SIZE)\n",
    "    if chebyshev_distances.shape != expected_shape:\n",
    "        print(f\"  {run_name}: unexpected shape {chebyshev_distances.shape}, skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Allocate result arrays\n",
    "    counts = np.zeros(n_steps, dtype=np.int32)\n",
    "    populations = np.zeros(n_steps, dtype=np.int32)\n",
    "    largest_bh_size = np.zeros(n_steps, dtype=np.int32)\n",
    "    \n",
    "    # Process each step\n",
    "    for step in tqdm(range(n_steps), desc=f\"  {run_name}\", leave=False):\n",
    "        distance_matrix = chebyshev_distances[step]\n",
    "        black_holes = find_black_holes(distance_matrix, BLACK_HOLE_THRESHOLD)\n",
    "        \n",
    "        counts[step] = len(black_holes)\n",
    "        populations[step] = sum(len(bh) for bh in black_holes)\n",
    "        largest_bh_size[step] = max((len(bh) for bh in black_holes), default=0)\n",
    "    \n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        'counts': torch.from_numpy(counts),\n",
    "        'populations': torch.from_numpy(populations),\n",
    "        'largest_bh_size': torch.from_numpy(largest_bh_size),\n",
    "    }\n",
    "    \n",
    "    save_file(save_dict, output_path)\n",
    "    \n",
    "    file_size_kb = output_path.stat().st_size / 1e3\n",
    "    print(f\"  {run_name}: saved {file_size_kb:.1f} KB\")\n",
    "\n",
    "print(f\"\\n✓ All runs processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "  001: 120.2 KB\n",
      "  002: 120.2 KB\n",
      "  003: 120.2 KB\n",
      "  004: 120.2 KB\n",
      "  005: 120.2 KB\n",
      "  006: 120.2 KB\n",
      "  007: 120.2 KB\n",
      "  008: 120.2 KB\n",
      "  009: 120.2 KB\n",
      "  010: 120.2 KB\n",
      "  011: 120.2 KB\n",
      "  012: 120.2 KB\n",
      "  013: 120.2 KB\n",
      "  014: 120.2 KB\n",
      "  015: 120.2 KB\n",
      "  016: 120.2 KB\n",
      "\n",
      "Total storage: 1923.9 KB (1.92 MB)\n",
      "\n",
      "Each file contains:\n",
      "  counts: (10001,) int32 - number of black holes per step\n",
      "  populations: (10001,) int32 - total population in black holes per step\n",
      "  largest_bh_size: (10001,) int32 - size of largest black hole per step\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check what we created\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "total_size = 0\n",
    "for run_dir in sorted(run_dirs):\n",
    "    output_path = run_dir / OUTPUT_FILE\n",
    "    if output_path.exists():\n",
    "        size_kb = output_path.stat().st_size / 1e3\n",
    "        total_size += size_kb\n",
    "        run_name = run_dir.name.split('_')[-1]\n",
    "        print(f\"  {run_name}: {size_kb:.1f} KB\")\n",
    "\n",
    "print(f\"\\nTotal storage: {total_size:.1f} KB ({total_size / 1e3:.2f} MB)\")\n",
    "print(f\"\\nEach file contains:\")\n",
    "print(f\"  counts: (10001,) int32 - number of black holes per step\")\n",
    "print(f\"  populations: (10001,) int32 - total population in black holes per step\")\n",
    "print(f\"  largest_bh_size: (10001,) int32 - size of largest black hole per step\")\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
