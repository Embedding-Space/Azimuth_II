{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 08.2a_h100: GPU-Optimized Embedding Evolution Training\n",
    "\n",
    "**H100-optimized version with maximum throughput**\n",
    "\n",
    "This notebook is optimized for high-end GPUs (H100, A100, etc.) with aggressive batching, pre-loaded data, and minimal I/O overhead.\n",
    "\n",
    "## Optimizations Applied\n",
    "\n",
    "1. **Pre-load corpus to GPU memory**: Entire dataset lives on GPU, zero CPU→GPU transfer per batch\n",
    "2. **Large batch sizes**: 4096-8192 to saturate GPU compute\n",
    "3. **Longer sequences**: 512 tokens (or more) for better GPU utilization\n",
    "4. **Periodic checkpointing**: Save every 100 steps instead of every step\n",
    "5. **Multi-worker data loading**: Parallel batch preparation\n",
    "6. **TF32 + bfloat16**: Maximum throughput on Ampere/Hopper GPUs\n",
    "7. **Gradient accumulation**: Optional for even larger effective batch sizes\n",
    "\n",
    "## Expected Performance\n",
    "\n",
    "- **H100**: 2000-5000 it/s\n",
    "- **A100**: 1000-3000 it/s\n",
    "- **Consumer GPUs (4090, etc.)**: 500-1500 it/s\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "VOCAB_SIZE = 128      # 128 for ASCII-only\n",
    "HIDDEN_DIM = 64       # Embedding dimension\n",
    "N_LAYER = 2           # Number of transformer layers\n",
    "N_HEAD = 2            # Number of attention heads\n",
    "MAX_SEQ_LEN = 512     # Context window (512 for GPU efficiency)\n",
    "\n",
    "# Initialization\n",
    "INIT_MODE = \"qwen\"  # \"normal\" or \"qwen\"\n",
    "\n",
    "# Training (GPU-optimized)\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.01\n",
    "BATCH_SIZE = 4096              # Large batch for GPU saturation\n",
    "GRADIENT_ACCUMULATION = 1      # Set to 2-4 for even larger effective batch\n",
    "NUM_TRAIN_STEPS = 50000        # Longer training to see more dynamics\n",
    "\n",
    "# Checkpointing\n",
    "SAVE_EVERY_N_STEPS = 100       # Save snapshot frequency\n",
    "\n",
    "# Data loading\n",
    "NUM_WORKERS = 8                # Parallel data loading\n",
    "PREFETCH_FACTOR = 4            # Batches to pre-fetch per worker\n",
    "\n",
    "# Data\n",
    "CORPUS_PATH = \"../data/training_corpus.txt\"\n",
    "OUTPUT_DIR = f\"../data/embeddings_{VOCAB_SIZE}vocab_{INIT_MODE}init_h100\"\n",
    "OUTPUT_FILE = f\"embedding_evolution.safetensors\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, TrainerCallback\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "import time\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Enable TF32 for maximum throughput on Ampere/Hopper\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Detect Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    print(f\"TF32 enabled: {torch.backends.cuda.matmul.allow_tf32}\")\n",
    "else:\n",
    "    raise RuntimeError(\"This notebook requires CUDA. Use 08.2a for CPU/MPS training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Load and Pre-Process Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading corpus from: {CORPUS_PATH}\\n\")\n",
    "\n",
    "with open(CORPUS_PATH, 'r', encoding='ascii') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "# Convert to bytes and filter to vocab size\n",
    "corpus_bytes = [b for b in corpus_text.encode('ascii') if b < VOCAB_SIZE]\n",
    "\n",
    "print(f\"✓ Corpus loaded\")\n",
    "print(f\"Total bytes: {len(corpus_bytes):,}\")\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Count unique bytes present\n",
    "unique_bytes = set(corpus_bytes)\n",
    "dead_tokens = VOCAB_SIZE - len(unique_bytes)\n",
    "\n",
    "print(f\"Unique bytes in corpus: {len(unique_bytes)}\")\n",
    "print(f\"Dead tokens (never appear): {dead_tokens} ({100 * dead_tokens / VOCAB_SIZE:.1f}%)\")\n",
    "\n",
    "# Pre-load to GPU as contiguous tensor\n",
    "corpus_tensor = torch.tensor(corpus_bytes, dtype=torch.long, device=DEVICE)\n",
    "print(f\"\\n✓ Corpus loaded to GPU memory\")\n",
    "print(f\"  Memory usage: {corpus_tensor.numel() * corpus_tensor.element_size() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## GPU-Optimized Dataset\n",
    "\n",
    "Samples directly from GPU tensor—zero CPU→GPU transfer during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUByteDataset(Dataset):\n",
    "    \"\"\"Dataset that samples from GPU tensor for zero-copy training.\n",
    "    \n",
    "    Returns overlapping sequences where:\n",
    "    - input_ids: tokens [0, 1, 2, ..., N-1]\n",
    "    - labels: tokens [1, 2, 3, ..., N] (shifted by 1 for next-token prediction)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Extract sequence directly from GPU tensor\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        \n",
    "        # Input: first max_seq_len tokens\n",
    "        # Target: shifted by 1 (next token prediction)\n",
    "        input_ids = chunk[:-1]\n",
    "        labels = chunk[1:]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "dataset = GPUByteDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "print(f\"✓ GPU dataset created\")\n",
    "print(f\"Training examples: {len(dataset):,}\")\n",
    "print(f\"Tokens per example: {MAX_SEQ_LEN}\")\n",
    "print(f\"Total tokens per epoch: {len(dataset) * MAX_SEQ_LEN:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Create Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "print(f\"  Vocabulary: {config.vocab_size} tokens\")\n",
    "print(f\"  Hidden dimension: {config.n_embd}\")\n",
    "print(f\"  Layers: {config.n_layer}\")\n",
    "print(f\"  Attention heads: {config.n_head}\")\n",
    "print(f\"  Context window: {config.n_positions} tokens\")\n",
    "print(f\"  Weight tying: {config.tie_word_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model = model.to(torch.bfloat16).to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "embedding_params = model.transformer.wte.weight.numel()\n",
    "\n",
    "print(f\"\\n✓ Model initialized (bfloat16, {DEVICE})\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Embedding parameters: {embedding_params:,} ({100 * embedding_params / total_params:.1f}% of total)\")\n",
    "print(f\"Model memory: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Apply Custom Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INIT_MODE == \"qwen\":\n",
    "    print(f\"\\nApplying Qwen-style initialization (singular unit vector)...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate one random unit vector\n",
    "        random_vector = torch.randn(HIDDEN_DIM, device=DEVICE)\n",
    "        random_vector = random_vector / random_vector.norm()\n",
    "        \n",
    "        # Set ALL embedding vectors to this single vector\n",
    "        model.transformer.wte.weight[:] = random_vector\n",
    "    \n",
    "    print(f\"✓ All {VOCAB_SIZE} tokens initialized to same random unit vector\")\n",
    "    print(f\"  Vector L2 norm: {random_vector.norm().item():.6f}\")\n",
    "    print(f\"  Vector mean component: {random_vector.mean().item():.6e}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nUsing normal initialization (default PyTorch)\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        norms = torch.norm(model.transformer.wte.weight, p=2, dim=1)\n",
    "        print(f\"  Token L2 norms: min={norms.min().item():.6f}, max={norms.max().item():.6f}, mean={norms.mean().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Pre-allocate Embedding History Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dtype = model.transformer.wte.weight.dtype\n",
    "\n",
    "# Pre-allocate on CPU\n",
    "embedding_history = torch.zeros(\n",
    "    (NUM_TRAIN_STEPS + 1, VOCAB_SIZE, HIDDEN_DIM),\n",
    "    dtype=embedding_dtype\n",
    ")\n",
    "\n",
    "# Save initial state (step 0)\n",
    "embedding_history[0] = model.transformer.wte.weight.data.clone().cpu()\n",
    "\n",
    "initial_norms = torch.norm(embedding_history[0], p=2, dim=1)\n",
    "initial_centroid = embedding_history[0].mean(dim=0)\n",
    "initial_centroid_norm = initial_centroid.norm().item()\n",
    "\n",
    "print(f\"✓ Pre-allocated embedding history tensor\")\n",
    "print(f\"  Shape: {embedding_history.shape}\")\n",
    "print(f\"  Dtype: {embedding_history.dtype}\")\n",
    "print(f\"  Memory: {embedding_history.element_size() * embedding_history.numel() / 1e6:.1f} MB\")\n",
    "print(f\"\\nInitial embeddings (step 0):\")\n",
    "print(f\"  Token L2 norms: min={initial_norms.min().item():.6f}, max={initial_norms.max().item():.6f}, mean={initial_norms.mean().item():.6f}\")\n",
    "print(f\"  Centroid L2 norm: {initial_centroid_norm:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Optimized Snapshot Callback\n",
    "\n",
    "Saves periodically (every N steps) instead of every step to minimize I/O overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedEmbeddingCallback(TrainerCallback):\n",
    "    \"\"\"Save embedding matrix periodically with timing metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_history, output_dir, output_file, save_every_n):\n",
    "        self.embedding_history = embedding_history\n",
    "        self.output_dir = output_dir\n",
    "        self.output_file = output_file\n",
    "        self.output_path = Path(output_dir) / output_file\n",
    "        self.save_every_n = save_every_n\n",
    "        self.last_print_time = time.time()\n",
    "        self.steps_since_print = 0\n",
    "        \n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        step = state.global_step\n",
    "        \n",
    "        # Always store in memory (cheap)\n",
    "        embeddings = model.transformer.wte.weight.data.clone().cpu()\n",
    "        self.embedding_history[step] = embeddings\n",
    "        \n",
    "        # Save to disk periodically\n",
    "        should_save = (step % self.save_every_n == 0) or (step == args.max_steps)\n",
    "        \n",
    "        if should_save:\n",
    "            save_file(\n",
    "                {'embedding_history': self.embedding_history[:step+1]},\n",
    "                self.output_path\n",
    "            )\n",
    "        \n",
    "        # Print throughput every 100 steps\n",
    "        self.steps_since_print += 1\n",
    "        if step % 100 == 0 and step > 0:\n",
    "            elapsed = time.time() - self.last_print_time\n",
    "            throughput = self.steps_since_print / elapsed\n",
    "            \n",
    "            centroid = embeddings.mean(dim=0)\n",
    "            centroid_norm = centroid.norm().item()\n",
    "            \n",
    "            saved_marker = \"[SAVED]\" if should_save else \"\"\n",
    "            print(f\"[Step {step:6d}] {throughput:6.1f} it/s | Centroid: {centroid_norm:.6f} {saved_marker}\")\n",
    "            \n",
    "            self.last_print_time = time.time()\n",
    "            self.steps_since_print = 0\n",
    "        \n",
    "        return control\n",
    "\n",
    "print(f\"✓ Optimized callback defined (save every {SAVE_EVERY_N_STEPS} steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_output\",\n",
    "    max_steps=NUM_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=100,\n",
    "    save_steps=NUM_TRAIN_STEPS + 1,  # Don't save model checkpoints\n",
    "    save_total_limit=0,\n",
    "    seed=RANDOM_SEED,\n",
    "    \n",
    "    # Data loading optimization\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    dataloader_prefetch_factor=PREFETCH_FACTOR,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_persistent_workers=(NUM_WORKERS > 0),\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,\n",
    "    tf32=True,  # Enable TF32 on Ampere/Hopper\n",
    "    \n",
    "    # Disable unnecessary features\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Steps: {training_args.max_steps:,}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} × {GRADIENT_ACCUMULATION} = {effective_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Weight decay: {training_args.weight_decay}\")\n",
    "print(f\"  Precision: bfloat16 + TF32\")\n",
    "print(f\"  Data workers: {NUM_WORKERS}\")\n",
    "print(f\"  Prefetch factor: {PREFETCH_FACTOR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[OptimizedEmbeddingCallback(\n",
    "        embedding_history, \n",
    "        OUTPUT_DIR, \n",
    "        OUTPUT_FILE,\n",
    "        SAVE_EVERY_N_STEPS\n",
    "    )],\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Train!\n",
    "\n",
    "Should see 2000-5000 it/s on H100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Starting training...\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training complete!\")\n",
    "print(f\"Total time: {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "print(f\"Average throughput: {NUM_TRAIN_STEPS / elapsed:.1f} it/s\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Final Save and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure final state is saved\n",
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILE\n",
    "save_file(\n",
    "    {'embedding_history': embedding_history},\n",
    "    output_path\n",
    ")\n",
    "\n",
    "# Analyze final embeddings\n",
    "final_embeddings = embedding_history[-1]\n",
    "final_norms = torch.norm(final_embeddings, p=2, dim=1)\n",
    "final_centroid = final_embeddings.mean(dim=0)\n",
    "final_centroid_norm = final_centroid.norm().item()\n",
    "centroid_displacement = (final_centroid - initial_centroid).norm().item()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TRAINING SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Model: GPT2 ({N_LAYER} layers, {N_HEAD} heads, {HIDDEN_DIM}-dim)\")\n",
    "print(f\"Vocabulary: {VOCAB_SIZE} tokens\")\n",
    "print(f\"Dead tokens: {dead_tokens} ({100 * dead_tokens / VOCAB_SIZE:.1f}%)\")\n",
    "print(f\"Initialization: {INIT_MODE}\")\n",
    "print(f\"Training steps: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"Sequence length: {MAX_SEQ_LEN}\")\n",
    "print(f\"Batch size: {effective_batch_size}\")\n",
    "print(f\"Precision: bfloat16\")\n",
    "print(f\"\\nOutput file: {output_path}\")\n",
    "print(f\"  Shape: {embedding_history.shape}\")\n",
    "print(f\"  Size: {output_path.stat().st_size / 1e6:.1f} MB\")\n",
    "print(f\"\\nInitial centroid norm: {initial_centroid_norm:.6f}\")\n",
    "print(f\"Final centroid norm: {final_centroid_norm:.6f}\")\n",
    "print(f\"Centroid displacement: {centroid_displacement:.6f}\")\n",
    "print(f\"\\nFinal token norms:\")\n",
    "print(f\"  Min: {final_norms.min().item():.6f}\")\n",
    "print(f\"  Max: {final_norms.max().item():.6f}\")\n",
    "print(f\"  Mean: {final_norms.mean().item():.6f}\")\n",
    "print(f\"  Std: {final_norms.std().item():.6f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
