{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 08.2a_rtx6000ada: RTX 6000 Ada Optimized Training\n",
    "\n",
    "**Tuned for RTX 6000 Ada (48GB VRAM, 62GB RAM, 14 vCPUs)**\n",
    "\n",
    "This notebook balances model size, batch size, and sequence length to achieve good GPU utilization on a single RTX 6000 Ada card.\n",
    "\n",
    "## Target Performance\n",
    "\n",
    "- RTX 6000 Ada theoretical: ~91 TFLOPS bfloat16\n",
    "- Target MFU: ~30-40% (realistic for small-scale training)\n",
    "- Expected throughput: 100-500 it/s\n",
    "\n",
    "## Parameters\n",
    "\n",
    "**Tuning guide:**\n",
    "- If GPU utilization low: increase `BATCH_SIZE` or `MAX_SEQ_LEN`\n",
    "- If OOM: decrease `BATCH_SIZE` or `HIDDEN_DIM`\n",
    "- If too slow: decrease `NUM_TRAIN_STEPS` or `SAVE_EVERY_N_STEPS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TUNABLE PARAMETERS - Adjust these for experimentation\n",
    "# ============================================================================\n",
    "\n",
    "# Model architecture\n",
    "VOCAB_SIZE = 128           # ASCII byte vocabulary\n",
    "HIDDEN_DIM = 256          # Embedding dimension (256/512/768/1024)\n",
    "N_LAYER = 6                # Transformer layers (4/6/8/12)\n",
    "N_HEAD = 8                 # Attention heads (4/8/16)\n",
    "MAX_SEQ_LEN = 1024         # Context window (512/1024/2048)\n",
    "\n",
    "# Training (start conservative, scale up)\n",
    "BATCH_SIZE = 64            # Per-device batch size (32/64/128/256)\n",
    "GRADIENT_ACCUMULATION = 1  # Effective batch = BATCH_SIZE × this\n",
    "NUM_TRAIN_STEPS = 10000    # Total training steps\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Initialization\n",
    "INIT_MODE = \"qwen\"         # \"normal\" or \"qwen\"\n",
    "\n",
    "# Checkpointing\n",
    "SAVE_EVERY_N_STEPS = 100   # Snapshot frequency (50/100/500)\n",
    "\n",
    "# Data loading\n",
    "NUM_WORKERS = 0            # MUST be 0 for GPU dataset (no multiprocessing)\n",
    "\n",
    "# Data\n",
    "CORPUS_PATH = \"../data/training_corpus.txt\"\n",
    "OUTPUT_DIR = f\"../data/embeddings_{VOCAB_SIZE}vocab_{INIT_MODE}init_rtx6000ada\"\n",
    "OUTPUT_FILE = \"embedding_evolution.safetensors\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ============================================================================\n",
    "# END TUNABLE PARAMETERS\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Model Size Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate model size before training\n",
    "def estimate_model_size(vocab, hidden, layers, heads, seq_len):\n",
    "    \"\"\"Rough parameter count estimate for GPT-2 style model.\"\"\"\n",
    "    # Embeddings (input + output tied)\n",
    "    emb = vocab * hidden\n",
    "    \n",
    "    # Per layer: attention + FFN\n",
    "    attn = 4 * hidden * hidden  # QKV + output projection\n",
    "    ffn = 8 * hidden * hidden   # 4x expansion\n",
    "    layer_params = (attn + ffn) * layers\n",
    "    \n",
    "    total = emb + layer_params\n",
    "    return total\n",
    "\n",
    "estimated_params = estimate_model_size(VOCAB_SIZE, HIDDEN_DIM, N_LAYER, N_HEAD, MAX_SEQ_LEN)\n",
    "estimated_size_mb = estimated_params * 2 / 1e6  # bfloat16 = 2 bytes\n",
    "\n",
    "print(f\"Estimated model size:\")\n",
    "print(f\"  Parameters: {estimated_params:,}\")\n",
    "print(f\"  Memory (bfloat16): {estimated_size_mb:.1f} MB\")\n",
    "print(f\"\\nBatch memory estimate:\")\n",
    "print(f\"  Activations per example: ~{MAX_SEQ_LEN * HIDDEN_DIM * N_LAYER * 4 / 1e6:.1f} MB\")\n",
    "print(f\"  Batch of {BATCH_SIZE}: ~{BATCH_SIZE * MAX_SEQ_LEN * HIDDEN_DIM * N_LAYER * 4 / 1e6:.1f} MB\")\n",
    "print(f\"  Total estimate: ~{estimated_size_mb + BATCH_SIZE * MAX_SEQ_LEN * HIDDEN_DIM * N_LAYER * 4 / 1e6:.1f} MB\")\n",
    "print(f\"\\nShould fit easily in 48GB VRAM ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, TrainerCallback\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "import time\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Enable TF32 for Ada architecture\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Hardware Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"This notebook requires CUDA\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "cuda_cap = torch.cuda.get_device_capability(0)\n",
    "\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
    "print(f\"CUDA Capability: {cuda_cap[0]}.{cuda_cap[1]}\")\n",
    "print(f\"TF32 enabled: {torch.backends.cuda.matmul.allow_tf32}\")\n",
    "print(f\"\\n✓ Hardware ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Load Corpus (CPU → GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "\n",
    "with open(CORPUS_PATH, 'r', encoding='ascii') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "corpus_bytes = [b for b in corpus_text.encode('ascii') if b < VOCAB_SIZE]\n",
    "\n",
    "unique_bytes = set(corpus_bytes)\n",
    "dead_tokens = VOCAB_SIZE - len(unique_bytes)\n",
    "\n",
    "print(f\"  Total bytes: {len(corpus_bytes):,}\")\n",
    "print(f\"  Unique: {len(unique_bytes)} / {VOCAB_SIZE}\")\n",
    "print(f\"  Dead tokens: {dead_tokens} ({100 * dead_tokens / VOCAB_SIZE:.1f}%)\")\n",
    "\n",
    "# Pre-load to GPU\n",
    "corpus_tensor = torch.tensor(corpus_bytes, dtype=torch.long, device=device)\n",
    "print(f\"\\n✓ Corpus on GPU: {corpus_tensor.numel() * corpus_tensor.element_size() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## GPU Dataset (Zero-Copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUByteDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = GPUByteDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "print(f\"✓ Dataset: {len(dataset):,} examples\")\n",
    "print(f\"  Tokens/example: {MAX_SEQ_LEN}\")\n",
    "print(f\"  Tokens/epoch: {len(dataset) * MAX_SEQ_LEN:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "embedding_params = model.transformer.wte.weight.numel()\n",
    "model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / 1e6\n",
    "\n",
    "print(f\"Model initialized:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Embedding parameters: {embedding_params:,}\")\n",
    "print(f\"  Model size: {model_size_mb:.2f} MB (bfloat16)\")\n",
    "print(f\"  Layers: {config.n_layer}, Heads: {config.n_head}, Hidden: {config.n_embd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Initialize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INIT_MODE == \"qwen\":\n",
    "    print(f\"\\nQwen-style initialization (singular unit vector)\")\n",
    "    with torch.no_grad():\n",
    "        random_vector = torch.randn(HIDDEN_DIM, device=device)\n",
    "        random_vector = random_vector / random_vector.norm()\n",
    "        model.transformer.wte.weight[:] = random_vector\n",
    "    print(f\"  All {VOCAB_SIZE} tokens → unit vector (norm={random_vector.norm().item():.6f})\")\n",
    "else:\n",
    "    print(f\"\\nNormal initialization (default PyTorch)\")\n",
    "    with torch.no_grad():\n",
    "        norms = torch.norm(model.transformer.wte.weight, p=2, dim=1)\n",
    "        print(f\"  Token norms: min={norms.min().item():.6f}, max={norms.max().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Pre-allocate Embedding History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_history = torch.zeros(\n",
    "    (NUM_TRAIN_STEPS + 1, VOCAB_SIZE, HIDDEN_DIM),\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "embedding_history[0] = model.transformer.wte.weight.data.clone().cpu()\n",
    "\n",
    "initial_centroid = embedding_history[0].mean(dim=0)\n",
    "initial_centroid_norm = initial_centroid.norm().item()\n",
    "\n",
    "history_size_mb = embedding_history.element_size() * embedding_history.numel() / 1e6\n",
    "\n",
    "print(f\"Embedding history allocated:\")\n",
    "print(f\"  Shape: {embedding_history.shape}\")\n",
    "print(f\"  Memory: {history_size_mb:.1f} MB\")\n",
    "print(f\"  Initial centroid norm: {initial_centroid_norm:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Snapshot Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingSnapshotCallback(TrainerCallback):\n",
    "    def __init__(self, embedding_history, output_dir, output_file, save_every_n):\n",
    "        self.embedding_history = embedding_history\n",
    "        self.output_path = Path(output_dir) / output_file\n",
    "        self.save_every_n = save_every_n\n",
    "        self.last_time = time.time()\n",
    "        self.steps_since_print = 0\n",
    "        \n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        step = state.global_step\n",
    "        \n",
    "        # Store in memory\n",
    "        self.embedding_history[step] = model.transformer.wte.weight.data.clone().cpu()\n",
    "        \n",
    "        # Save to disk periodically\n",
    "        should_save = (step % self.save_every_n == 0) or (step == args.max_steps)\n",
    "        if should_save:\n",
    "            save_file(\n",
    "                {'embedding_history': self.embedding_history[:step+1]},\n",
    "                self.output_path\n",
    "            )\n",
    "        \n",
    "        # Print every 100 steps\n",
    "        self.steps_since_print += 1\n",
    "        if step % 100 == 0 and step > 0:\n",
    "            elapsed = time.time() - self.last_time\n",
    "            throughput = self.steps_since_print / elapsed\n",
    "            \n",
    "            embeddings = self.embedding_history[step]\n",
    "            centroid_norm = embeddings.mean(dim=0).norm().item()\n",
    "            \n",
    "            marker = \"[SAVED]\" if should_save else \"\"\n",
    "            print(f\"[{step:5d}] {throughput:6.1f} it/s | centroid: {centroid_norm:.6f} {marker}\")\n",
    "            \n",
    "            self.last_time = time.time()\n",
    "            self.steps_since_print = 0\n",
    "        \n",
    "        return control\n",
    "\n",
    "print(f\"✓ Callback ready (save every {SAVE_EVERY_N_STEPS} steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_batch = BATCH_SIZE * GRADIENT_ACCUMULATION\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_output\",\n",
    "    max_steps=NUM_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=100,\n",
    "    \n",
    "    # Checkpointing (disable model saves, we only want embeddings)\n",
    "    save_steps=NUM_TRAIN_STEPS + 1,\n",
    "    save_total_limit=0,\n",
    "    \n",
    "    # Data loading (MUST be 0 for GPU dataset)\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    dataloader_pin_memory=False,\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    \n",
    "    # Misc\n",
    "    seed=RANDOM_SEED,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Steps: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE} × {GRADIENT_ACCUMULATION} = {effective_batch}\")\n",
    "print(f\"  Tokens/step: {effective_batch * MAX_SEQ_LEN:,}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Precision: bfloat16 + TF32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[EmbeddingSnapshotCallback(\n",
    "        embedding_history,\n",
    "        OUTPUT_DIR,\n",
    "        OUTPUT_FILE,\n",
    "        SAVE_EVERY_N_STEPS\n",
    "    )],\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Starting training...\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "avg_throughput = NUM_TRAIN_STEPS / elapsed\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training complete\")\n",
    "print(f\"Total time: {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "print(f\"Average: {avg_throughput:.1f} it/s\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Final Save & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final save\n",
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILE\n",
    "save_file({'embedding_history': embedding_history}, output_path)\n",
    "\n",
    "# Analysis\n",
    "final_embeddings = embedding_history[-1]\n",
    "final_norms = torch.norm(final_embeddings, p=2, dim=1)\n",
    "final_centroid = final_embeddings.mean(dim=0)\n",
    "final_centroid_norm = final_centroid.norm().item()\n",
    "centroid_displacement = (final_centroid - initial_centroid).norm().item()\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Model: {total_params:,} params, {N_LAYER} layers\")\n",
    "print(f\"  Init: {INIT_MODE}\")\n",
    "print(f\"  Steps: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"  Batch: {effective_batch} × {MAX_SEQ_LEN} tokens\")\n",
    "print(f\"\\nEmbedding evolution:\")\n",
    "print(f\"  Initial centroid norm: {initial_centroid_norm:.6f}\")\n",
    "print(f\"  Final centroid norm: {final_centroid_norm:.6f}\")\n",
    "print(f\"  Displacement: {centroid_displacement:.6f}\")\n",
    "print(f\"\\nFinal token norms:\")\n",
    "print(f\"  Min: {final_norms.min().item():.6f}\")\n",
    "print(f\"  Max: {final_norms.max().item():.6f}\")\n",
    "print(f\"  Mean: {final_norms.mean().item():.6f}\")\n",
    "print(f\"\\nSaved to: {output_path}\")\n",
    "print(f\"File size: {output_path.stat().st_size / 1e6:.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
