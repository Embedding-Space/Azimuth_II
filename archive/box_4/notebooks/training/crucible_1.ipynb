{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Crucible 1: The Full Evolution\n",
    "\n",
    "A clean-slate experiment to capture the complete lifecycle of dead token dynamics, from supernova through Fimbulwinter.\n",
    "\n",
    "**What's new:**\n",
    "- Hyperparameters tuned for faster freezing (lr=1e-3, no weight decay)\n",
    "- Lattice displacement ΔW′ computed live, not post-hoc\n",
    "- Last-motion tracker to pinpoint when each token freezes\n",
    "- Centroid tracking for cloud drift analysis\n",
    "\n",
    "**Data captured:**\n",
    "- W[t]: dead token embeddings (bfloat16)\n",
    "- ΔW′[t]: displacement in lattice-cell units (float32)\n",
    "- centroid[t]: cloud center of mass (float32)\n",
    "- loss[t]: training loss\n",
    "- last_motion_step: per-token freeze time (computed in-place)\n",
    "\n",
    "**Goal:** Validate that we capture the full evolution—supernova, cooling, stumbling, Fimbulwinter—with statistical confidence that we've reached the true end state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crucible 1: 5000 steps, lr=0.001, weight_decay=0.0\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "TOTAL_STEPS = 5000\n",
    "BATCH_SIZE = 128\n",
    "SEQ_LEN = 128\n",
    "LEARNING_RATE = 1e-3      # Faster than Thimble 8/9, matches Thimble 7\n",
    "WEIGHT_DECAY = 0.0        # No weight decay—removes confounding variable\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "EPSILON = 1e-8\n",
    "\n",
    "# Model parameters\n",
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 64\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 2\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Motion detection threshold (for last_motion_step tracking)\n",
    "MOTION_THRESHOLD = 0.1  # |ΔW′| below this counts as \"frozen\"\n",
    "\n",
    "# Paths\n",
    "CORPUS_PATH = '../../data/flannel_model_corpus.txt'\n",
    "TOKENIZER_PATH = '../../data/flannel_tokenizer_chars.json'\n",
    "DEAD_MASK_PATH = '../../tensors/Flannel/live_dead_tokens.safetensors'\n",
    "OUTPUT_DIR = '../../tensors/Crucible-1'\n",
    "\n",
    "print(f\"Crucible 1: {TOTAL_STEPS} steps, lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from safetensors.torch import save_file, load_file\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(f\"Random seed set to {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Dead Token Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer with vocab size 10000\n",
      "Dead tokens: 3699\n",
      "Live tokens: 6301\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "print(f\"Loaded tokenizer with vocab size {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "masks = load_file(DEAD_MASK_PATH)\n",
    "dead_mask = masks['dead_mask'].bool()\n",
    "dead_indices = masks['dead_indices'].long()\n",
    "n_dead = dead_mask.sum().item()\n",
    "\n",
    "print(f\"Dead tokens: {n_dead}\")\n",
    "print(f\"Live tokens: {VOCAB_SIZE - n_dead}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 10713 sequences of length 128\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, corpus_path, tokenizer, seq_len):\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        encoding = tokenizer.encode(text)\n",
    "        self.tokens = encoding.ids\n",
    "        self.seq_len = seq_len\n",
    "        self.n_sequences = len(self.tokens) // seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_sequences\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_len\n",
    "        chunk = self.tokens[start:start + self.seq_len]\n",
    "        return torch.tensor(chunk, dtype=torch.long)\n",
    "\n",
    "\n",
    "dataset = TextDataset(CORPUS_PATH, tokenizer, SEQ_LEN)\n",
    "print(f\"Dataset: {len(dataset)} sequences of length {SEQ_LEN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 748,288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jefferyharrell/Projects/Azimuth_II/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, num_heads, seq_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(seq_len, hidden_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=0.0,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.ln_f = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.pos_embedding.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        tok_emb = self.embedding(input_ids)\n",
    "        pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embedding(pos_ids)\n",
    "        \n",
    "        hidden = tok_emb + pos_emb\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len, device=input_ids.device)\n",
    "        hidden = self.transformer(hidden, mask=causal_mask, is_causal=True)\n",
    "        hidden = self.ln_f(hidden)\n",
    "        \n",
    "        logits = hidden @ self.embedding.weight.T\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = TinyLM(VOCAB_SIZE, HIDDEN_DIM, NUM_LAYERS, NUM_HEADS, SEQ_LEN)\n",
    "model = model.to(device).to(torch.bfloat16)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: AdamW (lr=0.001, weight_decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(BETA1, BETA2),\n",
    "    eps=EPSILON,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## ULP Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ulp_bf16(tensor_bf16: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute ULP for each element of a bfloat16 tensor.\"\"\"\n",
    "    bits = tensor_bf16.view(torch.uint16).to(torch.int32)\n",
    "    exponent = (bits >> 7) & 0xFF\n",
    "    effective_exp = torch.where(exponent == 0, torch.ones_like(exponent), exponent)\n",
    "    ulp = torch.pow(2.0, (effective_exp - 134).float())\n",
    "    return ulp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Data Collection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-allocated 7.10 GB for data collection\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pre-allocate tensors\n",
    "W_history = torch.zeros(TOTAL_STEPS + 1, n_dead, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "delta_W_prime_history = torch.zeros(TOTAL_STEPS, n_dead, HIDDEN_DIM, dtype=torch.float32)\n",
    "centroid_history = torch.zeros(TOTAL_STEPS + 1, HIDDEN_DIM, dtype=torch.float32)\n",
    "loss_history = torch.zeros(TOTAL_STEPS + 1, dtype=torch.float32)\n",
    "\n",
    "# Last motion tracker: -1 means \"never moved\" (shouldn't happen), updated each step\n",
    "last_motion_step = torch.full((n_dead,), -1, dtype=torch.int32)\n",
    "\n",
    "# Memory estimate\n",
    "total_bytes = (\n",
    "    W_history.numel() * 2 +\n",
    "    delta_W_prime_history.numel() * 4 +\n",
    "    centroid_history.numel() * 4 +\n",
    "    loss_history.numel() * 4 +\n",
    "    last_motion_step.numel() * 4\n",
    ")\n",
    "print(f\"Pre-allocated {total_bytes / 1e9:.2f} GB for data collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 5000 steps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5000/5000 [05:17<00:00, 15.73it/s, loss=5.2500, epoch=61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Final loss: 5.2500\n",
      "Completed 61 epochs\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Capture initial state (t=0)\n",
    "W_prev = model.embedding.weight.detach().cpu()[dead_mask].to(torch.bfloat16)\n",
    "W_history[0] = W_prev\n",
    "centroid_history[0] = W_prev.float().mean(dim=0)\n",
    "loss_history[0] = float('nan')\n",
    "\n",
    "print(f\"Starting training for {TOTAL_STEPS} steps...\")\n",
    "\n",
    "model.train()\n",
    "step = 0\n",
    "epoch = 0\n",
    "\n",
    "pbar = tqdm(total=TOTAL_STEPS, desc=\"Training\")\n",
    "\n",
    "while step < TOTAL_STEPS:\n",
    "    epoch += 1\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        if step >= TOTAL_STEPS:\n",
    "            break\n",
    "            \n",
    "        input_ids = batch.to(device)\n",
    "        \n",
    "        with torch.autocast(device_type=device if device != 'mps' else 'cpu', dtype=torch.bfloat16):\n",
    "            logits = model(input_ids)\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            loss = loss_fn(shift_logits.view(-1, VOCAB_SIZE), shift_labels.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # === Capture state ===\n",
    "        W_curr = model.embedding.weight.detach().cpu()[dead_mask].to(torch.bfloat16)\n",
    "        \n",
    "        # W[t]\n",
    "        W_history[step] = W_curr\n",
    "        \n",
    "        # ΔW = W[t] - W[t-1]\n",
    "        delta_W = W_curr.float() - W_prev.float()\n",
    "        \n",
    "        # ULP at W[t-1]\n",
    "        ulp = compute_ulp_bf16(W_prev)\n",
    "        \n",
    "        # ΔW′ = ΔW / ULP (lattice displacement)\n",
    "        delta_W_prime = delta_W / ulp\n",
    "        delta_W_prime_history[step - 1] = delta_W_prime  # step-1 because ΔW′ is indexed from 0\n",
    "        \n",
    "        # Centroid\n",
    "        centroid_history[step] = W_curr.float().mean(dim=0)\n",
    "        \n",
    "        # Loss\n",
    "        loss_history[step] = loss.item()\n",
    "        \n",
    "        # Update last_motion_step for tokens that moved\n",
    "        displacement_magnitude = torch.norm(delta_W_prime, dim=1)  # (n_dead,)\n",
    "        moved = displacement_magnitude >= MOTION_THRESHOLD\n",
    "        last_motion_step[moved] = step\n",
    "        \n",
    "        # Update W_prev for next iteration\n",
    "        W_prev = W_curr\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'epoch': epoch})\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "print(f\"\\nTraining complete. Final loss: {loss.item():.4f}\")\n",
    "print(f\"Completed {epoch} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Quick Fimbulwinter Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FIMBULWINTER CHECK\n",
      "============================================================\n",
      "Last motion (any token): step 2509\n",
      "Fimbulwinter duration: 2491 steps\n",
      "Fimbulwinter fraction: 49.8%\n",
      "\n",
      "Freeze time distribution:\n",
      "  Earliest freeze: step 560\n",
      "  Median freeze: step 1543\n",
      "  Latest freeze: step 2509\n",
      "\n",
      "✓ All 3699 tokens moved at least once\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FIMBULWINTER CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Last motion across all tokens\n",
    "global_last_motion = last_motion_step.max().item()\n",
    "fimbulwinter_duration = TOTAL_STEPS - global_last_motion\n",
    "\n",
    "print(f\"Last motion (any token): step {global_last_motion}\")\n",
    "print(f\"Fimbulwinter duration: {fimbulwinter_duration} steps\")\n",
    "print(f\"Fimbulwinter fraction: {fimbulwinter_duration / TOTAL_STEPS:.1%}\")\n",
    "\n",
    "# Distribution of freeze times\n",
    "print(f\"\\nFreeze time distribution:\")\n",
    "print(f\"  Earliest freeze: step {last_motion_step.min().item()}\")\n",
    "print(f\"  Median freeze: step {last_motion_step.float().median().item():.0f}\")\n",
    "print(f\"  Latest freeze: step {last_motion_step.max().item()}\")\n",
    "\n",
    "# Check for any tokens that never moved (shouldn't happen)\n",
    "never_moved = (last_motion_step == -1).sum().item()\n",
    "if never_moved > 0:\n",
    "    print(f\"\\n⚠️  {never_moved} tokens never moved (unexpected!)\")\n",
    "else:\n",
    "    print(f\"\\n✓ All {n_dead} tokens moved at least once\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Stillness Period Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STILLNESS PERIOD ANALYSIS\n",
      "============================================================\n",
      "Total stillness periods: 58\n",
      "\n",
      "Longest 10:\n",
      "  1. 2491 steps\n",
      "  2. 63 steps\n",
      "  3. 43 steps\n",
      "  4. 37 steps\n",
      "  5. 34 steps\n",
      "  6. 33 steps\n",
      "  7. 25 steps\n",
      "  8. 22 steps\n",
      "  9. 20 steps\n",
      "  10. 19 steps\n",
      "\n",
      "Final vs second-longest: 2491 / 63 = 39.5x\n",
      "\n",
      "Mean stillness: 51.8 steps\n",
      "Median stillness: 4 steps\n"
     ]
    }
   ],
   "source": [
    "# Compute displacement magnitude per step\n",
    "displacement_per_step = torch.norm(delta_W_prime_history, dim=2)  # (TOTAL_STEPS, n_dead)\n",
    "any_motion = (displacement_per_step >= MOTION_THRESHOLD).any(dim=1)  # (TOTAL_STEPS,) bool\n",
    "\n",
    "# Find runs of stillness\n",
    "stillness_runs = []\n",
    "current_run = 0\n",
    "\n",
    "for moved in any_motion:\n",
    "    if not moved:\n",
    "        current_run += 1\n",
    "    else:\n",
    "        if current_run > 0:\n",
    "            stillness_runs.append(current_run)\n",
    "        current_run = 0\n",
    "\n",
    "if current_run > 0:\n",
    "    stillness_runs.append(current_run)\n",
    "\n",
    "stillness_runs = sorted(stillness_runs, reverse=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STILLNESS PERIOD ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total stillness periods: {len(stillness_runs)}\")\n",
    "print(f\"\\nLongest 10:\")\n",
    "for i, run in enumerate(stillness_runs[:10]):\n",
    "    print(f\"  {i+1}. {run} steps\")\n",
    "\n",
    "if len(stillness_runs) >= 2:\n",
    "    print(f\"\\nFinal vs second-longest: {stillness_runs[0]} / {stillness_runs[1]} = {stillness_runs[0]/stillness_runs[1]:.1f}x\")\n",
    "\n",
    "print(f\"\\nMean stillness: {sum(stillness_runs)/len(stillness_runs):.1f} steps\")\n",
    "print(f\"Median stillness: {sorted(stillness_runs)[len(stillness_runs)//2]} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../../tensors/Crucible-1/crucible_1_trajectory.safetensors\n",
      "File size: 7.10 GB\n"
     ]
    }
   ],
   "source": [
    "data_to_save = {\n",
    "    'W': W_history.view(torch.uint16),  # (5001, 3699, 64) uint16\n",
    "    'delta_W_prime': delta_W_prime_history,  # (5000, 3699, 64) float32\n",
    "    'centroid': centroid_history,  # (5001, 64) float32\n",
    "    'loss': loss_history,  # (5001,) float32\n",
    "    'last_motion_step': last_motion_step,  # (3699,) int32\n",
    "    'dead_mask': dead_mask,\n",
    "    'dead_indices': dead_indices,\n",
    "}\n",
    "\n",
    "save_path = output_path / 'crucible_1_trajectory.safetensors'\n",
    "save_file(data_to_save, str(save_path))\n",
    "\n",
    "print(f\"Saved to {save_path}\")\n",
    "print(f\"File size: {save_path.stat().st_size / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metadata.json\n",
      "\n",
      "============================================================\n",
      "CRUCIBLE 1 COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "    'experiment': 'Crucible 1',\n",
    "    'series': 'Crucible',\n",
    "    'date': '2025-11-25',\n",
    "    'total_steps': TOTAL_STEPS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'seq_len': SEQ_LEN,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'beta1': BETA1,\n",
    "    'beta2': BETA2,\n",
    "    'epsilon': EPSILON,\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'random_seed': RANDOM_SEED,\n",
    "    'motion_threshold': MOTION_THRESHOLD,\n",
    "    'n_dead_tokens': n_dead,\n",
    "    'final_loss': loss_history[-1].item(),\n",
    "    'total_epochs': epoch,\n",
    "    'device': device,\n",
    "    'global_last_motion': int(global_last_motion),\n",
    "    'fimbulwinter_duration': int(fimbulwinter_duration),\n",
    "    'data_shapes': {\n",
    "        'W': list(W_history.shape),\n",
    "        'delta_W_prime': list(delta_W_prime_history.shape),\n",
    "        'centroid': list(centroid_history.shape),\n",
    "        'loss': list(loss_history.shape),\n",
    "        'last_motion_step': list(last_motion_step.shape),\n",
    "    },\n",
    "    'notes': 'First Crucible experiment. Fast lr, no weight decay. Full lifecycle capture.'\n",
    "}\n",
    "\n",
    "with open(output_path / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Saved metadata.json\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CRUCIBLE 1 COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
