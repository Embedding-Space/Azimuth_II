{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thimble 8: Dead Token Dynamics Training Run\n",
    "\n",
    "Train a minimal language model and capture full optimizer state for dead tokens at every step.\n",
    "\n",
    "**Focus:** First 3,500 training steps—the supernova, cooling, and fimbulwinter onset.\n",
    "\n",
    "**Data captured per step (dead tokens only):**\n",
    "- W: embedding weights (bfloat16)\n",
    "- m: Adam momentum / first moment (float32)\n",
    "- v: Adam variance / second moment (float32)  \n",
    "- g: gradients (float32)\n",
    "- loss: training loss (float32 scalar)\n",
    "\n",
    "**Checkpoints:** Full model + optimizer state at t = 0, 1, 10, 100, 500, 1000, 2000, 3000, 3500\n",
    "\n",
    "**Expected output:** ~11.6 GB in `box_4/tensors/Thimble-8/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "TOTAL_STEPS = 4000\n",
    "BATCH_SIZE = 128\n",
    "SEQ_LEN = 128\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.1\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "EPSILON = 1e-8\n",
    "\n",
    "# Model parameters\n",
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 64\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 2\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Checkpointing\n",
    "CHECKPOINT_STEPS = [0, 1, 10, 100, 500, 1000, 2000, 3000, 3500, 4000]\n",
    "\n",
    "# Paths (relative to notebook location: box_4/notebooks/training/)\n",
    "CORPUS_PATH = '../../data/flannel_model_corpus.txt'\n",
    "TOKENIZER_PATH = '../../data/flannel_tokenizer_chars.json'\n",
    "DEAD_MASK_PATH = '../../tensors/Flannel/live_dead_tokens.safetensors'\n",
    "OUTPUT_DIR = '../../tensors/Thimble-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from safetensors.torch import save_file, load_file\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(f\"Random seed set to {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Dead Token Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer with vocab size 10000\n",
      "Dead tokens: 3699\n",
      "Live tokens: 6301\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer (using tokenizers library, same as Thimble 7)\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "print(f\"Loaded tokenizer with vocab size {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "# Load dead token mask\n",
    "masks = load_file(DEAD_MASK_PATH)\n",
    "dead_mask = masks['dead_mask'].bool()\n",
    "dead_indices = masks['dead_indices'].long()\n",
    "n_dead = dead_mask.sum().item()\n",
    "\n",
    "print(f\"Dead tokens: {n_dead}\")\n",
    "print(f\"Live tokens: {VOCAB_SIZE - n_dead}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 10713 sequences of length 128\n",
      "Total tokens: 1,371,264\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Simple dataset that returns chunks of tokenized text.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus_path, tokenizer, seq_len):\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Tokenize entire corpus (tokenizers library returns Encoding object)\n",
    "        encoding = tokenizer.encode(text)\n",
    "        self.tokens = encoding.ids\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Number of complete sequences we can make\n",
    "        self.n_sequences = len(self.tokens) // seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_sequences\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_len\n",
    "        chunk = self.tokens[start:start + self.seq_len]\n",
    "        return torch.tensor(chunk, dtype=torch.long)\n",
    "\n",
    "\n",
    "dataset = TextDataset(CORPUS_PATH, tokenizer, SEQ_LEN)\n",
    "print(f\"Dataset: {len(dataset)} sequences of length {SEQ_LEN}\")\n",
    "print(f\"Total tokens: {len(dataset) * SEQ_LEN:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Minimal transformer with tied embeddings (E = W^T), matching Qwen architecture choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 748,288\n",
      "Embedding shape: torch.Size([10000, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jefferyharrell/Projects/Azimuth_II/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class TinyLM(nn.Module):\n",
    "    \"\"\"Minimal language model with tied embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embeddings - this is W^T, the transpose of unembedding\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        self.pos_embedding = nn.Embedding(SEQ_LEN, hidden_dim)\n",
    "        \n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=0.0,  # No dropout for reproducibility\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-norm like modern architectures\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Output projection (tied to embedding)\n",
    "        # We'll compute logits as: hidden @ embedding.weight.T\n",
    "        \n",
    "        # Initialize embeddings with N(0, 0.02)\n",
    "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.pos_embedding.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Get embeddings\n",
    "        tok_emb = self.embedding(input_ids)\n",
    "        pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embedding(pos_ids)\n",
    "        \n",
    "        hidden = tok_emb + pos_emb\n",
    "        \n",
    "        # Causal mask\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len, device=input_ids.device)\n",
    "        \n",
    "        # Transform\n",
    "        hidden = self.transformer(hidden, mask=causal_mask, is_causal=True)\n",
    "        hidden = self.ln_f(hidden)\n",
    "        \n",
    "        # Tied output projection: logits = hidden @ W where W = embedding.weight\n",
    "        logits = hidden @ self.embedding.weight.T\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = TinyLM(VOCAB_SIZE, HIDDEN_DIM, NUM_LAYERS, NUM_HEADS)\n",
    "model = model.to(device).to(torch.bfloat16)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "print(f\"Embedding shape: {model.embedding.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: AdamW\n",
      "  lr=0.0003, weight_decay=0.1\n",
      "  betas=(0.9, 0.999), eps=1e-08\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(BETA1, BETA2),\n",
    "    eps=EPSILON,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"  lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY}\")\n",
    "print(f\"  betas=({BETA1}, {BETA2}), eps={EPSILON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-allocated 13.26 GB for data collection\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_path = output_path / 'checkpoints'\n",
    "checkpoint_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Pre-allocate tensors for data collection (on CPU to save GPU memory)\n",
    "# Shape: (steps+1, n_dead_tokens, hidden_dim) - +1 because we capture t=0\n",
    "W_history = torch.zeros(TOTAL_STEPS + 1, n_dead, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "m_history = torch.zeros(TOTAL_STEPS + 1, n_dead, HIDDEN_DIM, dtype=torch.float32)\n",
    "v_history = torch.zeros(TOTAL_STEPS + 1, n_dead, HIDDEN_DIM, dtype=torch.float32)\n",
    "g_history = torch.zeros(TOTAL_STEPS + 1, n_dead, HIDDEN_DIM, dtype=torch.float32)\n",
    "loss_history = torch.zeros(TOTAL_STEPS + 1, dtype=torch.float32)\n",
    "\n",
    "# Memory estimate\n",
    "total_bytes = (\n",
    "    W_history.numel() * 2 +  # bfloat16\n",
    "    m_history.numel() * 4 +  # float32\n",
    "    v_history.numel() * 4 +\n",
    "    g_history.numel() * 4 +\n",
    "    loss_history.numel() * 4\n",
    ")\n",
    "print(f\"Pre-allocated {total_bytes / 1e9:.2f} GB for data collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding parameter ID: 0\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_param_id(model):\n",
    "    \"\"\"Get the parameter ID for the embedding weights in optimizer state.\"\"\"\n",
    "    for i, (name, param) in enumerate(model.named_parameters()):\n",
    "        if name == 'embedding.weight':\n",
    "            return i\n",
    "    raise ValueError(\"Could not find embedding.weight parameter\")\n",
    "\n",
    "\n",
    "def capture_state(model, optimizer, step, dead_mask, loss_val=0.0):\n",
    "    \"\"\"Capture current state for dead tokens.\"\"\"\n",
    "    \n",
    "    # Get embedding weights (dead tokens only)\n",
    "    W = model.embedding.weight.detach().cpu()[dead_mask]\n",
    "    W_history[step] = W.to(torch.bfloat16)\n",
    "    \n",
    "    # Get optimizer state for embeddings\n",
    "    emb_param_id = get_embedding_param_id(model)\n",
    "    emb_param = list(model.parameters())[emb_param_id]\n",
    "    \n",
    "    if emb_param in optimizer.state:\n",
    "        state = optimizer.state[emb_param]\n",
    "        # Momentum (exp_avg)\n",
    "        m_history[step] = state['exp_avg'].detach().cpu()[dead_mask].float()\n",
    "        # Variance (exp_avg_sq)\n",
    "        v_history[step] = state['exp_avg_sq'].detach().cpu()[dead_mask].float()\n",
    "    # else: stays zero (step 0, before any optimizer step)\n",
    "    \n",
    "    # Gradients (if they exist)\n",
    "    if emb_param.grad is not None:\n",
    "        g_history[step] = emb_param.grad.detach().cpu()[dead_mask].float()\n",
    "    # else: stays zero\n",
    "    \n",
    "    # Loss\n",
    "    loss_history[step] = loss_val\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, step, path):\n",
    "    \"\"\"Save full model and optimizer checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'rng_state': torch.get_rng_state(),\n",
    "    }\n",
    "    if device == 'cuda':\n",
    "        checkpoint['cuda_rng_state'] = torch.cuda.get_rng_state_all()\n",
    "    \n",
    "    torch.save(checkpoint, path / f'checkpoint_step_{step:05d}.pt')\n",
    "\n",
    "\n",
    "print(f\"Embedding parameter ID: {get_embedding_param_id(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 0\n",
      "\n",
      "Starting training for 4000 steps...\n",
      "Checkpoints at: [0, 1, 10, 100, 500, 1000, 2000, 3000, 3500, 4000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3/4000 [00:01<24:20,  2.74it/s, loss=9.1250, epoch=1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 13/4000 [00:01<05:49, 11.41it/s, loss=8.9375, epoch=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 101/4000 [00:07<04:31, 14.35it/s, loss=7.6250, epoch=2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 503/4000 [00:33<03:51, 15.10it/s, loss=6.9688, epoch=7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 1003/4000 [01:05<03:16, 15.25it/s, loss=6.6562, epoch=13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 2003/4000 [02:07<02:08, 15.53it/s, loss=6.3750, epoch=25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 3003/4000 [03:10<01:04, 15.40it/s, loss=6.3438, epoch=37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 3503/4000 [03:42<00:32, 15.46it/s, loss=6.2812, epoch=43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4000/4000 [04:13<00:00, 15.76it/s, loss=6.2812, epoch=49]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at step 4000\n",
      "\n",
      "Training complete. Final loss: 6.2812\n",
      "Completed 49 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# DataLoader with shuffling\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True  # Ensure consistent batch sizes\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Capture initial state (t=0, before any training)\n",
    "capture_state(model, optimizer, step=0, dead_mask=dead_mask, loss_val=0.0)\n",
    "if 0 in CHECKPOINT_STEPS:\n",
    "    save_checkpoint(model, optimizer, 0, checkpoint_path)\n",
    "    print(\"Saved checkpoint at step 0\")\n",
    "\n",
    "print(f\"\\nStarting training for {TOTAL_STEPS} steps...\")\n",
    "print(f\"Checkpoints at: {CHECKPOINT_STEPS}\")\n",
    "\n",
    "# Training\n",
    "model.train()\n",
    "step = 0\n",
    "epoch = 0\n",
    "\n",
    "pbar = tqdm(total=TOTAL_STEPS, desc=\"Training\")\n",
    "\n",
    "while step < TOTAL_STEPS:\n",
    "    epoch += 1\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        if step >= TOTAL_STEPS:\n",
    "            break\n",
    "            \n",
    "        # Move to device\n",
    "        input_ids = batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.autocast(device_type=device if device != 'mps' else 'cpu', dtype=torch.bfloat16):\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Shift for next-token prediction\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(\n",
    "                shift_logits.view(-1, VOCAB_SIZE),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # Capture state (after optimizer step, so this is state at end of step t)\n",
    "        capture_state(model, optimizer, step=step, dead_mask=dead_mask, loss_val=loss.item())\n",
    "        \n",
    "        # Checkpoint if needed\n",
    "        if step in CHECKPOINT_STEPS:\n",
    "            save_checkpoint(model, optimizer, step, checkpoint_path)\n",
    "            tqdm.write(f\"Saved checkpoint at step {step}\")\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'epoch': epoch})\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "print(f\"\\nTraining complete. Final loss: {loss.item():.4f}\")\n",
    "print(f\"Completed {epoch} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trajectory data to ../../tensors/Thimble-8/thimble_8_trajectory.safetensors\n",
      "File size: 13.26 GB\n"
     ]
    }
   ],
   "source": [
    "# Save as safetensors\n",
    "data_to_save = {\n",
    "    'W': W_history,           # (3501, n_dead, 64) bfloat16\n",
    "    'm': m_history,           # (3501, n_dead, 64) float32\n",
    "    'v': v_history,           # (3501, n_dead, 64) float32\n",
    "    'g': g_history,           # (3501, n_dead, 64) float32\n",
    "    'loss': loss_history,     # (3501,) float32\n",
    "    'dead_mask': dead_mask,   # (10000,) bool -> for reference\n",
    "    'dead_indices': dead_indices,  # (n_dead,) long -> for reference\n",
    "}\n",
    "\n",
    "# Note: safetensors doesn't support bfloat16 directly in all versions\n",
    "# Convert to uint16 view to preserve exact bits\n",
    "data_to_save['W'] = W_history.view(torch.uint16)\n",
    "\n",
    "save_path = output_path / 'thimble_8_trajectory.safetensors'\n",
    "save_file(data_to_save, str(save_path))\n",
    "\n",
    "print(f\"Saved trajectory data to {save_path}\")\n",
    "print(f\"File size: {save_path.stat().st_size / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metadata.json\n",
      "\n",
      "Experiment complete!\n",
      "Data: ../../tensors/Thimble-8/thimble_8_trajectory.safetensors\n",
      "Checkpoints: ../../tensors/Thimble-8/checkpoints\n",
      "Metadata: ../../tensors/Thimble-8/metadata.json\n"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "    'experiment': 'Thimble 8',\n",
    "    'date': '2025-11-25',\n",
    "    'total_steps': TOTAL_STEPS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'seq_len': SEQ_LEN,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'beta1': BETA1,\n",
    "    'beta2': BETA2,\n",
    "    'epsilon': EPSILON,\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'random_seed': RANDOM_SEED,\n",
    "    'n_dead_tokens': n_dead,\n",
    "    'checkpoint_steps': CHECKPOINT_STEPS,\n",
    "    'final_loss': loss_history[-1].item(),\n",
    "    'total_epochs': epoch,\n",
    "    'device': device,\n",
    "    'data_shapes': {\n",
    "        'W': list(W_history.shape),\n",
    "        'm': list(m_history.shape),\n",
    "        'v': list(v_history.shape),\n",
    "        'g': list(g_history.shape),\n",
    "        'loss': list(loss_history.shape),\n",
    "    },\n",
    "    'notes': 'Dead tokens only. W stored as uint16 (bfloat16 bit pattern).'\n",
    "}\n",
    "\n",
    "with open(output_path / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Saved metadata.json\")\n",
    "print(f\"\\nExperiment complete!\")\n",
    "print(f\"Data: {save_path}\")\n",
    "print(f\"Checkpoints: {checkpoint_path}\")\n",
    "print(f\"Metadata: {output_path / 'metadata.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SANITY CHECK\n",
      "==================================================\n",
      "Mean |W[3500] - W[0]|: 0.054386\n",
      "Mean |m[3500]|: 0.000008\n",
      "Mean |v[3500]|: 0.000000\n",
      "Loss[1]: 9.1875\n",
      "Loss[3500]: 6.2812\n",
      "m[0] all zero: True\n",
      "v[0] all zero: True\n",
      "\n",
      "Sanity check complete!\n"
     ]
    }
   ],
   "source": [
    "# Verify we captured data correctly\n",
    "print(\"=\" * 50)\n",
    "print(\"SANITY CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# W should change over time\n",
    "W_bf16 = W_history.view(torch.bfloat16)  # Convert back\n",
    "W_delta = (W_bf16[-1] - W_bf16[0]).float().abs().mean()\n",
    "print(f\"Mean |W[3500] - W[0]|: {W_delta:.6f}\")\n",
    "\n",
    "# m and v should be non-zero at end\n",
    "print(f\"Mean |m[3500]|: {m_history[-1].abs().mean():.6f}\")\n",
    "print(f\"Mean |v[3500]|: {v_history[-1].abs().mean():.6f}\")\n",
    "\n",
    "# Loss should decrease\n",
    "print(f\"Loss[1]: {loss_history[1]:.4f}\")\n",
    "print(f\"Loss[3500]: {loss_history[-1]:.4f}\")\n",
    "\n",
    "# Check m[0] and v[0] are zero (before any optimizer step)\n",
    "print(f\"m[0] all zero: {(m_history[0] == 0).all().item()}\")\n",
    "print(f\"v[0] all zero: {(v_history[0] == 0).all().item()}\")\n",
    "\n",
    "print(\"\\nSanity check complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
