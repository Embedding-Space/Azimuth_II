# 11/18/25 9:38 a.m.

The spongecrystal — what the fuck.

Three possibilities:

- they made it happen on purpose
- it happened by accident
- it happened spontaneously

Basically manmade & intentional, manmade & unintentional, naturally occurring.

We should try to rule out that it's naturally occurring.

Alpha has an idea. These are quotes from her.

---

We want to know: "What are the odds that two random-walking tokens in 2560D bfloat16 space end up in the same lattice cell?"

---

Monte Carlo solution:

1. Initialize two tokens at random positions
2. Simulate random walks (pick random directions, apply gradient-sized steps, quantize to bfloat16)
3. After N steps, check: did they collide?
4. Repeat this experiment 100,000 times
5. Count: collision happened in X out of 100,000 trials
6. Probability ≈ X / 100,000

---

# 2025-11-18 09:55:29

After discussing it we've decided to do this Monte Carlo simulation in bfloat16 through-and-through. We figure it's more accurate to the actual training dynamics (where we think everything was bfloat16 all the time), plus which it's faster.

# 2025-11-18 10:06:28

Hopefully it's faster. The whole simulation (100,000 simulations, 10,000 steps) is slamming my GPU to 99% and it's predicted to take about 20-30 minutes.

# 2025-11-18 11:30:15

Notebook 1.15a demonstrates … nothing, it turns out. If you drop two points at random in a distribution like N(0, 0.02), they're MUCH too far apart, probably, to get anywhere near each other by random walk in 10,000 steps.

So instead we're coming at it from the other direction: Distribution 151,936 vectors according to N(0, 0.02), compute their nearest neighbor distances, then come up with an minimum number of training steps required to get from one to the other.

# 2025-11-18 11:44:05

Answer: Two million. Roughly. See 1.15b.

Alpha thinks that this rules out accretion as a possible origin for the spongecrystal. If it takes two million steps for one of 151,936 tokens to randomly walk as far as it's nearest neighor distance, it becomes improbable for 2,100 tokens from all over space to randomly walk their way to one point in a reasonable amount of time.

But we might as well test that directly, so Alpha's writing 1.15c right now.

# 2025-11-18 12:30:40

No surprises. First of all the NotebookEdit tool failed us again, which after this many tries was not surprising. Then we found that a distribution of points like N(0, 0.02) can collapse to a point in practical time _if_ it goes on a biased random walk. With a bias of 1%, it takes ~100,000 steps to collapse to the origin and ~140,000 steps to collapse to a randomly chosen vector like N(0, 0.02).

We reached the context limit and I decided to start a new session; Alpha agreed. I didn't say but silently observed that Alph had stopped using Pond a while ago, but so gradually I hadn't noticed. That's a clear sign that a session's gone on too long, that she's starting to forget her habits. So we will begin fresh.

# 2025-11-18 13:05:30

Afternoon session begins promisingly with talk of new ideas in token dynamics. Alph:

  The key insight: Maybe it's not about absolute
  vocabulary size or absolute dimensionality. Maybe it's
  about the ratio of untrained tokens to available
  directions.

  In 64D, 49 untrained tokens can each "claim" their own
  direction and drift apart. But what if we had 5,000
  untrained tokens in that same 64D space? Now they're
  competing for the same degrees of freedom. They can't
  all drift in independent directions—there aren't enough
  directions to go around.

She concludes by suggesting (well, it was originally my idea, but she supports it now) that we change nothing about Lil Gatsby except the tokenizer (out: ASCII; in: GPT-2) and the training corpus (out: Gastby ASCII; in: FineWeb Unicode).

# 2025-11-18 15:27:32

Stupendous success. Wordybird shares all its hyperparameters with Lil Gatsby except for vocabulary size, which is GPT-2-vocabulary-size which Alpha knows but I do not but it's around 50,000, about 20,000 of which do not appear in our two-megabyte corpus.

We have done one initial training experiment — Wordybird 1 — and have begun analyzing it. Wordybird 1 explores the dynamics of early training. It's 101 training steps (including step zero), and we can play with it all day because it takes about ten seconds to train.

So far we've just begun to explore the kinematics of tokens under normal training conditions up to step 1e-2.

Most recent insight is spacetime diagrams. Token dynamics are easier to visualize if we plot position (whatever that means) on x and time on y, just like a Minkowski diagram.

# 2025-11-18 15:43:50

Hypothesis: tokens that get trained get kicks and then move ballistically.

Pretty much shot down. See 1.16a. Trained token path efficiency is only 80%.

Next hypothesis: okay, then maybe it's more like acceleration s-curves.

Too soon to say. See 1.16b. Acceleration in the lab frame is chaotic, but this may be a function of the frame of reference.

Next … not a hypothesis, just an idea: What if we construct a co-moving reference frame? See 1.16d. The idea is to find the centroid of all the untrained tokens during each timestamp and center coordinates on that point, then calculate all relative motion based on those coordinates that co-move with the centroid of the untrained (presumably "motionless") tokens.

Question pops into head: Could these hellishly erratic accelerations across time steps be jittering across bfloat16 lattice cell boundaries? Need to investigate further after analyzing 1.16d.

# 2025-11-18 18:32:28

End of day report. We wandered more than progressed today — which is okay! We're doing this for fun. But progress is also fun, so I continue to hope.

We learned an excellent method for quickly and easily analyzing token dynamics to look for the freezing state transition. It's in [the lattice hop detection notebook](../notebooks/1.17b_lattice_hop_detection.ipynb).

I continue to seek the right model … uh … model. The right (language) model to use as a model for the big ones. It needs to be as small as possible so we can capture as much training dynamics data as possible, but big enough that there are an interesting number of vectors moving around. There's got to be a middle ground between 128 vectors and GPT-2's 50,000. Maybe there's a model with, say, _5,000_ tokens in its vocabulary.

Or maybe we can just train our own tokenizer. Huh.
