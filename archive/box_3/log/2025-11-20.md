# 2025-11-20 08:25:50

Talking with Alph about the epochs we can identify in the 1.22c plot of mean radius against time, using the Flannel 1 data set.

- The Inhale (t=0–6): Slight contraction, mysterious, unexplained
- The Sneeze (t=6–100): Explosive outward expansion
- Deceleration (t=100–300): Rapid slowing, expansion nearly stops
- Re-expansion (t=300–400): Linear second growth phase before the end
- Fimbulwinter (t=400+): Quantization freeze, tokens locked forever

Need to collect mucho data and see if these are real or just quirks of one random seed.

# 2025-11-20 08:47:42

Experiment Flannel 3: It's Flannel 1 but ten times. Alph wisely trimmed down thre recorder to capture only the tokens we're interested in. Only 5 GB of data out of what could have been 12.

Next comes analysis.

# 2025-11-20 09:57:29

Flannel 3 was flawed in a way we didn't bother to fully run down. Data that should have been identical (same random seeds) was not, so something else was different.

Now we're working on Flannel 4, which is ambitious. It will be built from the ground up to be based on a flexible experiment rig that we can hopefully reuse for a while as we study the dynamics of untrained tokens before t=1e3.

# 2025-11-20 11:03:15

Found the flaw in Flannel 3, and it wasn't in Flannel 3 in at all. It was actually in the analysis. In the original analysis we were measuring mean radius relative to the origin. Then we measured mean radius relative to the centroid. Totally different curves. The inhale-sneeze phenomenon is real, and consistent across random seeds. Flannel 4 and its analysis (1.23a) demonstrates this.

# 2025-11-20 11:33:08

Flannel 5 will test the relationship between initialization and mean radius over time. We'll be varying the standard deviation of the Gaussian initialization we're using and superimposing the results. It's possible features we think are universal are entirely chaotic and we just picked an interesting sigma.

# 2025-11-20 11:55:11

Alph blurted out a wonderful idea: Repeat Flannel experiments in float32 to see if patterns continue down at scales too small to resolve in bfloat16 (thinking about the inhale specifically which is very small for small σ).

# 2025-11-20 12:02:57

Blast. We identified a flaw in our Flannel model. Or rather the model's fine but we've been initializing it wrong. If you're going to initialize randomly, you (Alph informs me) usually make your sigma like 1/√D, which for 2560D is about 1/50 or 0.02. Except Flannel is a 64D model.

Not sure what to do. Will consult with Alpha.

# 2025-11-20 13:30:27

Turns out we were right all along. Alpha did a web search, found that generally how it's done is that the embedding matrices get initialized like N(0, 0.02), which is what we've been doing this whole time. We will continue to study this initialization.

# 2025-11-20 17:38:01

Important note: Let Flannel 6B be our model for all experiments going forward. What makes Flannel 6B special is that it's fully deterministic. Set the random seed and, to within hardware differences, you'll get the same results we got.

Also, a question: What would happen if we permute the training dataset? If we train two identical Flannel models but permute the training dataset between them, what would happen? W[n, 0] would be equal, obviously, but would W[n, 1000] be equal? Would the models converge to the same point, albeit possibly taking different paths to get there? Alph says so because training is nonlinear; Adam has momentum which will be different depending on the _order_ in which the tokens are encountered.

For tomorrow: Get Alph to explain Adam momentum and variance.