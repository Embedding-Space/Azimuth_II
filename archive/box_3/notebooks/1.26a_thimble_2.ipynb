{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thimble 2: Gravity Probe B for AdamW\n",
    "\n",
    "**Objective:** Prove the AdamW formula by achieving **bitwise equality** between predicted and observed weights.\n",
    "\n",
    "**Hypothesis:** If we train in full float32 precision (no quantization) and apply the exact AdamW formula that PyTorch uses, we should get:\n",
    "\n",
    "```python\n",
    "predicted_W[t] == observed_W[t]  # Bitwise identical for all t\n",
    "```\n",
    "\n",
    "Not approximately equal. Not ratio ‚âà 1. **Exactly equal.**\n",
    "\n",
    "**Why this matters:**\n",
    "- Thimble 1 showed accounting discrepancies even after simulating bfloat16 quantization\n",
    "- We need to validate the formula itself before we can understand quantization effects\n",
    "- If float32 gives perfect equality, we know the formula is right and can debug quantization\n",
    "- If float32 still fails, we're missing something fundamental\n",
    "\n",
    "**Method:**\n",
    "- Train in **full float32** (no `.to(bfloat16)`)\n",
    "- Record W, gradients, momentum, variance‚Äîall in float32\n",
    "- Test: `predicted_W[t] == observed_W[t]` element-wise\n",
    "\n",
    "This is our Gravity Probe B. Let's prove Einstein. üêïü•Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Parameters set\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYERS = 2\n",
    "N_HEADS = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# Training\n",
    "NUM_STEPS = 1000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.0  # Disabled for simplicity\n",
    "\n",
    "# Optimizer (AdamW)\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "# Initialization\n",
    "INIT_SCALE = 0.02  # N(0, 0.02)\n",
    "SEED = 42\n",
    "\n",
    "# Paths\n",
    "TOKENIZER_PATH = \"../data/flannel_tokenizer_chars.json\"\n",
    "CORPUS_PATH = \"../data/flannel_model_corpus.txt\"\n",
    "TOKEN_MASK_PATH = \"../tensors/Flannel/live_dead_tokens.safetensors\"\n",
    "OUTPUT_PATH = \"../tensors/Thimble/thimble_2.safetensors\"\n",
    "\n",
    "print(\"‚úì Parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from tokenizers import Tokenizer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file, load_file\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Safety Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MEMORY & DISK SAFETY CHECK\n",
      "================================================================================\n",
      "\n",
      "Recording tensors (CPU memory, all float32):\n",
      "  W:         2.56 GB\n",
      "  grad_W:    2.56 GB\n",
      "  momentum:  2.56 GB\n",
      "  variance:  2.56 GB\n",
      "  losses:    0.0000 GB\n",
      "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Total:     10.25 GB\n",
      "\n",
      "Model memory (device, all float32):\n",
      "  Model weights: 0.00 GB (738,304 params)\n",
      "  Optimizer:     0.01 GB (Adam states)\n",
      "  Activations:   0.02 GB (batch=128)\n",
      "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Total:         0.03 GB\n",
      "\n",
      "Peak RAM estimate:\n",
      "  Recording:     10.25 GB\n",
      "  Model+opt+act: 0.03 GB\n",
      "  Corpus+misc:   1.01 GB\n",
      "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  Total:         11.29 GB\n",
      "\n",
      "Disk space needed:\n",
      "  Safetensors:   10.25 GB\n",
      "\n",
      "================================================================================\n",
      "‚úì SAFE: Peak RAM (11.3 GB) within 24 GB budget\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MEMORY & DISK SAFETY CHECK\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Recording tensors - ALL FLOAT32 this time\n",
    "bytes_f32 = 4  # float32\n",
    "\n",
    "recording_w = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_f32\n",
    "recording_grad = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_f32\n",
    "recording_momentum = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_f32\n",
    "recording_variance = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_f32\n",
    "recording_losses = (NUM_STEPS+1) * bytes_f32\n",
    "\n",
    "total_recording = recording_w + recording_grad + recording_momentum + recording_variance + recording_losses\n",
    "\n",
    "print(f\"Recording tensors (CPU memory, all float32):\")\n",
    "print(f\"  W:         {recording_w/1e9:.2f} GB\")\n",
    "print(f\"  grad_W:    {recording_grad/1e9:.2f} GB\")\n",
    "print(f\"  momentum:  {recording_momentum/1e9:.2f} GB\")\n",
    "print(f\"  variance:  {recording_variance/1e9:.2f} GB\")\n",
    "print(f\"  losses:    {recording_losses/1e9:.4f} GB\")\n",
    "print(f\"  {'‚îÄ'*40}\")\n",
    "print(f\"  Total:     {total_recording/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Model memory (float32 weights + float32 optimizer states)\n",
    "embedding_params = VOCAB_SIZE * HIDDEN_DIM\n",
    "params_per_layer = 12 * HIDDEN_DIM**2\n",
    "transformer_params = N_LAYERS * params_per_layer\n",
    "total_model_params = embedding_params + transformer_params\n",
    "\n",
    "model_memory = total_model_params * bytes_f32\n",
    "optimizer_memory = 2 * total_model_params * bytes_f32  # Adam: m and v\n",
    "activation_memory = BATCH_SIZE * MAX_SEQ_LEN * HIDDEN_DIM * N_LAYERS * 2 * bytes_f32\n",
    "\n",
    "print(f\"Model memory (device, all float32):\")\n",
    "print(f\"  Model weights: {model_memory/1e9:.2f} GB ({total_model_params:,} params)\")\n",
    "print(f\"  Optimizer:     {optimizer_memory/1e9:.2f} GB (Adam states)\")\n",
    "print(f\"  Activations:   {activation_memory/1e9:.2f} GB (batch={BATCH_SIZE})\")\n",
    "print(f\"  {'‚îÄ'*40}\")\n",
    "print(f\"  Total:         {(model_memory + optimizer_memory + activation_memory)/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Peak RAM\n",
    "corpus_memory = 1371328 * 8\n",
    "misc_overhead = 1e9\n",
    "peak_ram = total_recording + model_memory + optimizer_memory + activation_memory + corpus_memory + misc_overhead\n",
    "\n",
    "print(f\"Peak RAM estimate:\")\n",
    "print(f\"  Recording:     {total_recording/1e9:.2f} GB\")\n",
    "print(f\"  Model+opt+act: {(model_memory + optimizer_memory + activation_memory)/1e9:.2f} GB\")\n",
    "print(f\"  Corpus+misc:   {(corpus_memory + misc_overhead)/1e9:.2f} GB\")\n",
    "print(f\"  {'‚îÄ'*40}\")\n",
    "print(f\"  Total:         {peak_ram/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Disk space\n",
    "disk_needed = total_recording + 1e6\n",
    "print(f\"Disk space needed:\")\n",
    "print(f\"  Safetensors:   {disk_needed/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Safety verdict\n",
    "print(f\"{'='*80}\")\n",
    "if peak_ram <= 24e9:\n",
    "    print(f\"‚úì SAFE: Peak RAM ({peak_ram/1e9:.1f} GB) within 24 GB budget\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Peak RAM ({peak_ram/1e9:.1f} GB) exceeds 24 GB budget!\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"‚úì Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: ../data/flannel_tokenizer_chars.json\n",
      "  ‚úì Vocabulary: 10,000 tokens\n",
      "\n",
      "Loading corpus: ../data/flannel_model_corpus.txt\n",
      "  ‚úì Tokens: 1,371,328\n",
      "\n",
      "Loading token masks: ../tensors/Flannel/live_dead_tokens.safetensors\n",
      "  ‚úì Live: 6,301 | Dead: 3,699\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "print(f\"Loading tokenizer: {TOKENIZER_PATH}\")\n",
    "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "print(f\"  ‚úì Vocabulary: {tokenizer.get_vocab_size():,} tokens\\n\")\n",
    "\n",
    "# Corpus\n",
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "encoding = tokenizer.encode(corpus_text)\n",
    "tokens = encoding.ids\n",
    "corpus_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "print(f\"  ‚úì Tokens: {len(tokens):,}\\n\")\n",
    "\n",
    "# Token masks (for analysis later)\n",
    "print(f\"Loading token masks: {TOKEN_MASK_PATH}\")\n",
    "mask_data = load_file(TOKEN_MASK_PATH)\n",
    "live_mask = mask_data['live_mask'].bool()\n",
    "dead_mask = mask_data['dead_mask'].bool()\n",
    "n_live = live_mask.sum().item()\n",
    "n_dead = dead_mask.sum().item()\n",
    "print(f\"  ‚úì Live: {n_live:,} | Dead: {n_dead:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Dataset: 1,371,200 examples\n",
      "‚úì DataLoader: 10,713 batches per epoch\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = TokenDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "\n",
    "# DataLoader with deterministic sampling\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    generator=g,\n",
    "    worker_init_fn=seed_worker,\n",
    "    num_workers=0,  # Single-threaded for reproducibility\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Dataset: {len(dataset):,} examples\")\n",
    "print(f\"‚úì DataLoader: {len(dataloader):,} batches per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model (FLOAT32 - No Quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "\n",
      "  Architecture: 2 layers, 2 heads, 64d embeddings\n",
      "  Parameters: 748,288\n",
      "  Device: mps\n",
      "  Dtype: torch.float32 (FLOAT32 - no quantization)\n",
      "\n",
      "‚úì Model created\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating model...\\n\")\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYERS,\n",
    "    n_head=N_HEADS,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Initialize embedding weights with N(0, 0.02)\n",
    "with torch.no_grad():\n",
    "    nn.init.normal_(model.transformer.wte.weight, mean=0.0, std=INIT_SCALE)\n",
    "\n",
    "# Move to device - KEEP AS FLOAT32 (no .to(bfloat16))\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"  Architecture: {N_LAYERS} layers, {N_HEADS} heads, {HIDDEN_DIM}d embeddings\")\n",
    "print(f\"  Parameters: {n_params:,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Dtype: {model.transformer.wte.weight.dtype} (FLOAT32 - no quantization)\")\n",
    "print(f\"\\n‚úì Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Optimizer: AdamW\n",
      "  Learning rate: 0.001\n",
      "  Betas: (0.9, 0.999)\n",
      "  Epsilon: 1e-08\n",
      "  Weight decay: 0.0\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(ADAM_BETA1, ADAM_BETA2),\n",
    "    eps=ADAM_EPSILON,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Optimizer: AdamW\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Betas: ({ADAM_BETA1}, {ADAM_BETA2})\")\n",
    "print(f\"  Epsilon: {ADAM_EPSILON}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-allocate Recording Tensors (ALL FLOAT32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-allocating recording tensors...\n",
      "\n",
      "  W:         (1001, 10000, 64) (float32) = 2.56 GB\n",
      "  grad_W:    (1001, 10000, 64) (float32) = 2.56 GB\n",
      "  momentum:  (1001, 10000, 64) (float32) = 2.56 GB\n",
      "  variance:  (1001, 10000, 64) (float32) = 2.56 GB\n",
      "  losses:    (1001,) (float32) = 0.0000 GB\n",
      "\n",
      "  Total: 10.25 GB\n",
      "\n",
      "‚úì Tensors allocated (all float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPre-allocating recording tensors...\\n\")\n",
    "\n",
    "# ALL FLOAT32 - no dtype conversions\n",
    "W_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32)\n",
    "grad_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32)\n",
    "momentum_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32)\n",
    "variance_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32)\n",
    "loss_history = torch.zeros(NUM_STEPS+1, dtype=torch.float32)\n",
    "\n",
    "# Memory calculation\n",
    "bytes_per_elem = 4  # float32\n",
    "memory_w = W_history.numel() * bytes_per_elem\n",
    "memory_grad = grad_history.numel() * bytes_per_elem\n",
    "memory_momentum = momentum_history.numel() * bytes_per_elem\n",
    "memory_variance = variance_history.numel() * bytes_per_elem\n",
    "memory_loss = loss_history.numel() * bytes_per_elem\n",
    "total_memory = memory_w + memory_grad + memory_momentum + memory_variance + memory_loss\n",
    "\n",
    "print(f\"  W:         {tuple(W_history.shape)} (float32) = {memory_w/1e9:.2f} GB\")\n",
    "print(f\"  grad_W:    {tuple(grad_history.shape)} (float32) = {memory_grad/1e9:.2f} GB\")\n",
    "print(f\"  momentum:  {tuple(momentum_history.shape)} (float32) = {memory_momentum/1e9:.2f} GB\")\n",
    "print(f\"  variance:  {tuple(variance_history.shape)} (float32) = {memory_variance/1e9:.2f} GB\")\n",
    "print(f\"  losses:    {tuple(loss_history.shape)} (float32) = {memory_loss/1e9:.4f} GB\")\n",
    "print(f\"\\n  Total: {total_memory/1e9:.2f} GB\")\n",
    "print(f\"\\n‚úì Tensors allocated (all float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "THIMBLE 2: TRAINING (FLOAT32)\n",
      "================================================================================\n",
      "\n",
      "‚úì Recorded initial state (t=0)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5301ddadaffa44af94370b1359b478f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úì Training complete\n",
      "  Time: 71.9s (1.2 minutes)\n",
      "  Final loss: 6.4703\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"THIMBLE 2: TRAINING (FLOAT32)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Record initial state (step 0)\n",
    "W_history[0] = model.transformer.wte.weight.data.clone().cpu()\n",
    "loss_history[0] = float('nan')  # No loss before first step\n",
    "print(\"‚úì Recorded initial state (t=0)\\n\")\n",
    "\n",
    "# Create infinite iterator over dataloader\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "\n",
    "for step in tqdm(range(1, NUM_STEPS+1), desc=\"Training\"):\n",
    "    # Get next batch (cycle through dataset if needed)\n",
    "    try:\n",
    "        batch = next(data_iter)\n",
    "    except StopIteration:\n",
    "        data_iter = iter(dataloader)\n",
    "        batch = next(data_iter)\n",
    "    \n",
    "    # Move batch to device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # === RECORD GRADIENTS (before optimizer.step) ===\n",
    "    grad_history[step] = model.transformer.wte.weight.grad.clone().cpu()\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # === RECORD WEIGHTS & OPTIMIZER STATE (after optimizer.step) ===\n",
    "    W_history[step] = model.transformer.wte.weight.data.clone().cpu()\n",
    "    \n",
    "    # Get optimizer state for embedding weights\n",
    "    wte_param = model.transformer.wte.weight\n",
    "    if wte_param in optimizer.state:\n",
    "        opt_state = optimizer.state[wte_param]\n",
    "        momentum_history[step] = opt_state['exp_avg'].clone().cpu()\n",
    "        variance_history[step] = opt_state['exp_avg_sq'].clone().cpu()\n",
    "    \n",
    "    loss_history[step] = loss.item()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì Training complete\")\n",
    "print(f\"  Time: {elapsed:.1f}s ({elapsed/60:.1f} minutes)\")\n",
    "print(f\"  Final loss: {loss_history[-1]:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation: Test for BITWISE EQUALITY\n",
    "\n",
    "This is Gravity Probe B. We're proving Einstein.\n",
    "\n",
    "If the AdamW formula is correct, and we're doing exactly what PyTorch does, then:\n",
    "\n",
    "```python\n",
    "predicted_W[t] == observed_W[t]  # Should be True for every element\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating AdamW accounting (testing for bitwise equality)...\n",
      "\n",
      "t=  1: exact_match=False, max_diff=7.45e-09, ratio=1.000000\n",
      "t= 10: exact_match=False, max_diff=7.45e-09, ratio=1.000000\n",
      "t= 50: exact_match=False, max_diff=7.45e-09, ratio=1.000000\n",
      "t=100: exact_match=False, max_diff=7.45e-09, ratio=1.000000\n",
      "t=200: exact_match=False, max_diff=7.45e-09, ratio=1.000000\n",
      "t=500: exact_match=False, max_diff=1.49e-08, ratio=1.000000\n",
      "t=800: exact_match=False, max_diff=2.98e-08, ratio=1.000000\n",
      "\n",
      "If exact_match=True for all timesteps, we have proven the formula.\n",
      "If not, we're still missing something fundamental.\n",
      "\n",
      "‚úì Validation complete\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nValidating AdamW accounting (testing for bitwise equality)...\\n\")\n",
    "\n",
    "# Test at several timesteps\n",
    "test_steps = [1, 10, 50, 100, 200, 500, 800]\n",
    "\n",
    "for t in test_steps:\n",
    "    if t > NUM_STEPS:\n",
    "        continue\n",
    "    \n",
    "    # Get optimizer states\n",
    "    m_t = momentum_history[t]\n",
    "    v_t = variance_history[t]\n",
    "    W_prev = W_history[t-1]\n",
    "    \n",
    "    # Apply AdamW formula\n",
    "    bias_correction1 = 1 - ADAM_BETA1**t\n",
    "    bias_correction2 = 1 - ADAM_BETA2**t\n",
    "    m_hat = m_t / bias_correction1\n",
    "    v_hat = v_t / bias_correction2\n",
    "    \n",
    "    # Compute predicted W[t]\n",
    "    dW = -LEARNING_RATE * m_hat / (torch.sqrt(v_hat) + ADAM_EPSILON)\n",
    "    predicted_W = W_prev + dW\n",
    "    \n",
    "    # Observed W[t]\n",
    "    observed_W = W_history[t]\n",
    "    \n",
    "    # Test for exact equality\n",
    "    exactly_equal = torch.equal(predicted_W, observed_W)\n",
    "    max_abs_diff = torch.max(torch.abs(predicted_W - observed_W)).item()\n",
    "    \n",
    "    # Also compute approximate metrics for context\n",
    "    norm_pred = torch.norm(predicted_W - W_prev)\n",
    "    norm_obs = torch.norm(observed_W - W_prev)\n",
    "    ratio = norm_pred / norm_obs\n",
    "    \n",
    "    print(f\"t={t:3d}: exact_match={exactly_equal}, max_diff={max_abs_diff:.2e}, ratio={ratio:.6f}\")\n",
    "\n",
    "print(\"\\nIf exact_match=True for all timesteps, we have proven the formula.\")\n",
    "print(\"If not, we're still missing something fundamental.\")\n",
    "print(\"\\n‚úì Validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data to ../tensors/Thimble/thimble_2.safetensors...\n",
      "\n",
      "‚úì Saved successfully\n",
      "  File: thimble_2.safetensors\n",
      "  Size: 10.25 GB\n",
      "  Save time: 8.2s\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving data to {OUTPUT_PATH}...\\n\")\n",
    "\n",
    "# Create output directory if needed\n",
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build save dictionary\n",
    "save_dict = {\n",
    "    # Training trajectories (all float32)\n",
    "    'W': W_history,\n",
    "    'grad_W': grad_history,\n",
    "    'momentum_W': momentum_history,\n",
    "    'variance_W': variance_history,\n",
    "    'losses': loss_history,\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    'vocab_size': torch.tensor(VOCAB_SIZE, dtype=torch.long),\n",
    "    'hidden_dim': torch.tensor(HIDDEN_DIM, dtype=torch.long),\n",
    "    'n_layers': torch.tensor(N_LAYERS, dtype=torch.long),\n",
    "    'n_heads': torch.tensor(N_HEADS, dtype=torch.long),\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'num_steps': torch.tensor(NUM_STEPS, dtype=torch.long),\n",
    "    'batch_size': torch.tensor(BATCH_SIZE, dtype=torch.long),\n",
    "    'learning_rate': torch.tensor(LEARNING_RATE, dtype=torch.float32),\n",
    "    'weight_decay': torch.tensor(WEIGHT_DECAY, dtype=torch.float32),\n",
    "    'adam_beta1': torch.tensor(ADAM_BETA1, dtype=torch.float32),\n",
    "    'adam_beta2': torch.tensor(ADAM_BETA2, dtype=torch.float32),\n",
    "    'adam_epsilon': torch.tensor(ADAM_EPSILON, dtype=torch.float32),\n",
    "    'init_scale': torch.tensor(INIT_SCALE, dtype=torch.float32),\n",
    "    'seed': torch.tensor(SEED, dtype=torch.long),\n",
    "    \n",
    "    # Token counts\n",
    "    'n_live': torch.tensor(n_live, dtype=torch.long),\n",
    "    'n_dead': torch.tensor(n_dead, dtype=torch.long),\n",
    "}\n",
    "\n",
    "# Save\n",
    "save_start = time.time()\n",
    "save_file(save_dict, str(OUTPUT_PATH))\n",
    "save_elapsed = time.time() - save_start\n",
    "\n",
    "# File size\n",
    "file_size_bytes = Path(OUTPUT_PATH).stat().st_size\n",
    "file_size_gb = file_size_bytes / 1e9\n",
    "\n",
    "print(f\"‚úì Saved successfully\")\n",
    "print(f\"  File: {Path(OUTPUT_PATH).name}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB\")\n",
    "print(f\"  Save time: {save_elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "THIMBLE 2 COMPLETE: GRAVITY PROBE B\n",
      "================================================================================\n",
      "\n",
      "Trained language model for 1,000 steps in FULL FLOAT32\n",
      "  Seed: 42\n",
      "  Batch size: 128\n",
      "  Learning rate: 0.001\n",
      "  Weight decay: 0.0\n",
      "\n",
      "Recorded at every step (all float32):\n",
      "  ‚Ä¢ W: embedding weights\n",
      "  ‚Ä¢ grad_W: gradients\n",
      "  ‚Ä¢ momentum_W: Adam exp_avg\n",
      "  ‚Ä¢ variance_W: Adam exp_avg_sq\n",
      "  ‚Ä¢ losses: training loss\n",
      "\n",
      "Data saved: ../tensors/Thimble/thimble_2.safetensors\n",
      "  Size: 10.25 GB\n",
      "  Training time: 1.2 minutes\n",
      "\n",
      "Next: Analyze in separate notebook to test for BITWISE EQUALITY.\n",
      "If predicted_W[t] == observed_W[t], we have proven the formula.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"THIMBLE 2 COMPLETE: GRAVITY PROBE B\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Trained language model for {NUM_STEPS:,} steps in FULL FLOAT32\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print()\n",
    "print(f\"Recorded at every step (all float32):\")\n",
    "print(f\"  ‚Ä¢ W: embedding weights\")\n",
    "print(f\"  ‚Ä¢ grad_W: gradients\")\n",
    "print(f\"  ‚Ä¢ momentum_W: Adam exp_avg\")\n",
    "print(f\"  ‚Ä¢ variance_W: Adam exp_avg_sq\")\n",
    "print(f\"  ‚Ä¢ losses: training loss\")\n",
    "print()\n",
    "print(f\"Data saved: {OUTPUT_PATH}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB\")\n",
    "print(f\"  Training time: {elapsed/60:.1f} minutes\")\n",
    "print()\n",
    "print(f\"Next: Analyze in separate notebook to test for BITWISE EQUALITY.\")\n",
    "print(f\"If predicted_W[t] == observed_W[t], we have proven the formula.\")\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
