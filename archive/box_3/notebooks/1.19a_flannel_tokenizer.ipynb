{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.19a: Flannel Tokenizer Training\n",
    "\n",
    "**Goal:** Train a custom 1000-token BPE tokenizer on our mixed English-Thai corpus.\n",
    "\n",
    "## How BPE Tokenizer Training Works\n",
    "\n",
    "BPE (Byte Pair Encoding) is a greedy algorithm that learns a vocabulary from a corpus by iteratively merging the most frequent pairs of tokens.\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "1. **Start with a base vocabulary:** For byte-level BPE, this is the 256 individual bytes (0x00 through 0xFF).\n",
    "\n",
    "2. **Count all pairs in the corpus:** Scan through the entire training corpus and count how many times each pair of adjacent tokens appears.\n",
    "   - Example: If \"th\" appears together 1 million times, that pair gets a count of 1,000,000.\n",
    "\n",
    "3. **Merge the most frequent pair:** Find the pair with the highest count and create a new token for it.\n",
    "   - Example: Create token #257 = \"th\"\n",
    "   - Add it to the vocabulary\n",
    "   - Replace all instances of that pair in the corpus with the new token\n",
    "\n",
    "4. **Repeat:** Re-count pairs with the updated corpus (now \"th\" is a single token), merge the next most frequent pair, and continue.\n",
    "\n",
    "5. **Stop when you hit your target vocabulary size** (in our case, 1000 tokens).\n",
    "\n",
    "### Example\n",
    "\n",
    "```\n",
    "Corpus:     \"the cat\"\n",
    "Base vocab: ['t', 'h', 'e', ' ', 'c', 'a']\n",
    "\n",
    "Iteration 1:\n",
    "  Most frequent pair: ('t', 'h') appears 1 time\n",
    "  Create new token: 'th'\n",
    "  Corpus becomes: \"the cat\" → [th][e][ ][c][a][t]\n",
    "  Vocab: ['t', 'h', 'e', ' ', 'c', 'a', 'th']\n",
    "\n",
    "Iteration 2:\n",
    "  Most frequent pair: ('th', 'e') appears 1 time\n",
    "  Create new token: 'the'\n",
    "  Corpus becomes: [the][ ][c][a][t]\n",
    "  Vocab: ['t', 'h', 'e', ' ', 'c', 'a', 'th', 'the']\n",
    "\n",
    "...and so on\n",
    "```\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "- **Order doesn't matter:** The algorithm only cares about pair frequencies, not the order documents appear in the corpus.\n",
    "- **Greedy:** It always picks the most frequent pair at each step. This is optimal for compression but not necessarily for linguistic structure.\n",
    "- **Fast:** No neural networks, no gradients—just counting and sorting. Takes seconds to minutes on laptop CPUs.\n",
    "\n",
    "## This Notebook\n",
    "\n",
    "We'll use HuggingFace's `tokenizers` library (written in Rust for speed) to train a 1000-token BPE tokenizer on our mixed corpus:\n",
    "- 80% English (from FineWeb)\n",
    "- 20% Thai (from FineWeb-2)\n",
    "\n",
    "Expected result: The tokenizer will learn ~800 English subword tokens and ~200 Thai subword tokens (roughly proportional to their frequency in the training corpus).\n",
    "\n",
    "## Output\n",
    "\n",
    "- `../data/flannel_tokenizer.json` - Trained tokenizer in HuggingFace format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input corpus (from 1.18a)\n",
    "CORPUS_PATH = \"../data/flannel_tokenizer_corpus.txt\"\n",
    "\n",
    "# Output tokenizer\n",
    "TOKENIZER_OUTPUT = \"../data/flannel_tokenizer.json\"\n",
    "\n",
    "# Tokenizer parameters\n",
    "VOCAB_SIZE = 1000\n",
    "MIN_FREQUENCY = 2  # Ignore pairs that appear less than this\n",
    "\n",
    "# Special tokens\n",
    "SPECIAL_TOKENS = [\"<|endoftext|>\"]\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Corpus Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found corpus at ../data/flannel_tokenizer_corpus.txt\n",
      "  Size: 104,920,936 bytes (100.06 MB)\n"
     ]
    }
   ],
   "source": [
    "corpus_path = Path(CORPUS_PATH)\n",
    "\n",
    "if not corpus_path.exists():\n",
    "    raise FileNotFoundError(f\"Corpus not found at {CORPUS_PATH}. Run 1.18a first.\")\n",
    "\n",
    "# Check corpus size\n",
    "corpus_bytes = corpus_path.stat().st_size\n",
    "corpus_mb = corpus_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"✓ Found corpus at {CORPUS_PATH}\")\n",
    "print(f\"  Size: {corpus_bytes:,} bytes ({corpus_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tokenizer\n",
    "\n",
    "We'll use byte-level BPE, which starts with 256 base tokens (one per byte) and merges up to our target vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tokenizer...\n",
      "\n",
      "✓ Created byte-level BPE tokenizer\n",
      "  Base vocabulary: 256 bytes\n",
      "  Target vocabulary: 1,000 tokens\n",
      "  Merges to perform: 744\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating tokenizer...\\n\")\n",
    "\n",
    "# Create a byte-level BPE tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Use byte-level pre-tokenizer (splits on bytes, not characters)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "# Use byte-level decoder (converts bytes back to UTF-8)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "print(f\"✓ Created byte-level BPE tokenizer\")\n",
    "print(f\"  Base vocabulary: 256 bytes\")\n",
    "print(f\"  Target vocabulary: {VOCAB_SIZE:,} tokens\")\n",
    "print(f\"  Merges to perform: {VOCAB_SIZE - 256:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Tokenizer\n",
    "\n",
    "This is where the BPE algorithm runs. It will:\n",
    "1. Scan the entire corpus\n",
    "2. Count all byte pairs\n",
    "3. Iteratively merge the most frequent pairs until we reach 1000 tokens\n",
    "\n",
    "Expected time: 1-3 minutes on your M4 Pro for ~100 MB of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on ../data/flannel_tokenizer_corpus.txt...\n",
      "\n",
      "This will take 1-3 minutes. The tokenizer is:\n",
      "  1. Scanning 100 MB of text\n",
      "  2. Counting all byte pairs\n",
      "  3. Performing 744 merge operations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "✓ Training complete in 5.6 seconds (0.09 minutes)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training tokenizer on {CORPUS_PATH}...\\n\")\n",
    "print(f\"This will take 1-3 minutes. The tokenizer is:\")\n",
    "print(f\"  1. Scanning {corpus_mb:.0f} MB of text\")\n",
    "print(f\"  2. Counting all byte pairs\")\n",
    "print(f\"  3. Performing {VOCAB_SIZE - 256:,} merge operations\\n\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=MIN_FREQUENCY,\n",
    "    special_tokens=SPECIAL_TOKENS,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Train (this is the slow part)\n",
    "start_time = time.time()\n",
    "tokenizer.train(files=[str(corpus_path)], trainer=trainer)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Training complete in {elapsed:.1f} seconds ({elapsed/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Tokenizer\n",
    "\n",
    "Let's see what the tokenizer learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer statistics:\n",
      "\n",
      "  Vocabulary size: 1,000 tokens\n",
      "  Special tokens: 1\n",
      "  Learned merges: 744\n",
      "\n",
      "Example tokens (first 20 after base bytes):\n",
      "   256: Ġl\n",
      "   257: à¸ĩ\n",
      "   258: ve\n",
      "   259: st\n",
      "   260: Ġe\n",
      "   261: Ġn\n",
      "   262: à¸±\n",
      "   263: ro\n",
      "   264: Ġre\n",
      "   265: à¸¡\n",
      "   266: Ġy\n",
      "   267: Ġg\n",
      "   268: ĠI\n",
      "   269: ly\n",
      "   270: à¹Ģà¸\n",
      "   271: ct\n",
      "   272: Ġbe\n",
      "   273: à¸µ\n",
      "   274: ĠT\n",
      "   275: ut\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTokenizer statistics:\\n\")\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"  Vocabulary size: {vocab_size:,} tokens\")\n",
    "print(f\"  Special tokens: {len(SPECIAL_TOKENS)}\")\n",
    "print(f\"  Learned merges: {vocab_size - 256:,}\")\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Show some example tokens\n",
    "print(f\"\\nExample tokens (first 20 after base bytes):\")\n",
    "sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
    "for token, idx in sorted_vocab[256:276]:  # Skip base bytes, show first 20 merges\n",
    "    # Decode the token for display\n",
    "    try:\n",
    "        display = repr(token)[1:-1]  # Remove outer quotes from repr\n",
    "        print(f\"  {idx:4d}: {display}\")\n",
    "    except:\n",
    "        print(f\"  {idx:4d}: [unprintable]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tokenizer\n",
    "\n",
    "Let's encode some test strings to verify the tokenizer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing tokenizer...\n",
      "\n",
      "Input:  'Hello, world!'\n",
      "Tokens: ['H', 'ell', 'o', ',', 'Ġwor', 'ld', '!']\n",
      "IDs:    [40, 977, 79, 12, 450, 342, 1]\n",
      "Count:  7 tokens\n",
      "\n",
      "Input:  'The quick brown fox jumps over the lazy dog.'\n",
      "Tokens: ['The', 'Ġqu', 'ick', 'Ġb', 'ro', 'wn', 'Ġf', 'o', 'x', 'Ġj', 'um', 'p', 's', 'Ġover', 'Ġthe', 'Ġl', 'a', 'z', 'y', 'Ġdo', 'g', '.']\n",
      "IDs:    [465, 605, 585, 224, 263, 681, 223, 79, 88, 459, 414, 80, 83, 648, 209, 256, 65, 90, 89, 441, 71, 14]\n",
      "Count:  22 tokens\n",
      "\n",
      "Input:  'สวัสดีครับ'\n",
      "Tokens: ['à¸ª', 'à¸§', 'à¸±', 'à¸ª', 'à¸Ķ', 'à¸µ', 'à¸Ħà¸£', 'à¸±', 'à¸ļ']\n",
      "IDs:    [317, 282, 262, 317, 291, 273, 751, 262, 311]\n",
      "Count:  9 tokens\n",
      "\n",
      "Input:  'ภาษาไทย'\n",
      "Tokens: ['à¸ł', 'à¸²à¸', '©', 'à¸²', 'à¹Ħ', 'à¸Ĺ', 'à¸¢']\n",
      "IDs:    [737, 233, 103, 227, 333, 303, 290]\n",
      "Count:  7 tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTesting tokenizer...\\n\")\n",
    "\n",
    "test_strings = [\n",
    "    \"Hello, world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"สวัสดีครับ\",  # Thai: \"Hello\"\n",
    "    \"ภาษาไทย\"      # Thai: \"Thai language\"\n",
    "]\n",
    "\n",
    "for test_str in test_strings:\n",
    "    encoding = tokenizer.encode(test_str)\n",
    "    tokens = encoding.tokens\n",
    "    ids = encoding.ids\n",
    "    \n",
    "    print(f\"Input:  {repr(test_str)}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"IDs:    {ids}\")\n",
    "    print(f\"Count:  {len(ids)} tokens\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenizer to ../data/flannel_tokenizer.json...\n",
      "\n",
      "✓ Saved tokenizer\n",
      "  Path: ../data/flannel_tokenizer.json\n",
      "  Size: 54.2 KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving tokenizer to {TOKENIZER_OUTPUT}...\\n\")\n",
    "\n",
    "# Ensure directory exists\n",
    "Path(TOKENIZER_OUTPUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save in HuggingFace format (JSON)\n",
    "tokenizer.save(str(TOKENIZER_OUTPUT))\n",
    "\n",
    "# Verify file was created\n",
    "output_path = Path(TOKENIZER_OUTPUT)\n",
    "if output_path.exists():\n",
    "    output_kb = output_path.stat().st_size / 1024\n",
    "    print(f\"✓ Saved tokenizer\")\n",
    "    print(f\"  Path: {TOKENIZER_OUTPUT}\")\n",
    "    print(f\"  Size: {output_kb:.1f} KB\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Failed to save tokenizer to {TOKENIZER_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TOKENIZER TRAINING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Training corpus:\n",
      "  Path: ../data/flannel_tokenizer_corpus.txt\n",
      "  Size: 100.06 MB\n",
      "  Composition: ~80% English, ~20% Thai\n",
      "\n",
      "Tokenizer:\n",
      "  Type: Byte-level BPE\n",
      "  Vocabulary size: 1,000 tokens\n",
      "  Base tokens: 256 bytes\n",
      "  Learned merges: 744\n",
      "  Training time: 5.6 seconds\n",
      "\n",
      "Output:\n",
      "  Path: ../data/flannel_tokenizer.json\n",
      "\n",
      "Next steps:\n",
      "  → Use this tokenizer to train Flannel models (notebook 1.20a+)\n",
      "  → Expect ~200 Thai tokens to be dead (never appear in English model training)\n",
      "  → Watch what happens to those dead tokens during training\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TOKENIZER TRAINING COMPLETE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Training corpus:\")\n",
    "print(f\"  Path: {CORPUS_PATH}\")\n",
    "print(f\"  Size: {corpus_mb:.2f} MB\")\n",
    "print(f\"  Composition: ~80% English, ~20% Thai\")\n",
    "print()\n",
    "\n",
    "print(f\"Tokenizer:\")\n",
    "print(f\"  Type: Byte-level BPE\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,} tokens\")\n",
    "print(f\"  Base tokens: 256 bytes\")\n",
    "print(f\"  Learned merges: {vocab_size - 256:,}\")\n",
    "print(f\"  Training time: {elapsed:.1f} seconds\")\n",
    "print()\n",
    "\n",
    "print(f\"Output:\")\n",
    "print(f\"  Path: {TOKENIZER_OUTPUT}\")\n",
    "print()\n",
    "\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  → Use this tokenizer to train Flannel models (notebook 1.20a+)\")\n",
    "print(f\"  → Expect ~200 Thai tokens to be dead (never appear in English model training)\")\n",
    "print(f\"  → Watch what happens to those dead tokens during training\")\n",
    "print()\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
