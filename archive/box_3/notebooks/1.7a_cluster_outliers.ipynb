{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7a: Cluster Outlier Detection\n",
    "\n",
    "**Hypothesis:** The cluster contains a distant outlier - a token on the same ray as the core cluster but with much larger norm.\n",
    "\n",
    "**Evidence:** Median distance from centroid is 0.00089653, but max is 1.16091752.\n",
    "\n",
    "**Prediction:** The largest pairwise L∞ distance will identify two tokens, one of which has ||t|| >> ||centroid||.\n",
    "\n",
    "**Test:** Compute all pairwise L∞ distances, find argmax, check norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to analyze\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Detect available device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W from ../tensors/Qwen3-4B-Instruct-2507/W.safetensors\n",
      "  Shape: torch.Size([151936, 2560])\n",
      "\n",
      "Loaded cluster from ../tensors/Qwen3-4B-Instruct-2507/1.6a_cluster_mask.safetensors\n",
      "  Cluster size: 2,248 tokens\n",
      "  Centroid norm: 0.37091014\n"
     ]
    }
   ],
   "source": [
    "# Load W\n",
    "W_path = Path(f\"../tensors/{MODEL_NAME}/W.safetensors\")\n",
    "W = load_file(W_path)[\"W\"].to(torch.float32)\n",
    "\n",
    "print(f\"Loaded W from {W_path}\")\n",
    "print(f\"  Shape: {W.shape}\")\n",
    "\n",
    "# Load cluster data from 1.6a\n",
    "cluster_path = Path(f\"../tensors/{MODEL_NAME}/1.6a_cluster_mask.safetensors\")\n",
    "cluster_data = load_file(cluster_path)\n",
    "\n",
    "cluster_mask = cluster_data[\"cluster_mask\"].to(torch.bool)\n",
    "cluster_token_ids = cluster_data[\"cluster_token_ids\"].to(torch.int64)\n",
    "centroid = cluster_data[\"centroid\"].to(torch.float32)\n",
    "n_cluster = cluster_data[\"n_cluster\"].item()\n",
    "\n",
    "print(f\"\\nLoaded cluster from {cluster_path}\")\n",
    "print(f\"  Cluster size: {n_cluster:,} tokens\")\n",
    "print(f\"  Centroid norm: {centroid.norm().item():.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Cluster Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2,248 cluster embeddings\n",
      "  Dimensionality: 2,560\n"
     ]
    }
   ],
   "source": [
    "# Get cluster embeddings\n",
    "W_cluster = W[cluster_mask]\n",
    "\n",
    "print(f\"Extracted {W_cluster.shape[0]:,} cluster embeddings\")\n",
    "print(f\"  Dimensionality: {W_cluster.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Pairwise L∞ Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing pairwise L∞ distances (batched to avoid OOM)...\n",
      "\n",
      "  Processed rows    0- 256, batch max: 0.13134766\n",
      "  Processed rows  256- 512, batch max: 0.08604431\n",
      "  Processed rows  512- 768, batch max: 0.08551025\n",
      "  Processed rows  768-1024, batch max: 0.08551025\n",
      "  Processed rows 1024-1280, batch max: 0.08551407\n",
      "  Processed rows 1280-1536, batch max: 0.10180664\n",
      "  Processed rows 1536-1792, batch max: 0.08576202\n",
      "  Processed rows 1792-2048, batch max: 0.08660889\n",
      "  Processed rows 2048-2248, batch max: 0.08551025\n",
      "\n",
      "✓ Computed pairwise L∞ distances in batches of 256\n",
      "  Global maximum: 0.13134766 at (14, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing pairwise L∞ distances (batched to avoid OOM)...\\n\")\n",
    "\n",
    "# Move to device for computation\n",
    "W_cluster_device = W_cluster.to(device)\n",
    "\n",
    "# Batch size for computing distances (to avoid allocating 48GB)\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Track global maximum\n",
    "global_max_linf = -1.0\n",
    "global_i = -1\n",
    "global_j = -1\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Process in batches of rows\n",
    "    for batch_start in range(0, n_cluster, BATCH_SIZE):\n",
    "        batch_end = min(batch_start + BATCH_SIZE, n_cluster)\n",
    "        batch_size = batch_end - batch_start\n",
    "        \n",
    "        # Get batch of tokens: shape (batch_size, d)\n",
    "        batch_tokens = W_cluster_device[batch_start:batch_end]\n",
    "        \n",
    "        # Compute differences with ALL tokens: (batch_size, 1, d) - (1, n_cluster, d)\n",
    "        # This creates (batch_size, n_cluster, d) instead of (n_cluster, n_cluster, d)\n",
    "        diffs = batch_tokens.unsqueeze(1) - W_cluster_device.unsqueeze(0)\n",
    "        \n",
    "        # L∞ for this batch: (batch_size, n_cluster)\n",
    "        linf_batch = diffs.abs().max(dim=2).values\n",
    "        \n",
    "        # Find max in this batch\n",
    "        batch_max = linf_batch.max().item()\n",
    "        \n",
    "        if batch_max > global_max_linf:\n",
    "            # Update global maximum\n",
    "            global_max_linf = batch_max\n",
    "            \n",
    "            # Find indices within batch\n",
    "            flat_idx = linf_batch.argmax().item()\n",
    "            local_i = flat_idx // n_cluster\n",
    "            local_j = flat_idx % n_cluster\n",
    "            \n",
    "            # Convert to global indices\n",
    "            global_i = batch_start + local_i\n",
    "            global_j = local_j\n",
    "        \n",
    "        print(f\"  Processed rows {batch_start:4d}-{batch_end:4d}, batch max: {batch_max:.8f}\")\n",
    "\n",
    "print(f\"\\n✓ Computed pairwise L∞ distances in batches of {BATCH_SIZE}\")\n",
    "print(f\"  Global maximum: {global_max_linf:.8f} at ({global_i}, {global_j})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Maximum L∞ Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting result...\n",
      "\n",
      "Maximum L∞ distance: 0.13134766\n",
      "  Between tokens: 48494 and 71473\n",
      "  (Cluster indices: 14 and 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExtracting result...\\n\")\n",
    "\n",
    "# The global_i and global_j are already computed from the batched loop\n",
    "max_linf = global_max_linf\n",
    "i = global_i\n",
    "j = global_j\n",
    "\n",
    "# Get actual token IDs\n",
    "token_i = cluster_token_ids[i].item()\n",
    "token_j = cluster_token_ids[j].item()\n",
    "\n",
    "print(f\"Maximum L∞ distance: {max_linf:.8f}\")\n",
    "print(f\"  Between tokens: {token_i} and {token_j}\")\n",
    "print(f\"  (Cluster indices: {i} and {j})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HYPOTHESIS TEST\n",
      "============================================================\n",
      "\n",
      "Token 48494:\n",
      "  Norm: 1.16991878\n",
      "  Distance from centroid: 1.10977745\n",
      "\n",
      "Token 71473:\n",
      "  Norm: 1.21912193\n",
      "  Distance from centroid: 1.16093612\n",
      "\n",
      "Centroid:\n",
      "  Norm: 0.37091014\n",
      "\n",
      "============================================================\n",
      "\n",
      "RESULT:\n",
      "  Outlier: token 71473\n",
      "    Norm: 1.21912193 (3.29× centroid)\n",
      "    Distance from centroid: 1.16093612\n",
      "\n",
      "  Core token: token 48494\n",
      "    Norm: 1.16991878 (3.15× centroid)\n",
      "    Distance from centroid: 1.10977745\n",
      "\n",
      "Hypothesis supported: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPOTHESIS TEST\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Get the two tokens' embeddings\n",
    "t_i = W[token_i]\n",
    "t_j = W[token_j]\n",
    "\n",
    "# Compute norms\n",
    "norm_i = t_i.norm().item()\n",
    "norm_j = t_j.norm().item()\n",
    "norm_centroid = centroid.norm().item()\n",
    "\n",
    "# Compute distances from centroid\n",
    "dist_i = (t_i - centroid).norm().item()\n",
    "dist_j = (t_j - centroid).norm().item()\n",
    "\n",
    "print(f\"Token {token_i}:\")\n",
    "print(f\"  Norm: {norm_i:.8f}\")\n",
    "print(f\"  Distance from centroid: {dist_i:.8f}\")\n",
    "print()\n",
    "print(f\"Token {token_j}:\")\n",
    "print(f\"  Norm: {norm_j:.8f}\")\n",
    "print(f\"  Distance from centroid: {dist_j:.8f}\")\n",
    "print()\n",
    "print(f\"Centroid:\")\n",
    "print(f\"  Norm: {norm_centroid:.8f}\")\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Identify the outlier\n",
    "if norm_i > norm_j:\n",
    "    outlier_id = token_i\n",
    "    outlier_norm = norm_i\n",
    "    outlier_dist = dist_i\n",
    "    core_id = token_j\n",
    "    core_norm = norm_j\n",
    "    core_dist = dist_j\n",
    "else:\n",
    "    outlier_id = token_j\n",
    "    outlier_norm = norm_j\n",
    "    outlier_dist = dist_j\n",
    "    core_id = token_i\n",
    "    core_norm = norm_i\n",
    "    core_dist = dist_i\n",
    "\n",
    "print(f\"RESULT:\")\n",
    "print(f\"  Outlier: token {outlier_id}\")\n",
    "print(f\"    Norm: {outlier_norm:.8f} ({outlier_norm/norm_centroid:.2f}× centroid)\")\n",
    "print(f\"    Distance from centroid: {outlier_dist:.8f}\")\n",
    "print()\n",
    "print(f\"  Core token: token {core_id}\")\n",
    "print(f\"    Norm: {core_norm:.8f} ({core_norm/norm_centroid:.2f}× centroid)\")\n",
    "print(f\"    Distance from centroid: {core_dist:.8f}\")\n",
    "print()\n",
    "print(f\"Hypothesis supported: {outlier_norm > 2 * norm_centroid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster statistics:\n",
      "  Norms:\n",
      "    Min: 0.37029281\n",
      "    Max: 1.21912193\n",
      "    Median: 0.37091675\n",
      "    Mean: 0.37223831\n",
      "\n",
      "  Distances from centroid:\n",
      "    Min: 0.00125120\n",
      "    Max: 1.16093612\n",
      "    Median: 0.00125222\n",
      "    Mean: 0.00374321\n"
     ]
    }
   ],
   "source": [
    "# Compute all norms and distances for context\n",
    "all_norms = W_cluster.norm(dim=1)\n",
    "all_distances = (W_cluster - centroid).norm(dim=1)\n",
    "\n",
    "print(\"\\nCluster statistics:\")\n",
    "print(f\"  Norms:\")\n",
    "print(f\"    Min: {all_norms.min().item():.8f}\")\n",
    "print(f\"    Max: {all_norms.max().item():.8f}\")\n",
    "print(f\"    Median: {all_norms.median().item():.8f}\")\n",
    "print(f\"    Mean: {all_norms.mean().item():.8f}\")\n",
    "print()\n",
    "print(f\"  Distances from centroid:\")\n",
    "print(f\"    Min: {all_distances.min().item():.8f}\")\n",
    "print(f\"    Max: {all_distances.max().item():.8f}\")\n",
    "print(f\"    Median: {all_distances.median().item():.8f}\")\n",
    "print(f\"    Mean: {all_distances.mean().item():.8f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
