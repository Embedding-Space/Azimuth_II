{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1.1a: Extract Unembedding Matrix W\n",
    "\n",
    "This notebook extracts the unembedding matrix **W** from a language model and saves it in its native bfloat16 format.\n",
    "\n",
    "## What is W?\n",
    "\n",
    "In a transformer language model, the **unembedding matrix** (also called `lm_head` in many implementations) maps the model's final hidden states back to vocabulary logits. For a model with:\n",
    "- Vocabulary size V (number of tokens)\n",
    "- Hidden dimension d (size of internal representations)\n",
    "\n",
    "W is a V × d matrix where each row represents one token as a d-dimensional vector.\n",
    "\n",
    "## Coordinate System: Gamma (γ)\n",
    "\n",
    "We call this representation **gamma space** (γ): the origin-centered coordinate frame aligned with the model's natural axes. This is the raw geometric structure as trained.\n",
    "\n",
    "Later notebooks may introduce:\n",
    "- **γ'** (gamma prime): centroid-subtracted coordinates\n",
    "- Other derived coordinate systems\n",
    "\n",
    "But we start here with the pristine matrix as the model contains it.\n",
    "\n",
    "## Why Save as bfloat16?\n",
    "\n",
    "Modern models train in bfloat16 for efficiency. We preserve this:\n",
    "1. **Scientific integrity**: This is the actual representation the model uses\n",
    "2. **Enables quantization analysis**: Later we'll examine the discrete lattice structure\n",
    "3. **Storage efficiency**: Half the size of float32\n",
    "\n",
    "Individual notebooks will convert to float32 explicitly when needed for numerical precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to extract from (Hugging Face identifier)\n",
    "MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "# MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# MODEL_ID = \"Qwen/Qwen1.5-4B-Chat\"\n",
    "# MODEL_ID = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "# MODEL_ID = \"google/gemma-3-4b-it\"\n",
    "# MODEL_ID = \"ibm-granite/granite-4.0-micro\"\n",
    "# MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Output name (drop organization prefix for cleaner paths)\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from safetensors.torch import save_file\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Load Model and Extract W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen3-4B-Instruct-2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c23b8488404c8ab025d87b1b67e29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,  # Load in native precision\n",
    "    device_map=\"cpu\",            # Keep on CPU (we're just extracting weights)\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unembedding matrix W:\n",
      "  Shape: torch.Size([151936, 2560]) (vocab_size × hidden_dim)\n",
      "  Dtype: torch.bfloat16\n",
      "  Device: cpu\n",
      "  Memory: 741.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Extract unembedding matrix W\n",
    "W = model.lm_head.weight.data\n",
    "\n",
    "print(f\"Unembedding matrix W:\")\n",
    "print(f\"  Shape: {W.shape} (vocab_size × hidden_dim)\")\n",
    "print(f\"  Dtype: {W.dtype}\")\n",
    "print(f\"  Device: {W.device}\")\n",
    "print(f\"  Memory: {W.element_size() * W.nelement() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Save W to Disk\n",
    "\n",
    "We save in safetensors format (fast, safe, cross-platform) to `../tensors/{model_name}/W.safetensors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved W to ../tensors/Qwen3-4B-Instruct-2507/W.safetensors\n",
      "  File size: 741.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(f\"../tensors/{MODEL_NAME}\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save W in pristine bfloat16 format\n",
    "output_path = output_dir / \"W.safetensors\"\n",
    "save_file({\"W\": W}, output_path)\n",
    "\n",
    "print(f\"✓ Saved W to {output_path}\")\n",
    "print(f\"  File size: {output_path.stat().st_size / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've extracted the unembedding matrix W and saved it in its native bfloat16 format. This matrix represents the vocabulary as a point cloud in high-dimensional space—each token is a vector.\n",
    "\n",
    "**Next step:** Compute geometric properties (norms, distances, etc.) to begin exploring this space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
