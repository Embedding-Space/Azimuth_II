{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.12e: Wordybird 3 - Continuation from Checkpoint 100\n",
    "\n",
    "**Experiment:** Load Wordybird 1's final state (step 100) and continue training for another 100 steps.\n",
    "\n",
    "## The Mystery\n",
    "\n",
    "**Wordybird 1 (steps 0-100):** Lattice hop analysis (1.17b) showed apparent freeze around step 42, with mean displacement magnitude dropping to ~1e-5 by step 100.\n",
    "\n",
    "**Wordybird 2 (steps 0-1000):** Same analysis showed gradual freeze continuing all the way to step 900!\n",
    "\n",
    "**The Question:** Was Wordybird 1 truly frozen at step 100, or was it just the beginning of a longer freeze process?\n",
    "\n",
    "## Test\n",
    "\n",
    "If WB1 was **truly frozen** at step 100:\n",
    "- WB3 (steps 101-200) should show zero non-lattice movement\n",
    "- Tokens remain stuck in lattice cells\n",
    "- Loss might still improve (attention keeps learning)\n",
    "\n",
    "If WB1 was **not frozen** (threshold issue or incomplete freeze):\n",
    "- WB3 should show continued movement\n",
    "- Gradual freeze continues past step 100\n",
    "- Matches WB2's gradual decay pattern\n",
    "\n",
    "## Wordybird 3 Parameters\n",
    "\n",
    "**Starting from:**\n",
    "- Checkpoint: `box_3/tensors/Wordybird/checkpoint-100`\n",
    "- Model state: WB1 step 100 (embeddings, attention layers)\n",
    "- Optimizer state: Adam momentum & variance preserved\n",
    "\n",
    "**Training:**\n",
    "- **Steps: 100** (continuing from 100 → 200)\n",
    "- All other hyperparameters identical to WB1\n",
    "- Record every step (101 snapshots total)\n",
    "\n",
    "**Output:** `1.12e_wordybird_3.safetensors`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (unchanged)\n",
    "VOCAB_SIZE = 50257  # GPT-2\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYER = 2\n",
    "N_HEAD = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "NUM_TRAIN_STEPS = 100  # Continue for another 100 steps\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# Optimizer: Adam (state will be loaded from checkpoint)\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "# Data\n",
    "CORPUS_PATH = \"../data/fineweb_2mb_unicode.txt\"\n",
    "TOKEN_MASK_PATH = \"../tensors/Wordybird/fineweb_token_masks.safetensors\"\n",
    "CHECKPOINT_PATH = \"../tensors/Wordybird/checkpoint-100\"  # ← Load from here\n",
    "OUTPUT_DIR = \"../tensors/Wordybird\"\n",
    "OUTPUT_FILE = \"1.12e_wordybird_3.safetensors\"  # ← New output\n",
    "\n",
    "# Instrumentation\n",
    "RECORD_EVERY_N_STEPS = 1  # Record every step\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file, load_file\n",
    "import time\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 tokenizer...\n",
      "\n",
      "✓ Loaded GPT-2 tokenizer\n",
      "  Vocabulary size: 50,257 tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading GPT-2 tokenizer...\\n\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(f\"✓ Loaded GPT-2 tokenizer\")\n",
    "print(f\"  Vocabulary size: {len(tokenizer):,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading corpus: ../data/fineweb_2mb_unicode.txt\n",
      "\n",
      "✓ Loaded corpus\n",
      "  Size: 2.00 MB\n",
      "  Characters: 2,089,201\n",
      "\n",
      "Tokenizing corpus...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (475160 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenized\n",
      "  Tokens: 475,160\n",
      "\n",
      "✓ Corpus on device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading corpus: {CORPUS_PATH}\\n\")\n",
    "\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "corpus_bytes = len(corpus_text.encode('utf-8'))\n",
    "corpus_mb = corpus_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"✓ Loaded corpus\")\n",
    "print(f\"  Size: {corpus_mb:.2f} MB\")\n",
    "print(f\"  Characters: {len(corpus_text):,}\")\n",
    "print()\n",
    "\n",
    "# Tokenize\n",
    "print(\"Tokenizing corpus...\\n\")\n",
    "tokens = tokenizer.encode(corpus_text)\n",
    "\n",
    "print(f\"✓ Tokenized\")\n",
    "print(f\"  Tokens: {len(tokens):,}\")\n",
    "print()\n",
    "\n",
    "# Pre-load to device\n",
    "corpus_tensor = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "print(f\"✓ Corpus on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Token Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading token masks: ../tensors/Wordybird/fineweb_token_masks.safetensors\n",
      "\n",
      "✓ Loaded token masks\n",
      "  Trained tokens: 30,590 (60.9%)\n",
      "  Untrained tokens: 19,667 (39.1%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading token masks: {TOKEN_MASK_PATH}\\n\")\n",
    "\n",
    "mask_data = load_file(TOKEN_MASK_PATH)\n",
    "trained_mask = mask_data['trained_mask']\n",
    "untrained_mask = mask_data['untrained_mask']\n",
    "trained_indices = mask_data['trained_indices']\n",
    "untrained_indices = mask_data['untrained_indices']\n",
    "\n",
    "n_trained = trained_mask.sum().item()\n",
    "n_untrained = untrained_mask.sum().item()\n",
    "\n",
    "print(f\"✓ Loaded token masks\")\n",
    "print(f\"  Trained tokens: {n_trained:,} ({100*n_trained/VOCAB_SIZE:.1f}%)\")\n",
    "print(f\"  Untrained tokens: {n_untrained:,} ({100*n_untrained/VOCAB_SIZE:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset: 475,032 examples\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = TokenDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "print(f\"\\n✓ Dataset: {len(dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING WORDYBIRD 1 CHECKPOINT (STEP 100)\n",
      "================================================================================\n",
      "\n",
      "Loading from: ../tensors/Wordybird/checkpoint-100\n",
      "\n",
      "✓ Model loaded from checkpoint\n",
      "  Total parameters: 3,324,736\n",
      "  Embedding parameters: 3,216,448\n",
      "  Model dtype: torch.bfloat16\n",
      "\n",
      "Initial state (step 100 from WB1):\n",
      "  Untrained centroid norm: 0.325751\n",
      "  Bounding radius: 0.211612\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"LOADING WORDYBIRD 1 CHECKPOINT (STEP 100)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Loading from: {CHECKPOINT_PATH}\\n\")\n",
    "\n",
    "# Load model (this includes all weights at step 100)\n",
    "model = GPT2LMHeadModel.from_pretrained(CHECKPOINT_PATH)\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "embedding_params = model.transformer.wte.weight.numel()\n",
    "\n",
    "print(f\"✓ Model loaded from checkpoint\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Embedding parameters: {embedding_params:,}\")\n",
    "print(f\"  Model dtype: {model.transformer.wte.weight.dtype}\")\n",
    "print()\n",
    "\n",
    "# Verify initial state\n",
    "W_initial = model.transformer.wte.weight.cpu().float()\n",
    "W_untrained_initial = W_initial[untrained_indices]\n",
    "centroid_initial = W_untrained_initial.mean(dim=0)\n",
    "radius_initial = torch.norm(W_untrained_initial - centroid_initial, dim=1).max().item()\n",
    "\n",
    "print(f\"Initial state (step 100 from WB1):\")\n",
    "print(f\"  Untrained centroid norm: {torch.norm(centroid_initial).item():.6f}\")\n",
    "print(f\"  Bounding radius: {radius_initial:.6f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Recorder class defined\n"
     ]
    }
   ],
   "source": [
    "class ComprehensiveRecorder:\n",
    "    \"\"\"Records embeddings, gradients, optimizer state, logits, loss at every step in bfloat16.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim, record_every_n, starting_step=100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.record_every_n = record_every_n\n",
    "        self.starting_step = starting_step  # ← NEW: track that we start at 100\n",
    "        \n",
    "        # Storage\n",
    "        self.recorded_steps = []\n",
    "        self.embeddings = []\n",
    "        self.grads = []\n",
    "        self.momentum = []\n",
    "        self.variance = []\n",
    "        self.logits = []\n",
    "        self.losses = []\n",
    "        \n",
    "        # Temporary storage\n",
    "        self.current_step = starting_step\n",
    "        self.recorded_initial = False\n",
    "        self.grad_before = None\n",
    "        self.loss_value = None\n",
    "        self.logits_sample = None\n",
    "    \n",
    "    def record_initial_state(self, model, optimizer=None):\n",
    "        \"\"\"Record step 100: initial state from checkpoint.\"\"\"\n",
    "        if not self.recorded_initial:\n",
    "            W = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "            \n",
    "            # Extract optimizer state (if optimizer exists)\n",
    "            param = model.transformer.wte.weight\n",
    "            if optimizer is not None and param in optimizer.state:\n",
    "                state = optimizer.state[param]\n",
    "                mom_src = state.get('exp_avg', None)\n",
    "                var_src = state.get('exp_avg_sq', None)\n",
    "                mom = mom_src.clone().cpu().bfloat16() if mom_src is not None else torch.zeros_like(W)\n",
    "                var = var_src.clone().cpu().bfloat16() if var_src is not None else torch.zeros_like(W)\n",
    "            else:\n",
    "                # Optimizer not initialized yet, use zeros\n",
    "                mom = torch.zeros_like(W)\n",
    "                var = torch.zeros_like(W)\n",
    "            \n",
    "            self.recorded_steps.append(self.starting_step)\n",
    "            self.embeddings.append(W)\n",
    "            self.grads.append(torch.zeros_like(W))  # No grad at checkpoint load\n",
    "            self.momentum.append(mom)\n",
    "            self.variance.append(var)\n",
    "            self.logits.append(torch.zeros(self.vocab_size, dtype=torch.bfloat16))\n",
    "            self.losses.append(torch.tensor(float('nan'), dtype=torch.bfloat16))\n",
    "            \n",
    "            self.recorded_initial = True\n",
    "            self.current_step = self.starting_step + 1\n",
    "            \n",
    "            opt_status = \"with optimizer state\" if (optimizer is not None and param in optimizer.state) else \"without optimizer state (will load on first step)\"\n",
    "            print(f\"✓ Recorded initial state (step {self.starting_step} from checkpoint, {opt_status})\")\n",
    "    \n",
    "    def record_before_step(self, model, loss, logits):\n",
    "        \"\"\"Call after forward/backward, before optimizer step.\"\"\"\n",
    "        if self.current_step % self.record_every_n == 0:\n",
    "            if model.transformer.wte.weight.grad is not None:\n",
    "                self.grad_before = model.transformer.wte.weight.grad.clone().cpu().bfloat16()\n",
    "            else:\n",
    "                self.grad_before = torch.zeros(self.vocab_size, self.hidden_dim, dtype=torch.bfloat16)\n",
    "            \n",
    "            self.loss_value = loss.item()\n",
    "            self.logits_sample = logits[0, -1, :].detach().cpu().bfloat16()\n",
    "    \n",
    "    def record_after_step(self, model, optimizer):\n",
    "        \"\"\"Call after optimizer step.\"\"\"\n",
    "        if self.current_step % self.record_every_n == 0:\n",
    "            if self.grad_before is not None and self.loss_value is not None:\n",
    "                W = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "\n",
    "                param = model.transformer.wte.weight\n",
    "                if param in optimizer.state:\n",
    "                    state = optimizer.state[param]\n",
    "                    mom_src = state.get('exp_avg', None)\n",
    "                    var_src = state.get('exp_avg_sq', None)\n",
    "                    mom = mom_src.clone().cpu().bfloat16() if mom_src is not None else torch.zeros_like(W)\n",
    "                    var = var_src.clone().cpu().bfloat16() if var_src is not None else torch.zeros_like(W)\n",
    "                else:\n",
    "                    mom = torch.zeros_like(W)\n",
    "                    var = torch.zeros_like(W)\n",
    "\n",
    "                self.recorded_steps.append(self.current_step)\n",
    "                self.embeddings.append(W)\n",
    "                self.grads.append(self.grad_before)\n",
    "                self.momentum.append(mom)\n",
    "                self.variance.append(var)\n",
    "                self.logits.append(self.logits_sample)\n",
    "                self.losses.append(torch.tensor(self.loss_value, dtype=torch.bfloat16))\n",
    "\n",
    "                self.grad_before = None\n",
    "                self.loss_value = None\n",
    "                self.logits_sample = None\n",
    "                \n",
    "                if self.current_step % 10 == 0:\n",
    "                    print(f\"  Recorded step {self.current_step}\")\n",
    "\n",
    "        self.current_step += 1\n",
    "    \n",
    "    def get_data(self):\n",
    "        \"\"\"Return recorded data as stacked tensors.\"\"\"\n",
    "        print(f\"\\nStacking {len(self.embeddings)} recorded states...\")\n",
    "        \n",
    "        return {\n",
    "            'recorded_steps': torch.tensor(self.recorded_steps, dtype=torch.long),\n",
    "            'embeddings': torch.stack(self.embeddings) if self.embeddings else torch.tensor([]),\n",
    "            'grads': torch.stack(self.grads) if self.grads else torch.tensor([]),\n",
    "            'momentum': torch.stack(self.momentum) if self.momentum else torch.tensor([]),\n",
    "            'variance': torch.stack(self.variance) if self.variance else torch.tensor([]),\n",
    "            'logits': torch.stack(self.logits) if self.logits else torch.tensor([]),\n",
    "            'losses': torch.stack(self.losses) if self.losses else torch.tensor([]),\n",
    "        }\n",
    "\n",
    "print(\"✓ Recorder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Trainer with Instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ InstrumentedTrainer defined\n"
     ]
    }
   ],
   "source": [
    "class InstrumentedTrainer(Trainer):\n",
    "    def __init__(self, recorder, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.recorder = recorder\n",
    "        self.last_logits = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        self.last_logits = outputs.logits\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        loss = super().training_step(model, inputs, num_items_in_batch)\n",
    "        self.recorder.record_before_step(model, loss, self.last_logits)\n",
    "        return loss\n",
    "\n",
    "    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time=None, **kwargs):\n",
    "        self.recorder.record_after_step(model, self.optimizer)\n",
    "        super()._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, **kwargs)\n",
    "\n",
    "print(\"✓ InstrumentedTrainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Trainer ready (continuing from checkpoint)\n"
     ]
    }
   ],
   "source": [
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "recorder = ComprehensiveRecorder(VOCAB_SIZE, HIDDEN_DIM, RECORD_EVERY_N_STEPS, starting_step=100)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_steps=NUM_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    adam_beta1=ADAM_BETA1,\n",
    "    adam_beta2=ADAM_BETA2,\n",
    "    adam_epsilon=ADAM_EPSILON,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=10,\n",
    "    save_steps=NUM_TRAIN_STEPS + 1,\n",
    "    save_total_limit=0,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    "    bf16=True,\n",
    "    seed=RANDOM_SEED,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = InstrumentedTrainer(\n",
    "    recorder=recorder,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Trainer ready (continuing from checkpoint)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Initial State (Step 100 from Checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Recorded initial state (step 100 from checkpoint, without optimizer state (will load on first step))\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "recorder.record_initial_state(model, trainer.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train (Continue for 100 More Steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING WORDYBIRD 3 TRAINING (STEPS 101-200)\n",
      "================================================================================\n",
      "\n",
      "Continuing from Wordybird 1 step 100...\n",
      "  Steps: 101-200 (100 additional steps)\n",
      "  Recording: every step\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.925400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.745400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>7.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.707400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>7.660700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.703000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Recorded step 110\n",
      "  Recorded step 120\n",
      "  Recorded step 130\n",
      "  Recorded step 140\n",
      "  Recorded step 150\n",
      "  Recorded step 160\n",
      "  Recorded step 170\n",
      "  Recorded step 180\n",
      "  Recorded step 190\n",
      "  Recorded step 200\n",
      "\n",
      "================================================================================\n",
      "✓ Training complete\n",
      "  Elapsed time: 14.0 seconds\n",
      "  Throughput: 7.1 steps/second\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STARTING WORDYBIRD 3 TRAINING (STEPS 101-200)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nContinuing from Wordybird 1 step 100...\")\n",
    "print(f\"  Steps: 101-200 (100 additional steps)\")\n",
    "print(f\"  Recording: every step\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training complete\")\n",
    "print(f\"  Elapsed time: {elapsed:.1f} seconds\")\n",
    "print(f\"  Throughput: {NUM_TRAIN_STEPS / elapsed:.1f} steps/second\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Recorded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data for save...\n",
      "\n",
      "\n",
      "Stacking 101 recorded states...\n",
      "Saving to: ../tensors/Wordybird/1.12e_wordybird_3.safetensors\n",
      "\n",
      "✓ Saved successfully\n",
      "  File: ../tensors/Wordybird/1.12e_wordybird_3.safetensors\n",
      "  Size: 2609.0 MB\n",
      "  Save time: 0.8 seconds\n",
      "  Recorded steps: 101\n",
      "  Step range: 100 to 200\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nPreparing data for save...\\n\")\n",
    "\n",
    "recorded_data = recorder.get_data()\n",
    "\n",
    "save_dict = {\n",
    "    'recorded_steps': recorded_data['recorded_steps'],\n",
    "    'embeddings': recorded_data['embeddings'],\n",
    "    'grads': recorded_data['grads'],\n",
    "    'momentum': recorded_data['momentum'],\n",
    "    'variance': recorded_data['variance'],\n",
    "    'logits': recorded_data['logits'],\n",
    "    'losses': recorded_data['losses'],\n",
    "    # Metadata\n",
    "    'learning_rate': torch.tensor(LEARNING_RATE, dtype=torch.float32),\n",
    "    'weight_decay': torch.tensor(WEIGHT_DECAY, dtype=torch.float32),\n",
    "    'adam_beta1': torch.tensor(ADAM_BETA1, dtype=torch.float32),\n",
    "    'adam_beta2': torch.tensor(ADAM_BETA2, dtype=torch.float32),\n",
    "    'n_trained': torch.tensor(n_trained, dtype=torch.long),\n",
    "    'n_untrained': torch.tensor(n_untrained, dtype=torch.long),\n",
    "    'starting_step': torch.tensor(100, dtype=torch.long),  # Mark that we continued from 100\n",
    "}\n",
    "\n",
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILE\n",
    "\n",
    "print(f\"Saving to: {output_path}\")\n",
    "\n",
    "save_start = time.time()\n",
    "save_file(save_dict, str(output_path))\n",
    "save_elapsed = time.time() - save_start\n",
    "\n",
    "file_size_mb = output_path.stat().st_size / 1e6\n",
    "\n",
    "print(f\"\\n✓ Saved successfully\")\n",
    "print(f\"  File: {output_path}\")\n",
    "print(f\"  Size: {file_size_mb:.1f} MB\")\n",
    "print(f\"  Save time: {save_elapsed:.1f} seconds\")\n",
    "print(f\"  Recorded steps: {len(recorded_data['recorded_steps'])}\")\n",
    "print(f\"  Step range: {recorded_data['recorded_steps'][0]} to {recorded_data['recorded_steps'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUICK VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Data shapes:\n",
      "  embeddings: torch.Size([101, 50257, 64])\n",
      "  losses: torch.Size([101])\n",
      "\n",
      "Untrained token displacement (steps 100 → 200):\n",
      "  Max: 2.55e-01\n",
      "  Mean: 2.47e-01\n",
      "  Median: 2.47e-01\n",
      "\n",
      "  → STILL MOVING (mean displacement ≥ 1e-4)\n",
      "  → Freeze continues past step 100\n",
      "\n",
      "Loss trajectory:\n",
      "  Step 101: 8.0625\n",
      "  Step 200: 7.7500\n",
      "  Change: -0.3125\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"QUICK VERIFICATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "embeddings = recorded_data['embeddings']\n",
    "losses = recorded_data['losses']\n",
    "\n",
    "print(f\"Data shapes:\")\n",
    "print(f\"  embeddings: {embeddings.shape}\")\n",
    "print(f\"  losses: {losses.shape}\")\n",
    "print()\n",
    "\n",
    "# Analyze untrained token movement\n",
    "W_step100 = embeddings[0, untrained_indices].float()  # Initial (from checkpoint)\n",
    "W_step200 = embeddings[-1, untrained_indices].float()  # Final\n",
    "\n",
    "# Compute displacements\n",
    "displacements = torch.norm(W_step200 - W_step100, dim=1)\n",
    "max_displacement = displacements.max().item()\n",
    "mean_displacement = displacements.mean().item()\n",
    "median_displacement = displacements.median().item()\n",
    "\n",
    "print(f\"Untrained token displacement (steps 100 → 200):\")\n",
    "print(f\"  Max: {max_displacement:.2e}\")\n",
    "print(f\"  Mean: {mean_displacement:.2e}\")\n",
    "print(f\"  Median: {median_displacement:.2e}\")\n",
    "print()\n",
    "\n",
    "# Compare to WB1 step 0 → 100 for context\n",
    "if mean_displacement < 1e-4:\n",
    "    print(f\"  → FROZEN (mean displacement < 1e-4)\")\n",
    "    print(f\"  → Supports WB1 freeze hypothesis!\")\n",
    "else:\n",
    "    print(f\"  → STILL MOVING (mean displacement ≥ 1e-4)\")\n",
    "    print(f\"  → Freeze continues past step 100\")\n",
    "\n",
    "print()\n",
    "print(f\"Loss trajectory:\")\n",
    "print(f\"  Step 101: {losses[1].float().item():.4f}\")\n",
    "print(f\"  Step 200: {losses[-1].float().item():.4f}\")\n",
    "print(f\"  Change: {(losses[-1].float() - losses[1].float()).item():+.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WORDYBIRD 3 COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Experiment: Continuation from WB1 checkpoint (step 100 → 200)\n",
      "  Continued for: 100 additional steps\n",
      "  Data saved: ../tensors/Wordybird/1.12e_wordybird_3.safetensors\n",
      "  Size: 2609.0 MB\n",
      "\n",
      "Next steps:\n",
      "  1. Run 1.17b lattice hop analysis on WB3\n",
      "  2. Compare to WB1 and WB2 freeze patterns\n",
      "  3. Determine if WB1 was truly frozen or still freezing\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"WORDYBIRD 3 COMPLETE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Experiment: Continuation from WB1 checkpoint (step 100 → 200)\")\n",
    "print(f\"  Continued for: {NUM_TRAIN_STEPS} additional steps\")\n",
    "print(f\"  Data saved: {output_path}\")\n",
    "print(f\"  Size: {file_size_mb:.1f} MB\")\n",
    "print()\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  1. Run 1.17b lattice hop analysis on WB3\")\n",
    "print(f\"  2. Compare to WB1 and WB2 freeze patterns\")\n",
    "print(f\"  3. Determine if WB1 was truly frozen or still freezing\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
