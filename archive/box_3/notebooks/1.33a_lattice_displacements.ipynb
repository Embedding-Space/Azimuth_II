{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.33a: Lattice Displacement Calculation (Revised)\n",
    "\n",
    "Computing displacements in lattice-cell coordinates for all tokens, with proper validation and memory management.\n",
    "\n",
    "## Memory Budget\n",
    "\n",
    "Maximum 24 GB RAM at any one time. We maximize usage up to that ceiling.\n",
    "\n",
    "## The Algorithm\n",
    "\n",
    "For a displacement ŒîW = W[t+1] - W[t], the lattice coordinate displacement is:\n",
    "\n",
    "$$\\Delta W' = \\frac{\\Delta W}{U[t]}$$\n",
    "\n",
    "where U[t] = ULP(W[t]) is the lattice spacing at the starting position.\n",
    "\n",
    "**Expected result**: ŒîW‚Ä≤ should be exact integers (since both W values live on the bfloat16 lattice).\n",
    "\n",
    "## Validation Strategy\n",
    "\n",
    "Compute delta_W_prime in **float32** to detect non-integer results:\n",
    "\n",
    "- `frac = |x - floor(x)|`\n",
    "- `frac == 0`: ‚úÖ Exact integer (expected)\n",
    "- `0 < frac < Œµ`: ‚ö†Ô∏è Float rounding error (needs attention, but algorithm is basically correct)\n",
    "- `frac >= Œµ`: üö® GENERAL QUARTERS - algorithm is broken\n",
    "\n",
    "**Float32 limitation**: Can only represent integers exactly up to 2^24 = 16,777,216. We check for overflow.\n",
    "\n",
    "## What We DON'T Do (Yet)\n",
    "\n",
    "We do NOT save results until we know what integer type is needed. Save cells are left empty, ready to execute once we determine the range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "THIMBLE_PATH = \"../tensors/Thimble/thimble_7.h5\"\n",
    "OUTPUT_PATH = \"../tensors/Thimble/1.33a_lattice_displacements.safetensors\"\n",
    "\n",
    "# Validation thresholds\n",
    "EPSILON = 1e-6  # Tolerance for \"almost integer\" (float32 rounding error)\n",
    "\n",
    "# Float32 integer fidelity limit\n",
    "FLOAT32_INT_LIMIT = 2**24  # 16,777,216\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports complete\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../tensors/Thimble/thimble_7.h5...\n",
      "\n",
      "‚úì Loaded W\n",
      "  Shape: torch.Size([6001, 10000, 64])\n",
      "  Dtype: torch.bfloat16\n",
      "  Steps: 6001, Tokens: 10000, Dimensions: 64\n",
      "  Dead tokens: 3699 (37.0%)\n",
      "  Memory: 7.68 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading data from {THIMBLE_PATH}...\\n\")\n",
    "\n",
    "with h5py.File(THIMBLE_PATH, 'r') as f:\n",
    "    # Load W for all tokens in bfloat16: (6001, 10000, 64)\n",
    "    W = torch.from_numpy(f['W'][:]).view(torch.bfloat16).to(device)\n",
    "    \n",
    "    # Load dead token mask\n",
    "    dead_mask = torch.from_numpy(f['dead_mask'][:]).bool()\n",
    "\n",
    "n_steps, n_tokens, n_dims = W.shape\n",
    "n_dead = dead_mask.sum().item()\n",
    "dead_token_ids = torch.where(dead_mask)[0]\n",
    "\n",
    "print(f\"‚úì Loaded W\")\n",
    "print(f\"  Shape: {W.shape}\")\n",
    "print(f\"  Dtype: {W.dtype}\")\n",
    "print(f\"  Steps: {n_steps}, Tokens: {n_tokens}, Dimensions: {n_dims}\")\n",
    "print(f\"  Dead tokens: {n_dead} ({n_dead/n_tokens:.1%})\")\n",
    "print(f\"  Memory: {W.element_size() * W.nelement() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute ULP Matrix\n",
    "\n",
    "**Note:** We compute ULP in float32 (not bfloat16) because `torch.nextafter` works reliably in float32 across all devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ULP matrix...\n",
      "\n",
      "  Moving W to CPU for ULP calculation...\n",
      "  Computing ULP on CPU (this takes ~5-6 seconds)...\n",
      "  Moving U to mps...\n",
      "\n",
      "‚úì ULP matrix computed\n",
      "  Shape: torch.Size([6001, 10000, 64])\n",
      "  Dtype: torch.bfloat16\n",
      "  Memory: 7.68 GB\n",
      "\n",
      "  Exact zeros in W: 263 (0.000007%)\n",
      "  Min ULP: 1.175494e-38\n",
      "  Max ULP: 3.906250e-03\n",
      "\n",
      "  Current memory usage: 15.36 GB (W + U)\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing ULP matrix...\\n\")\n",
    "\n",
    "def compute_ulp(x):\n",
    "    \"\"\"\n",
    "    Compute ULP for bfloat16 values, handling zeros correctly.\n",
    "    \n",
    "    For non-zero values: ULP = nextafter(|x|, |x|+1) - |x|\n",
    "    For exact zeros: ULP = smallest positive normal bfloat16 ‚âà 1.175e-38\n",
    "    \n",
    "    Returns bfloat16 tensor on CPU.\n",
    "    \"\"\"\n",
    "    x_abs = x.abs()\n",
    "    ulp = torch.nextafter(x_abs, x_abs + torch.ones_like(x_abs)) - x_abs\n",
    "    \n",
    "    # For exact zeros, use smallest normal bfloat16\n",
    "    min_normal_bf16 = torch.tensor(2.0**-126, dtype=torch.bfloat16, device=x.device)\n",
    "    ulp = torch.where(x == 0, min_normal_bf16, ulp)\n",
    "    \n",
    "    return ulp\n",
    "\n",
    "# Move W to CPU for ULP calculation (torch.nextafter doesn't work for bfloat16 on MPS)\n",
    "print(\"  Moving W to CPU for ULP calculation...\")\n",
    "W_cpu = W.cpu()\n",
    "\n",
    "# Compute ULP on CPU\n",
    "print(\"  Computing ULP on CPU (this takes ~5-6 seconds)...\")\n",
    "U_cpu = compute_ulp(W_cpu)\n",
    "\n",
    "# Move result back to device\n",
    "print(f\"  Moving U to {device}...\")\n",
    "U = U_cpu.to(device)\n",
    "\n",
    "# Can free CPU copy\n",
    "del W_cpu, U_cpu\n",
    "\n",
    "print(f\"\\n‚úì ULP matrix computed\")\n",
    "print(f\"  Shape: {U.shape}\")\n",
    "print(f\"  Dtype: {U.dtype}\")\n",
    "print(f\"  Memory: {U.element_size() * U.nelement() / 1e9:.2f} GB\")\n",
    "\n",
    "# Diagnostic: count exact zeros\n",
    "n_zeros = (W == 0).sum().item()\n",
    "print(f\"\\n  Exact zeros in W: {n_zeros:,} ({n_zeros/W.numel():.6%})\")\n",
    "print(f\"  Min ULP: {U.min().item():.6e}\")\n",
    "print(f\"  Max ULP: {U.max().item():.6e}\")\n",
    "\n",
    "# Current memory usage\n",
    "mem_W = W.element_size() * W.nelement() / 1e9\n",
    "mem_U = U.element_size() * U.nelement() / 1e9\n",
    "print(f\"\\n  Current memory usage: {mem_W + mem_U:.2f} GB (W + U)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute delta_W (Cartesian Displacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing delta_W (Cartesian displacement)...\n",
      "\n",
      "‚úì delta_W computed\n",
      "  Shape: torch.Size([6000, 10000, 64])\n",
      "  Dtype: torch.bfloat16\n",
      "  Memory: 7.68 GB\n",
      "\n",
      "  Current memory usage: 23.04 GB (W + U + delta_W)\n",
      "  Budget remaining: 0.96 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing delta_W (Cartesian displacement)...\\n\")\n",
    "\n",
    "# Compute displacement in bfloat16\n",
    "delta_W = W[1:] - W[:-1]  # (6000, 10000, 64)\n",
    "\n",
    "print(f\"‚úì delta_W computed\")\n",
    "print(f\"  Shape: {delta_W.shape}\")\n",
    "print(f\"  Dtype: {delta_W.dtype}\")\n",
    "print(f\"  Memory: {delta_W.element_size() * delta_W.nelement() / 1e9:.2f} GB\")\n",
    "\n",
    "# Current memory usage\n",
    "mem_delta_W = delta_W.element_size() * delta_W.nelement() / 1e9\n",
    "total_mem = mem_W + mem_U + mem_delta_W\n",
    "print(f\"\\n  Current memory usage: {total_mem:.2f} GB (W + U + delta_W)\")\n",
    "print(f\"  Budget remaining: {24 - total_mem:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute delta_W_prime (Lattice Displacement)\n",
    "\n",
    "Divide by starting ULP. This MUST be done in float32 to detect non-integer outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing delta_W_prime (lattice displacement)...\n",
      "\n",
      "‚úì Freed W from memory\n",
      "‚úì delta_W_prime computed\n",
      "  Shape: torch.Size([6000, 10000, 64])\n",
      "  Dtype: torch.float32\n",
      "  Memory: 15.36 GB\n",
      "\n",
      "‚úì Freed intermediate tensors\n",
      "  Current memory usage: 23.04 GB (U + delta_W_prime)\n",
      "  Budget remaining: 0.96 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing delta_W_prime (lattice displacement)...\\n\")\n",
    "\n",
    "# We can now discard W (we only need U and delta_W)\n",
    "del W\n",
    "print(\"‚úì Freed W from memory\")\n",
    "\n",
    "# Get starting ULP (t, not t+1)\n",
    "U_start = U[:-1]  # (6000, 10000, 64)\n",
    "\n",
    "# Convert to float32 for division (to detect non-integers)\n",
    "delta_W_f32 = delta_W.to(torch.float32)\n",
    "U_start_f32 = U_start.to(torch.float32)\n",
    "\n",
    "# Compute lattice displacement\n",
    "delta_W_prime = delta_W_f32 / U_start_f32\n",
    "\n",
    "print(f\"‚úì delta_W_prime computed\")\n",
    "print(f\"  Shape: {delta_W_prime.shape}\")\n",
    "print(f\"  Dtype: {delta_W_prime.dtype}\")\n",
    "print(f\"  Memory: {delta_W_prime.element_size() * delta_W_prime.nelement() / 1e9:.2f} GB\")\n",
    "\n",
    "# Current memory\n",
    "mem_delta_W_prime = delta_W_prime.element_size() * delta_W_prime.nelement() / 1e9\n",
    "mem_U_full = U.element_size() * U.nelement() / 1e9\n",
    "# We still have: U (full), delta_W (bf16), delta_W_f32, U_start_f32, delta_W_prime\n",
    "# But delta_W and U_start can be freed now\n",
    "\n",
    "del delta_W, U_start, delta_W_f32, U_start_f32\n",
    "print(\"\\n‚úì Freed intermediate tensors\")\n",
    "\n",
    "current_mem = mem_U_full + mem_delta_W_prime\n",
    "print(f\"  Current memory usage: {current_mem:.2f} GB (U + delta_W_prime)\")\n",
    "print(f\"  Budget remaining: {24 - current_mem:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check: Float32 Integer Fidelity\n",
    "\n",
    "Float32 can only represent integers exactly up to 2^24 = 16,777,216. Beyond that, gaps appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking float32 integer fidelity...\n",
      "\n",
      "delta_W_prime range:\n",
      "  Min: 0.00e+00\n",
      "  Max: 0.00e+00\n",
      "  Max absolute: 0.00e+00\n",
      "\n",
      "Float32 exact integer limit: 16,777,216 (2^24)\n",
      "\n",
      "‚úì All values within float32 exact integer range.\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking float32 integer fidelity...\\n\")\n",
    "\n",
    "min_val = delta_W_prime.min().item()\n",
    "max_val = delta_W_prime.max().item()\n",
    "max_abs = max(abs(min_val), abs(max_val))\n",
    "\n",
    "print(f\"delta_W_prime range:\")\n",
    "print(f\"  Min: {min_val:.2e}\")\n",
    "print(f\"  Max: {max_val:.2e}\")\n",
    "print(f\"  Max absolute: {max_abs:.2e}\")\n",
    "print()\n",
    "print(f\"Float32 exact integer limit: {FLOAT32_INT_LIMIT:,} (2^24)\")\n",
    "\n",
    "if max_abs > FLOAT32_INT_LIMIT:\n",
    "    n_overflow = (delta_W_prime.abs() > FLOAT32_INT_LIMIT).sum().item()\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: {n_overflow:,} values exceed float32 integer fidelity limit!\")\n",
    "    print(f\"   These values cannot be represented exactly as integers in float32.\")\n",
    "    print(f\"   Validation results may be unreliable for these values.\")\n",
    "else:\n",
    "    print(f\"\\n‚úì All values within float32 exact integer range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation: Integer Quantization Check\n",
    "\n",
    "Check that all lattice coordinates are integers (or nearly so).\n",
    "\n",
    "Categories:\n",
    "- `frac == 0`: ‚úÖ Exact integer\n",
    "- `0 < frac < Œµ`: ‚ö†Ô∏è Float rounding error (acceptable)\n",
    "- `frac ‚âà 0.5`: ‚ö†Ô∏è Half-integer (flag for investigation)\n",
    "- `frac >= Œµ`: üö® GENERAL QUARTERS (algorithm broken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating integer quantization...\n",
      "\n",
      "================================================================================\n",
      "INTEGER QUANTIZATION VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Total coordinates: 3,840,000,000\n",
      "Epsilon: 1e-06\n",
      "\n",
      "Exact integers:    3,840,000,000  (100.000000%)\n",
      "Almost integers:   0  (0.000000%)  [float32 rounding]\n",
      "Half-integers:     0  (0.000000%)  [FLAGGED]\n",
      "Non-integers:      0  (0.000000%)  [ERROR!]\n",
      "\n",
      "‚úì‚úì‚úì PERFECT INTEGER LATTICE ‚úì‚úì‚úì\n",
      "  All coordinates are EXACT integers.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nValidating integer quantization...\\n\")\n",
    "\n",
    "# Flatten all coordinates\n",
    "coords = delta_W_prime.flatten().cpu()\n",
    "n_total = len(coords)\n",
    "\n",
    "# Compute fractional part\n",
    "frac_part = torch.abs(coords - torch.floor(coords))\n",
    "\n",
    "# Classify\n",
    "exact = (frac_part == 0)\n",
    "almost = (frac_part > 0) & (frac_part < EPSILON)\n",
    "half = (torch.abs(frac_part - 0.5) < EPSILON)  # Near 0.5\n",
    "non_int = (frac_part >= EPSILON) & ~half\n",
    "\n",
    "n_exact = exact.sum().item()\n",
    "n_almost = almost.sum().item()\n",
    "n_half = half.sum().item()\n",
    "n_non = non_int.sum().item()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INTEGER QUANTIZATION VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"Total coordinates: {n_total:,}\")\n",
    "print(f\"Epsilon: {EPSILON:.0e}\")\n",
    "print()\n",
    "print(f\"Exact integers:    {n_exact:,}  ({n_exact/n_total:.6%})\")\n",
    "print(f\"Almost integers:   {n_almost:,}  ({n_almost/n_total:.6%})  [float32 rounding]\")\n",
    "print(f\"Half-integers:     {n_half:,}  ({n_half/n_total:.6%})  [FLAGGED]\")\n",
    "print(f\"Non-integers:      {n_non:,}  ({n_non/n_total:.6%})  [ERROR!]\")\n",
    "print()\n",
    "\n",
    "# Determine status\n",
    "if n_non > 0:\n",
    "    print(\"üö® \" * 20)\n",
    "    print(\"\\nüö¢ GENERAL QUARTERS! GENERAL QUARTERS! ALL HANDS TO BATTLE STATIONS! üö¢\")\n",
    "    print()\n",
    "    print(f\"Found {n_non:,} NON-INTEGER coordinates!\")\n",
    "    print(\"This indicates the lattice coordinate algorithm is not correct.\")\n",
    "    print()\n",
    "    print(\"üö® \" * 20)\n",
    "elif n_half > 0:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Found half-integer coordinates.\")\n",
    "    print(\"   This may indicate edge cases in the algorithm.\")\n",
    "    print(\"   Flagged for investigation.\")\n",
    "elif n_almost > 0:\n",
    "    print(\"‚úì PASS (with float32 rounding)\")\n",
    "    print(f\"  All coordinates are integers within tolerance.\")\n",
    "    print(f\"  {n_almost:,} coordinates have minor float32 rounding errors.\")\n",
    "else:\n",
    "    print(\"‚úì‚úì‚úì PERFECT INTEGER LATTICE ‚úì‚úì‚úì\")\n",
    "    print(\"  All coordinates are EXACT integers.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation: Non-Standard Coordinates (If Any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì No non-standard coordinates found.\n"
     ]
    }
   ],
   "source": [
    "if n_non > 0 or n_half > 0:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"INVESTIGATING NON-STANDARD COORDINATES\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    if n_non > 0:\n",
    "        print(f\"Non-integer coordinates (sample of 20):\\n\")\n",
    "        non_int_indices = torch.where(non_int)[0].numpy()\n",
    "        sample_size = min(20, len(non_int_indices))\n",
    "        sample_indices = np.random.choice(non_int_indices, size=sample_size, replace=False)\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices, 1):\n",
    "            val = coords[idx].item()\n",
    "            nearest = np.floor(val)\n",
    "            frac = abs(val - nearest)\n",
    "            print(f\"  {i:2d}. {val:20.10f}  (nearest: {nearest:12.0f}, frac: {frac:.10f})\")\n",
    "    \n",
    "    if n_half > 0:\n",
    "        print(f\"\\nHalf-integer coordinates (sample of 20):\\n\")\n",
    "        half_indices = torch.where(half)[0].numpy()\n",
    "        sample_size = min(20, len(half_indices))\n",
    "        sample_indices = np.random.choice(half_indices, size=sample_size, replace=False)\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices, 1):\n",
    "            val = coords[idx].item()\n",
    "            print(f\"  {i:2d}. {val:20.10f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"\\n‚úì No non-standard coordinates found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range Analysis: What Integer Type Do We Need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing range to determine integer type...\n",
      "\n",
      "Integer range:\n",
      "  Min: 0\n",
      "  Max: 0\n",
      "  Max absolute: 0\n",
      "\n",
      "Integer type requirements:\n",
      "  ‚úì int8  : range [¬±127]  FITS\n",
      "  ‚úì int16 : range [¬±32,767]  FITS\n",
      "  ‚úì int32 : range [¬±2,147,483,647]  FITS\n",
      "  ‚úì int64 : range [¬±9,223,372,036,854,775,807]  FITS\n",
      "\n",
      "‚úì Recommended integer type: int8\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnalyzing range to determine integer type...\\n\")\n",
    "\n",
    "# Round to nearest integer for range analysis\n",
    "coords_int = torch.round(coords)\n",
    "\n",
    "min_int = coords_int.min().item()\n",
    "max_int = coords_int.max().item()\n",
    "max_abs_int = max(abs(min_int), abs(max_int))\n",
    "\n",
    "print(f\"Integer range:\")\n",
    "print(f\"  Min: {min_int:,.0f}\")\n",
    "print(f\"  Max: {max_int:,.0f}\")\n",
    "print(f\"  Max absolute: {max_abs_int:,.0f}\")\n",
    "print()\n",
    "\n",
    "# Determine required integer type\n",
    "int_types = [\n",
    "    ('int8', 2**7 - 1, 127),\n",
    "    ('int16', 2**15 - 1, 32_767),\n",
    "    ('int32', 2**31 - 1, 2_147_483_647),\n",
    "    ('int64', 2**63 - 1, 9_223_372_036_854_775_807),\n",
    "]\n",
    "\n",
    "print(\"Integer type requirements:\")\n",
    "recommended_type = None\n",
    "for dtype, limit, limit_val in int_types:\n",
    "    fits = max_abs_int <= limit\n",
    "    status = \"‚úì\" if fits else \"‚úó\"\n",
    "    print(f\"  {status} {dtype:6s}: range [¬±{limit_val:,}]  {'FITS' if fits else 'TOO SMALL'}\")\n",
    "    if fits and recommended_type is None:\n",
    "        recommended_type = dtype\n",
    "\n",
    "print()\n",
    "if recommended_type:\n",
    "    print(f\"‚úì Recommended integer type: {recommended_type}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Values exceed int64 range!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing summary statistics...\n",
      "\n",
      "Displacement magnitude (L2 norm):\n",
      "  All values finite:\n",
      "  Min:    0.00e+00 cells\n",
      "  Max:    0.00e+00 cells\n",
      "  Mean:   0.00 cells\n",
      "  Median: 0.00 cells\n",
      "\n",
      "Percentiles:\n",
      "   50.0%:         0.00 cells\n",
      "   90.0%:         0.00 cells\n",
      "   95.0%:         0.00 cells\n",
      "   99.0%:         0.00 cells\n",
      "   99.9%:         0.00 cells\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing summary statistics...\\n\")\n",
    "\n",
    "# Displacement magnitudes (L2 norm)\n",
    "displacement_mag = torch.norm(delta_W_prime, dim=2).cpu().numpy()  # (6000, 10000)\n",
    "\n",
    "# Filter out inf/nan for statistics\n",
    "mag_finite = displacement_mag[np.isfinite(displacement_mag)]\n",
    "\n",
    "print(f\"Displacement magnitude (L2 norm):\")\n",
    "if len(mag_finite) < len(displacement_mag.flatten()):\n",
    "    n_inf = np.isinf(displacement_mag).sum()\n",
    "    n_nan = np.isnan(displacement_mag).sum()\n",
    "    print(f\"  ‚ö†Ô∏è  Found {n_inf:,} inf and {n_nan:,} nan values\")\n",
    "    print(f\"  Statistics computed on {len(mag_finite):,} finite values:\")\n",
    "else:\n",
    "    print(f\"  All values finite:\")\n",
    "\n",
    "print(f\"  Min:    {mag_finite.min():.2e} cells\")\n",
    "print(f\"  Max:    {mag_finite.max():.2e} cells\")\n",
    "print(f\"  Mean:   {mag_finite.mean():.2f} cells\")\n",
    "print(f\"  Median: {np.median(mag_finite):.2f} cells\")\n",
    "print()\n",
    "print(f\"Percentiles:\")\n",
    "for p in [50, 90, 95, 99, 99.9]:\n",
    "    print(f\"  {p:5.1f}%: {np.percentile(mag_finite, p):12.2f} cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Exponent Crossings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detecting exponent crossings...\n",
      "\n",
      "‚úì Exponent crossings detected\n",
      "  Total transitions: 3,840,000,000\n",
      "  Same exponent: 3,806,213,693 (99.120148%)\n",
      "  Crossed exponent: 33,786,307 (0.879852%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDetecting exponent crossings...\\n\")\n",
    "\n",
    "# Reload W temporarily to check exponents (we freed it earlier)\n",
    "with h5py.File(THIMBLE_PATH, 'r') as f:\n",
    "    W_for_exp = torch.from_numpy(f['W'][:]).view(torch.bfloat16)\n",
    "\n",
    "# Extract exponents from bfloat16\n",
    "# bfloat16: [sign: 1 bit][exponent: 8 bits][mantissa: 7 bits]\n",
    "W_uint16 = W_for_exp.view(torch.uint16).numpy()\n",
    "exponents = (W_uint16 >> 7) & 0xFF  # Shift right 7, mask to 8 bits\n",
    "\n",
    "# Compare consecutive timesteps\n",
    "exp_t = exponents[:-1]   # (6000, 10000, 64)\n",
    "exp_t1 = exponents[1:]   # (6000, 10000, 64)\n",
    "\n",
    "exponent_crossings = (exp_t != exp_t1)\n",
    "\n",
    "n_transitions = exp_t.size\n",
    "n_crossings = exponent_crossings.sum()\n",
    "\n",
    "print(f\"‚úì Exponent crossings detected\")\n",
    "print(f\"  Total transitions: {n_transitions:,}\")\n",
    "print(f\"  Same exponent: {n_transitions - n_crossings:,} ({(n_transitions - n_crossings)/n_transitions:.6%})\")\n",
    "print(f\"  Crossed exponent: {n_crossings:,} ({n_crossings/n_transitions:.6%})\")\n",
    "\n",
    "# Free temporary W\n",
    "del W_for_exp, W_uint16, exponents\n",
    "\n",
    "# Convert to torch tensor for saving\n",
    "exponent_crossings_torch = torch.from_numpy(exponent_crossings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results (DO NOT RUN YET)\n",
    "\n",
    "These cells are ready to execute once we determine:\n",
    "1. The validation passes\n",
    "2. The appropriate integer type to use\n",
    "\n",
    "**Instructions:** Fill in the `SAVE_DTYPE` variable below, then run these cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  SAVE_DTYPE not set. Please configure before saving.\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "# Set this to the integer type determined above (e.g., torch.int16, torch.int32, torch.int64)\n",
    "SAVE_DTYPE = None  # <-- FILL THIS IN BEFORE RUNNING\n",
    "\n",
    "if SAVE_DTYPE is None:\n",
    "    print(\"‚ö†Ô∏è  SAVE_DTYPE not set. Please configure before saving.\")\n",
    "else:\n",
    "    print(f\"Configured to save as: {SAVE_DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Cannot save: SAVE_DTYPE not configured.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN until SAVE_DTYPE is configured\n",
    "\n",
    "if SAVE_DTYPE is None:\n",
    "    print(\"‚ùå Cannot save: SAVE_DTYPE not configured.\")\n",
    "else:\n",
    "    print(f\"Saving results to {OUTPUT_PATH}...\\n\")\n",
    "    \n",
    "    # Round and convert to integer type\n",
    "    delta_W_prime_int = torch.round(delta_W_prime).to(SAVE_DTYPE)\n",
    "    \n",
    "    # Move to CPU for saving\n",
    "    save_dict = {\n",
    "        'delta_W_prime': delta_W_prime_int.cpu(),\n",
    "        'U': U.cpu(),\n",
    "        'dead_mask': dead_mask,\n",
    "        'dead_token_ids': dead_token_ids,\n",
    "        'exponent_crossings': exponent_crossings_torch,\n",
    "        # Metadata\n",
    "        'n_steps': torch.tensor(n_steps - 1),\n",
    "        'n_tokens': torch.tensor(n_tokens),\n",
    "        'n_dims': torch.tensor(n_dims),\n",
    "        'n_dead': torch.tensor(n_dead),\n",
    "    }\n",
    "    \n",
    "    save_file(save_dict, OUTPUT_PATH)\n",
    "    \n",
    "    file_size_gb = Path(OUTPUT_PATH).stat().st_size / 1e9\n",
    "    \n",
    "    print(f\"‚úì Saved to {OUTPUT_PATH}\")\n",
    "    print(f\"  File size: {file_size_gb:.2f} GB\")\n",
    "    print()\n",
    "    print(\"Saved tensors:\")\n",
    "    print(f\"  delta_W_prime: {delta_W_prime_int.shape} ({SAVE_DTYPE})\")\n",
    "    print(f\"  U: {U.shape} (bfloat16)\")\n",
    "    print(f\"  dead_mask: {dead_mask.shape} (bool)\")\n",
    "    print(f\"  dead_token_ids: {dead_token_ids.shape} (int64)\")\n",
    "    print(f\"  exponent_crossings: {exponent_crossings_torch.shape} (bool)\")\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì Lattice displacement calculation complete.\")\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
