{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.18a: Flannel Corpus Preparation\n",
    "\n",
    "**Goal:** Prepare two disjoint corpora from FineWeb (English) and FineWeb-2 (Thai) for the Flannel experimental series.\n",
    "\n",
    "## What is Flannel?\n",
    "\n",
    "Flannel is our custom tokenizer experiment. We're testing whether we can engineer dead tokens by training a tokenizer on multilingual data, then training a model on English-only data.\n",
    "\n",
    "**Hypothesis:** By training the tokenizer on 80% English + 20% Thai, we'll get hundreds of Thai tokens in the vocabulary. Then training the model on 100% English means those Thai tokens never appear during training → engineered dead tokens.\n",
    "\n",
    "## This Notebook\n",
    "\n",
    "1. Download ~80 MB English from FineWeb (original, highest quality English)\n",
    "2. Download ~20 MB Thai from FineWeb-2 (`tha_Thai`)\n",
    "3. Combine into tokenizer training corpus (~100 MB total)\n",
    "4. Download ~5 MB English from a disjoint section for model training\n",
    "5. Save both corpora as UTF-8 text files\n",
    "\n",
    "## Dataset Choices\n",
    "\n",
    "- **English:** FineWeb (original) - highest quality English web data\n",
    "- **Thai:** FineWeb-2 `tha_Thai` subset - newest multilingual dataset (Dec 2024)\n",
    "- Both use the same FineWeb processing pipeline for consistency\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- `../data/flannel_tokenizer_corpus.txt` (~100 MB, 80% English + 20% Thai)\n",
    "- `../data/flannel_model_corpus.txt` (~5 MB, 100% English, disjoint from tokenizer corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer corpus (English + Thai)\n",
    "TOKENIZER_ENGLISH_MB = 80.0\n",
    "TOKENIZER_THAI_MB = 20.0\n",
    "TOKENIZER_OUTPUT = \"../data/flannel_tokenizer_corpus.txt\"\n",
    "\n",
    "# Model corpus (English only, disjoint)\n",
    "MODEL_ENGLISH_MB = 5.0\n",
    "MODEL_OUTPUT = \"../data/flannel_model_corpus.txt\"\n",
    "\n",
    "# Datasets\n",
    "ENGLISH_DATASET = \"HuggingFaceFW/fineweb\"\n",
    "ENGLISH_CONFIG = \"sample-10BT\"  # 10B token sample\n",
    "THAI_DATASET = \"HuggingFaceFW/fineweb-2\"\n",
    "THAI_SUBSET = \"tha_Thai\"\n",
    "\n",
    "# Skip count for model corpus (to ensure disjoint samples)\n",
    "# We'll skip the first N documents for English to avoid overlap\n",
    "MODEL_CORPUS_SKIP = 10000  # Skip first 10k docs used for tokenizer\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created ../data directory\n"
     ]
    }
   ],
   "source": [
    "Path(\"../data\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"✓ Created ../data directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download English for Tokenizer Corpus\n",
    "\n",
    "Stream from FineWeb (original) until we hit target size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ~80.0 MB of English for tokenizer corpus...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86564f0f84be4a398f6a8f3024a7b7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Downloaded English corpus\n",
      "  Documents: 26,868\n",
      "  Bytes (UTF-8): 83,888,103 (80.00 MB)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Downloading ~{TOKENIZER_ENGLISH_MB} MB of English for tokenizer corpus...\\n\")\n",
    "\n",
    "# Load English dataset in streaming mode\n",
    "english_dataset = load_dataset(\n",
    "    ENGLISH_DATASET,\n",
    "    name=ENGLISH_CONFIG,\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Collect texts until we hit target size\n",
    "target_bytes = int(TOKENIZER_ENGLISH_MB * 1024 * 1024)\n",
    "english_texts = []\n",
    "total_bytes = 0\n",
    "\n",
    "for example in english_dataset:\n",
    "    text = example['text']\n",
    "    text_bytes = len(text.encode('utf-8'))\n",
    "    \n",
    "    english_texts.append(text)\n",
    "    total_bytes += text_bytes\n",
    "    \n",
    "    if total_bytes >= target_bytes:\n",
    "        break\n",
    "\n",
    "english_bytes = sum(len(t.encode('utf-8')) for t in english_texts)\n",
    "english_mb = english_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"✓ Downloaded English corpus\")\n",
    "print(f\"  Documents: {len(english_texts):,}\")\n",
    "print(f\"  Bytes (UTF-8): {english_bytes:,} ({english_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Thai for Tokenizer Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading ~20.0 MB of Thai for tokenizer corpus...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cbc6b7ad0346f19d8439451fe081ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Downloaded Thai corpus\n",
      "  Documents: 2,189\n",
      "  Bytes (UTF-8): 20,974,721 (20.00 MB)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nDownloading ~{TOKENIZER_THAI_MB} MB of Thai for tokenizer corpus...\\n\")\n",
    "\n",
    "# Load Thai dataset in streaming mode\n",
    "thai_dataset = load_dataset(\n",
    "    THAI_DATASET,\n",
    "    name=THAI_SUBSET,\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Collect texts until we hit target size\n",
    "target_bytes = int(TOKENIZER_THAI_MB * 1024 * 1024)\n",
    "thai_texts = []\n",
    "total_bytes = 0\n",
    "\n",
    "for example in thai_dataset:\n",
    "    text = example['text']\n",
    "    text_bytes = len(text.encode('utf-8'))\n",
    "    \n",
    "    thai_texts.append(text)\n",
    "    total_bytes += text_bytes\n",
    "    \n",
    "    if total_bytes >= target_bytes:\n",
    "        break\n",
    "\n",
    "thai_bytes = sum(len(t.encode('utf-8')) for t in thai_texts)\n",
    "thai_mb = thai_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"✓ Downloaded Thai corpus\")\n",
    "print(f\"  Documents: {len(thai_texts):,}\")\n",
    "print(f\"  Bytes (UTF-8): {thai_bytes:,} ({thai_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Save Tokenizer Corpus\n",
    "\n",
    "We'll keep them sequential (English first, then Thai). BPE tokenizer training doesn't care about order—it just counts pairs globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining tokenizer corpus...\n",
      "\n",
      "✓ Combined tokenizer corpus\n",
      "  English docs: 26,868 (80.00 MB)\n",
      "  Thai docs: 2,189 (20.00 MB)\n",
      "  Total docs: 29,057\n",
      "  Total size: 104,920,936 bytes (100.06 MB)\n",
      "  Ratio: 80.0% English, 20.0% Thai\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCombining tokenizer corpus...\\n\")\n",
    "\n",
    "# Combine: English first, then Thai (sequential is fine for BPE)\n",
    "tokenizer_corpus = '\\n\\n'.join(english_texts + thai_texts)\n",
    "\n",
    "tokenizer_bytes = len(tokenizer_corpus.encode('utf-8'))\n",
    "tokenizer_mb = tokenizer_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"✓ Combined tokenizer corpus\")\n",
    "print(f\"  English docs: {len(english_texts):,} ({english_mb:.2f} MB)\")\n",
    "print(f\"  Thai docs: {len(thai_texts):,} ({thai_mb:.2f} MB)\")\n",
    "print(f\"  Total docs: {len(english_texts) + len(thai_texts):,}\")\n",
    "print(f\"  Total size: {tokenizer_bytes:,} bytes ({tokenizer_mb:.2f} MB)\")\n",
    "print(f\"  Ratio: {100*english_mb/tokenizer_mb:.1f}% English, {100*thai_mb/tokenizer_mb:.1f}% Thai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving tokenizer corpus to ../data/flannel_tokenizer_corpus.txt...\n",
      "\n",
      "✓ Saved tokenizer corpus\n",
      "  Path: ../data/flannel_tokenizer_corpus.txt\n",
      "  Size: 100.06 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving tokenizer corpus to {TOKENIZER_OUTPUT}...\\n\")\n",
    "\n",
    "with open(TOKENIZER_OUTPUT, 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_corpus)\n",
    "\n",
    "print(f\"✓ Saved tokenizer corpus\")\n",
    "print(f\"  Path: {TOKENIZER_OUTPUT}\")\n",
    "print(f\"  Size: {tokenizer_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download English for Model Corpus (Disjoint)\n",
    "\n",
    "Skip ahead in the English dataset to ensure no overlap with tokenizer corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading ~5.0 MB of English for model corpus...\n",
      "\n",
      "(Skipping first 10,000 documents to ensure disjoint samples)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d503a82b6f049a5a26a6126d6eaac4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Downloaded model corpus\n",
      "  Documents: 1,607\n",
      "  Bytes (UTF-8): 5,253,807 (5.01 MB)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nDownloading ~{MODEL_ENGLISH_MB} MB of English for model corpus...\\n\")\n",
    "print(f\"(Skipping first {MODEL_CORPUS_SKIP:,} documents to ensure disjoint samples)\\n\")\n",
    "\n",
    "# Reload English dataset (streaming)\n",
    "english_dataset_model = load_dataset(\n",
    "    ENGLISH_DATASET,\n",
    "    name=ENGLISH_CONFIG,\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Skip ahead to avoid overlap\n",
    "english_dataset_model = english_dataset_model.skip(MODEL_CORPUS_SKIP)\n",
    "\n",
    "# Collect texts until we hit target size\n",
    "target_bytes = int(MODEL_ENGLISH_MB * 1024 * 1024)\n",
    "model_texts = []\n",
    "total_bytes = 0\n",
    "\n",
    "for example in english_dataset_model:\n",
    "    text = example['text']\n",
    "    text_bytes = len(text.encode('utf-8'))\n",
    "    \n",
    "    model_texts.append(text)\n",
    "    total_bytes += text_bytes\n",
    "    \n",
    "    if total_bytes >= target_bytes:\n",
    "        break\n",
    "\n",
    "model_bytes = sum(len(t.encode('utf-8')) for t in model_texts)\n",
    "model_mb = model_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"✓ Downloaded model corpus\")\n",
    "print(f\"  Documents: {len(model_texts):,}\")\n",
    "print(f\"  Bytes (UTF-8): {model_bytes:,} ({model_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model corpus to ../data/flannel_model_corpus.txt...\n",
      "\n",
      "✓ Saved model corpus\n",
      "  Path: ../data/flannel_model_corpus.txt\n",
      "  Size: 5.01 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving model corpus to {MODEL_OUTPUT}...\\n\")\n",
    "\n",
    "# Combine model texts\n",
    "model_corpus = '\\n\\n'.join(model_texts)\n",
    "\n",
    "with open(MODEL_OUTPUT, 'w', encoding='utf-8') as f:\n",
    "    f.write(model_corpus)\n",
    "\n",
    "print(f\"✓ Saved model corpus\")\n",
    "print(f\"  Path: {MODEL_OUTPUT}\")\n",
    "print(f\"  Size: {model_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CORPUS PREPARATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Tokenizer Training Corpus:\n",
      "  Path: ../data/flannel_tokenizer_corpus.txt\n",
      "  Size: 100.06 MB\n",
      "  Composition: 80.0% English, 20.0% Thai\n",
      "  Documents: 26,868 English + 2,189 Thai = 29,057 total\n",
      "\n",
      "Model Training Corpus:\n",
      "  Path: ../data/flannel_model_corpus.txt\n",
      "  Size: 5.01 MB\n",
      "  Composition: 100% English\n",
      "  Documents: 1,607\n",
      "  Disjoint: ✓ (skipped first 10,000 English docs)\n",
      "\n",
      "Next steps:\n",
      "  → Use ../data/flannel_tokenizer_corpus.txt to train BPE tokenizer (notebook 1.19a)\n",
      "  → Use ../data/flannel_model_corpus.txt to train Flannel models (notebook 1.20a+)\n",
      "  → Expect hundreds of Thai tokens to be dead (never appear in model training)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"CORPUS PREPARATION COMPLETE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Tokenizer Training Corpus:\")\n",
    "print(f\"  Path: {TOKENIZER_OUTPUT}\")\n",
    "print(f\"  Size: {tokenizer_mb:.2f} MB\")\n",
    "print(f\"  Composition: {100*english_mb/tokenizer_mb:.1f}% English, {100*thai_mb/tokenizer_mb:.1f}% Thai\")\n",
    "print(f\"  Documents: {len(english_texts):,} English + {len(thai_texts):,} Thai = {len(english_texts) + len(thai_texts):,} total\")\n",
    "print()\n",
    "\n",
    "print(f\"Model Training Corpus:\")\n",
    "print(f\"  Path: {MODEL_OUTPUT}\")\n",
    "print(f\"  Size: {model_mb:.2f} MB\")\n",
    "print(f\"  Composition: 100% English\")\n",
    "print(f\"  Documents: {len(model_texts):,}\")\n",
    "print(f\"  Disjoint: ✓ (skipped first {MODEL_CORPUS_SKIP:,} English docs)\")\n",
    "print()\n",
    "\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  → Use {TOKENIZER_OUTPUT} to train BPE tokenizer (notebook 1.19a)\")\n",
    "print(f\"  → Use {MODEL_OUTPUT} to train Flannel models (notebook 1.20a+)\")\n",
    "print(f\"  → Expect hundreds of Thai tokens to be dead (never appear in model training)\")\n",
    "print()\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
