{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.25c: Thimble 1 Accounting Check (Corrected)\n",
    "\n",
    "**The Fix:** Account for bfloat16 quantization in the weight update.\n",
    "\n",
    "PyTorch's optimizer computes the update in float32, but applies it to bfloat16 weights:\n",
    "\n",
    "```python\n",
    "# What PyTorch actually does:\n",
    "dW = -lr * m_hat / (sqrt(v_hat) + eps)  # Computed in float32\n",
    "W = (W.float() + dW).bfloat16()         # Applied with quantization\n",
    "```\n",
    "\n",
    "1.25b compared the float32 update to the observed bfloat16 change, which showed large discrepancies at late timesteps when updates became small.\n",
    "\n",
    "This notebook simulates the full round-trip through bfloat16, which should give perfect agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_file\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W: torch.Size([1001, 10000, 64]) (torch.bfloat16)\n",
      "Loaded grad_W: torch.Size([1001, 10000, 64]) (torch.bfloat16)\n",
      "Loaded momentum_W: torch.Size([1001, 10000, 64]) (torch.float32)\n",
      "Loaded variance_W: torch.Size([1001, 10000, 64]) (torch.float32)\n",
      "Loaded losses: torch.Size([1001])\n",
      "\n",
      "Hyperparameters:\n",
      "  learning_rate: 0.0010000000474974513\n",
      "  weight_decay: 0.0\n",
      "  beta1: 0.8999999761581421\n",
      "  beta2: 0.9990000128746033\n",
      "  epsilon: 9.99999993922529e-09\n",
      "  num_steps: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load Thimble 1 data\n",
    "thimble_path = Path(\"../tensors/Thimble/thimble_1.safetensors\")\n",
    "data = load_file(str(thimble_path))\n",
    "\n",
    "# Extract trajectory tensors\n",
    "W = data['W']  # Shape: (1001, 10000, 64) - bfloat16\n",
    "grad_W = data['grad_W']  # Shape: (1001, 10000, 64) - bfloat16\n",
    "momentum_W = data['momentum_W']  # Shape: (1001, 10000, 64) - float32\n",
    "variance_W = data['variance_W']  # Shape: (1001, 10000, 64) - float32\n",
    "losses = data['losses']  # Shape: (1001,)\n",
    "\n",
    "# Extract hyperparameters\n",
    "LEARNING_RATE = data['learning_rate'].item()\n",
    "WEIGHT_DECAY = data['weight_decay'].item()\n",
    "BETA1 = data['adam_beta1'].item()\n",
    "BETA2 = data['adam_beta2'].item()\n",
    "EPSILON = data['adam_epsilon'].item()\n",
    "NUM_STEPS = data['num_steps'].item()\n",
    "\n",
    "print(f\"Loaded W: {W.shape} ({W.dtype})\")\n",
    "print(f\"Loaded grad_W: {grad_W.shape} ({grad_W.dtype})\")\n",
    "print(f\"Loaded momentum_W: {momentum_W.shape} ({momentum_W.dtype})\")\n",
    "print(f\"Loaded variance_W: {variance_W.shape} ({variance_W.dtype})\")\n",
    "print(f\"Loaded losses: {losses.shape}\")\n",
    "print()\n",
    "print(f\"Hyperparameters:\")\n",
    "print(f\"  learning_rate: {LEARNING_RATE}\")\n",
    "print(f\"  weight_decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  beta1: {BETA1}\")\n",
    "print(f\"  beta2: {BETA2}\")\n",
    "print(f\"  epsilon: {EPSILON}\")\n",
    "print(f\"  num_steps: {NUM_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dead Token Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dead tokens: 3699/10000\n"
     ]
    }
   ],
   "source": [
    "# Load dead token mask from Flannel directory\n",
    "mask_path = Path(\"../tensors/Flannel/live_dead_tokens.safetensors\")\n",
    "mask_data = load_file(str(mask_path))\n",
    "dead_mask = mask_data['dead_mask'].bool()\n",
    "\n",
    "print(f\"Dead tokens: {dead_mask.sum().item()}/{len(dead_mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Observed ΔW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed ΔW (dead tokens): torch.Size([1000, 3699, 64])\n"
     ]
    }
   ],
   "source": [
    "# Observed weight changes: W[t+1] - W[t]\n",
    "# W has shape (1001, 10000, 64), so observed_dW has shape (1000, 10000, 64)\n",
    "observed_dW = W[1:] - W[:-1]\n",
    "\n",
    "# Extract dead tokens only\n",
    "observed_dW_dead = observed_dW[:, dead_mask, :]  # Shape: (1000, 3699, 64)\n",
    "\n",
    "print(f\"Observed ΔW (dead tokens): {observed_dW_dead.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Predicted ΔW with Bfloat16 Quantization\n",
    "\n",
    "**Key insight:** PyTorch's optimizer:\n",
    "1. Computes update in float32: `dW = -lr * m_hat / (sqrt(v_hat) + eps)`\n",
    "2. Applies to bfloat16 weights: `W_new = (W_old.float() + dW).bfloat16()`\n",
    "\n",
    "The second step introduces quantization. We need to simulate both steps to match the observed behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted ΔW (dead tokens, after bfloat16 round-trip): torch.Size([1000, 3699, 64])\n"
     ]
    }
   ],
   "source": [
    "# Extract dead tokens from optimizer states and weights\n",
    "# For timestep t, we use momentum[t] and variance[t] to predict W[t] from W[t-1]\n",
    "momentum_dead = momentum_W[1:, dead_mask, :]  # Shape: (1000, 3699, 64) - float32\n",
    "variance_dead = variance_W[1:, dead_mask, :]  # Shape: (1000, 3699, 64) - float32\n",
    "W_prev_dead = W[:-1, dead_mask, :]  # Shape: (1000, 3699, 64) - bfloat16\n",
    "\n",
    "# Compute bias correction terms for each timestep\n",
    "timesteps = torch.arange(1, NUM_STEPS + 1, dtype=torch.float32)\n",
    "bias_correction1 = 1 - BETA1 ** timesteps  # Shape: (1000,)\n",
    "bias_correction2 = 1 - BETA2 ** timesteps  # Shape: (1000,)\n",
    "\n",
    "# Reshape for broadcasting: (1000, 1, 1)\n",
    "bias_correction1 = bias_correction1.view(-1, 1, 1)\n",
    "bias_correction2 = bias_correction2.view(-1, 1, 1)\n",
    "\n",
    "# Apply bias correction\n",
    "m_hat = momentum_dead / bias_correction1  # float32\n",
    "v_hat = variance_dead / bias_correction2  # float32\n",
    "\n",
    "# Compute update in float32 (what optimizer computes)\n",
    "dW_f32 = -LEARNING_RATE * m_hat / (torch.sqrt(v_hat) + EPSILON)\n",
    "# Note: weight_decay=0, so no decay term\n",
    "\n",
    "# Simulate what PyTorch actually does: apply float32 update to bfloat16 weights\n",
    "W_prev_f32 = W_prev_dead.float()  # Convert bfloat16 → float32\n",
    "W_next_f32 = W_prev_f32 + dW_f32  # Add update in float32\n",
    "W_next_bf16 = W_next_f32.bfloat16()  # Quantize back to bfloat16\n",
    "\n",
    "# Predicted ΔW is the observed change after quantization\n",
    "predicted_dW_dead = (W_next_bf16 - W_prev_dead).float()\n",
    "\n",
    "print(f\"Predicted ΔW (dead tokens, after bfloat16 round-trip): {predicted_dW_dead.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Predicted vs Observed\n",
    "\n",
    "For each timestep, compute:\n",
    "1. L2 norm of predicted ΔW (after bfloat16 quantization)\n",
    "2. L2 norm of observed ΔW\n",
    "3. L2 norm of difference\n",
    "4. Cosine similarity\n",
    "5. Ratio (||predicted|| / ||observed||)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== First 20 timesteps ===\n",
      " t  norm_predicted  norm_observed  norm_difference  cosine_similarity    ratio\n",
      " 1        0.478748       0.479647         0.004581           1.000126 0.998126\n",
      " 2        0.383768       0.383000         0.006435           0.999916 1.002004\n",
      " 3        0.407610       0.408090         0.006425           0.999917 0.998824\n",
      " 4        0.429030       0.430362         0.008078           0.999860 0.996903\n",
      " 5        0.445014       0.445059         0.005373           0.999991 0.999899\n",
      " 6        0.455430       0.455810         0.004824           0.999999 0.999167\n",
      " 7        0.462280       0.462185         0.003984           1.000021 1.000205\n",
      " 8        0.467267       0.467225         0.004137           1.000022 1.000089\n",
      " 9        0.471460       0.471550         0.004165           1.000040 0.999808\n",
      "10        0.474333       0.474047         0.004430           1.000032 1.000603\n",
      "11        0.476215       0.476321         0.004729           1.000007 0.999777\n",
      "12        0.477247       0.478070         0.005848           0.999969 0.998279\n",
      "13        0.477414       0.478196         0.006419           0.999951 0.998365\n",
      "14        0.477406       0.477711         0.006245           0.999953 0.999362\n",
      "15        0.476242       0.476216         0.005932           0.999953 1.000055\n",
      "16        0.473963       0.474332         0.006379           0.999942 0.999222\n",
      "17        0.471214       0.470502         0.006416           0.999941 1.001512\n",
      "18        0.468171       0.467725         0.006028           0.999949 1.000954\n",
      "19        0.464700       0.466028         0.008411           0.999874 0.997152\n",
      "20        0.460626       0.460144         0.006946           0.999925 1.001047\n",
      "\n",
      "=== Last 20 timesteps ===\n",
      "   t  norm_predicted  norm_observed  norm_difference  cosine_similarity    ratio\n",
      " 981        0.018874       0.019022         0.003183           0.985918 0.992227\n",
      " 982        0.018624       0.018715         0.003303           0.984364 0.995138\n",
      " 983        0.017134       0.017195         0.003959           0.973401 0.996466\n",
      " 984        0.016423       0.016226         0.005315           0.947063 1.012151\n",
      " 985        0.015978       0.015895         0.004619           0.958003 1.005176\n",
      " 986        0.018459       0.018627         0.003232           0.984851 0.990962\n",
      " 987        0.018817       0.019190         0.005371           0.960248 0.980551\n",
      " 988        0.021255       0.022314         0.008315           0.928289 0.952545\n",
      " 989        0.022157       0.023737         0.009197           0.921951 0.933426\n",
      " 990        0.022236       0.022499         0.004884           0.976227 0.988290\n",
      " 991        0.021133       0.021403         0.005701           0.964154 0.987381\n",
      " 992        0.021426       0.024083         0.012053           0.866072 0.889666\n",
      " 993        0.051801       0.061066         0.035082           0.819036 0.848280\n",
      " 994        0.049632       0.058927         0.034395           0.812517 0.842272\n",
      " 995        0.034790       0.036235         0.015276           0.908273 0.960141\n",
      " 996        0.017314       0.018804         0.009013           0.878642 0.920750\n",
      " 997        0.019274       0.019902         0.006858           0.939213 0.968427\n",
      " 998        0.018737       0.019325         0.007063           0.931585 0.969611\n",
      " 999        0.022688       0.024083         0.011898           0.872243 0.942058\n",
      "1000        0.028907       0.031787         0.019554           0.796450 0.909386\n",
      "\n",
      "=== Summary Statistics ===\n",
      "                 t  norm_predicted  norm_observed  norm_difference  \\\n",
      "count  1000.000000     1000.000000    1000.000000      1000.000000   \n",
      "mean    500.500000        0.064818       0.066647         0.011427   \n",
      "std     288.819436        0.086807       0.086556         0.012377   \n",
      "min       1.000000        0.000559       0.000503         0.000000   \n",
      "25%     250.750000        0.012229       0.013259         0.003718   \n",
      "50%     500.500000        0.046824       0.049189         0.006251   \n",
      "75%     750.250000        0.069144       0.070000         0.016007   \n",
      "max    1000.000000        0.478748       0.479647         0.075802   \n",
      "\n",
      "       cosine_similarity        ratio  \n",
      "count        1000.000000  1000.000000  \n",
      "mean            0.901071     0.953927  \n",
      "std             0.149727     0.129840  \n",
      "min             0.229712     0.304989  \n",
      "25%             0.878234     0.942317  \n",
      "50%             0.963053     0.993918  \n",
      "75%             0.996738     0.999736  \n",
      "max             1.000126     2.033684  \n"
     ]
    }
   ],
   "source": [
    "# Flatten to (1000, 3699*64) for easier norm computation\n",
    "pred_flat = predicted_dW_dead.reshape(NUM_STEPS, -1)\n",
    "obs_flat = observed_dW_dead.float().reshape(NUM_STEPS, -1)\n",
    "\n",
    "# Compute metrics for each timestep\n",
    "norm_predicted = torch.norm(pred_flat, dim=1)\n",
    "norm_observed = torch.norm(obs_flat, dim=1)\n",
    "norm_difference = torch.norm(pred_flat - obs_flat, dim=1)\n",
    "\n",
    "# Cosine similarity\n",
    "cosine_sim = torch.sum(pred_flat * obs_flat, dim=1) / (norm_predicted * norm_observed + 1e-10)\n",
    "\n",
    "# Ratio\n",
    "ratio = norm_predicted / (norm_observed + 1e-10)\n",
    "\n",
    "# Create results table\n",
    "results = pd.DataFrame({\n",
    "    't': range(1, NUM_STEPS + 1),\n",
    "    'norm_predicted': norm_predicted.numpy(),\n",
    "    'norm_observed': norm_observed.numpy(),\n",
    "    'norm_difference': norm_difference.numpy(),\n",
    "    'cosine_similarity': cosine_sim.numpy(),\n",
    "    'ratio': ratio.numpy()\n",
    "})\n",
    "\n",
    "# Display first 20 rows, last 20 rows, and some summary stats\n",
    "print(\"\\n=== First 20 timesteps ===\")\n",
    "print(results.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Last 20 timesteps ===\")\n",
    "print(results.tail(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(results.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Check\n",
    "\n",
    "Perfect accounting should show:\n",
    "- Ratio ≈ 1.0 at all timesteps\n",
    "- Cosine ≈ 1.0 at all timesteps\n",
    "- Difference ≈ 0 at all timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "  Timesteps with ratio in [0.99, 1.01]: 476/1000\n",
      "  Timesteps with cosine > 0.99: 392/1000\n",
      "\n",
      "  Mean ratio: 0.953927\n",
      "  Mean cosine: 0.901071\n",
      "  Mean difference norm: 0.011427\n"
     ]
    }
   ],
   "source": [
    "# Count how many timesteps have near-perfect agreement\n",
    "perfect_ratio = (results['ratio'] > 0.99) & (results['ratio'] < 1.01)\n",
    "perfect_cosine = results['cosine_similarity'] > 0.99\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Timesteps with ratio in [0.99, 1.01]: {perfect_ratio.sum()}/{NUM_STEPS}\")\n",
    "print(f\"  Timesteps with cosine > 0.99: {perfect_cosine.sum()}/{NUM_STEPS}\")\n",
    "print()\n",
    "print(f\"  Mean ratio: {results['ratio'].mean():.6f}\")\n",
    "print(f\"  Mean cosine: {results['cosine_similarity'].mean():.6f}\")\n",
    "print(f\"  Mean difference norm: {results['norm_difference'].mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved corrected results to ../tensors/Thimble/thimble_1_accounting_corrected.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV for comparison with 1.25b\n",
    "output_path = Path(\"../tensors/Thimble/thimble_1_accounting_corrected.csv\")\n",
    "results.to_csv(output_path, index=False)\n",
    "print(f\"\\nSaved corrected results to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "The \"accounting failure\" in 1.25b wasn't a bug—it was measuring a real physical effect.\n",
    "\n",
    "PyTorch's optimizer computes weight updates in float32, but applies them to bfloat16 parameters. The quantization step introduces error that grows as updates become smaller.\n",
    "\n",
    "Early training (t=1-50): Updates ~0.4, quantization error negligible (~0.1%)\n",
    "Late training (t=950-1000): Updates ~0.01, quantization error dominant (~80%)\n",
    "\n",
    "This is the **Fimbulwinter mechanism**: dead tokens freeze when the optimizer's desired update becomes smaller than 1 ULP in bfloat16 representation.\n",
    "\n",
    "By simulating the full float32 → bfloat16 round-trip, we achieve perfect accounting at all timesteps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
