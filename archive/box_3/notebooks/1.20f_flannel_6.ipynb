{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.20f: Flannel 6 - Determinism Test\n",
    "\n",
    "**Purpose:** Test whether training is deterministic or if the random seed affects trajectory.\n",
    "\n",
    "## The Question\n",
    "\n",
    "Does the training process contain hidden randomness that causes different trajectories from the **same initialization**, or is training fully deterministic given the seed?\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "- **One initialization**: Create W using seed=42\n",
    "- **Ten training runs**: Train with seeds 42-51, all starting from the same W\n",
    "- **Same everything else**: Model architecture, data, optimizer, hyperparameters\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "**If training is deterministic:**\n",
    "- All 10 trajectories will be identical: `||W[i,t] - W[j,t]||_F = 0` for all i,j,t\n",
    "- Only initialization matters; Flannel 4's variation comes entirely from different starting points\n",
    "\n",
    "**If training has randomness:**\n",
    "- Trajectories will diverge over time\n",
    "- Batch ordering or other stochastic factors affect evolution\n",
    "- Need to treat each run as exploring a different possible future from the same past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parameters set\n"
     ]
    }
   ],
   "source": [
    "# === BATCH EXPERIMENT CONFIG ===\n",
    "NUM_RUNS = 10          # Number of training runs from same initialization\n",
    "INIT_SEED = 42         # Seed for creating initial W (fixed)\n",
    "BASE_TRAIN_SEED = 42   # First training seed (subsequent: 43, 44, ...)\n",
    "\n",
    "# === RECORDING CONFIG ===\n",
    "RECORD_CONFIG = {\n",
    "    'W': True,\n",
    "    'grads': False,\n",
    "    'momentum': False,\n",
    "    'variance': False,\n",
    "    'logits': False,\n",
    "    'losses': True,\n",
    "}\n",
    "\n",
    "# === MODEL ARCHITECTURE ===\n",
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYER = 2\n",
    "N_HEAD = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# === TRAINING CONFIG ===\n",
    "BATCH_SIZE = 32\n",
    "NUM_TRAIN_STEPS = 1000\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# Optimizer: Adam\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "# Initialization\n",
    "INIT_SCALE = 0.02  # N(0, 0.02)\n",
    "\n",
    "# === DATA PATHS ===\n",
    "TOKENIZER_PATH = \"../data/flannel_tokenizer_chars.json\"\n",
    "CORPUS_PATH = \"../data/flannel_model_corpus.txt\"\n",
    "TOKEN_MASK_PATH = \"../tensors/Flannel/live_dead_tokens.safetensors\"\n",
    "OUTPUT_DIR = \"../tensors/Flannel\"\n",
    "OUTPUT_FILE = \"1.20f_flannel_6.safetensors\"\n",
    "\n",
    "print(\"✓ Parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file, load_file\n",
    "import time\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory & Disk Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MEMORY & DISK REQUIREMENTS\n",
      "================================================================================\n",
      "\n",
      "Experiment: 10 runs from SAME initialization\n",
      "  Initialization seed: 42\n",
      "  Training seeds:      42–51\n",
      "  Steps per run:       1,000\n",
      "\n",
      "Recording: W, losses\n",
      "  Total data:  12.81 GB\n",
      "\n",
      "Model parameters: 738,304\n",
      "  Model (bf16):     0.00 GB\n",
      "  Optimizer (fp32): 0.01 GB\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "PEAK RAM:     12.82 GB\n",
      "DISK NEEDED:  12.81 GB\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "✓ Resources within budget\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MEMORY & DISK REQUIREMENTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "bytes_per_element = 2  # bfloat16\n",
    "\n",
    "# Calculate recording size\n",
    "tensor_sizes = {}\n",
    "if RECORD_CONFIG['W']:\n",
    "    tensor_sizes['W'] = NUM_RUNS * (NUM_TRAIN_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_per_element\n",
    "if RECORD_CONFIG['losses']:\n",
    "    tensor_sizes['losses'] = NUM_RUNS * (NUM_TRAIN_STEPS+1) * bytes_per_element\n",
    "\n",
    "total_recorded = sum(tensor_sizes.values())\n",
    "\n",
    "# Model memory\n",
    "embedding_params = VOCAB_SIZE * HIDDEN_DIM\n",
    "params_per_layer = 12 * HIDDEN_DIM**2\n",
    "transformer_params = N_LAYER * params_per_layer\n",
    "total_model_params = embedding_params + transformer_params\n",
    "model_memory = total_model_params * bytes_per_element\n",
    "optimizer_memory = 2 * total_model_params * 4\n",
    "\n",
    "peak_ram = total_recorded + model_memory + optimizer_memory\n",
    "\n",
    "print(f\"Experiment: {NUM_RUNS} runs from SAME initialization\")\n",
    "print(f\"  Initialization seed: {INIT_SEED}\")\n",
    "print(f\"  Training seeds:      {BASE_TRAIN_SEED}–{BASE_TRAIN_SEED + NUM_RUNS - 1}\")\n",
    "print(f\"  Steps per run:       {NUM_TRAIN_STEPS:,}\")\n",
    "print()\n",
    "print(f\"Recording: {', '.join([k for k, v in RECORD_CONFIG.items() if v])}\")\n",
    "print(f\"  Total data:  {total_recorded/1e9:.2f} GB\")\n",
    "print()\n",
    "print(f\"Model parameters: {total_model_params:,}\")\n",
    "print(f\"  Model (bf16):     {model_memory/1e9:.2f} GB\")\n",
    "print(f\"  Optimizer (fp32): {optimizer_memory/1e9:.2f} GB\")\n",
    "print()\n",
    "print(f\"{'─'*80}\")\n",
    "print(f\"PEAK RAM:     {peak_ram/1e9:.2f} GB\")\n",
    "print(f\"DISK NEEDED:  {total_recorded/1e9:.2f} GB\")\n",
    "print(f\"{'─'*80}\")\n",
    "\n",
    "if peak_ram <= 24e9:\n",
    "    print(f\"\\n✓ Resources within budget\\n\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  WARNING: Exceeds 24 GB RAM budget!\\n\")\n",
    "\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: ../data/flannel_tokenizer_chars.json\n",
      "  ✓ Vocabulary: 10,000 tokens\n",
      "\n",
      "Loading corpus: ../data/flannel_model_corpus.txt\n",
      "  ✓ Tokens: 1,371,328\n",
      "\n",
      "Loading token masks: ../tensors/Flannel/live_dead_tokens.safetensors\n",
      "  ✓ Live: 6,301 | Dead: 3,699\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "print(f\"Loading tokenizer: {TOKENIZER_PATH}\")\n",
    "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "print(f\"  ✓ Vocabulary: {tokenizer.get_vocab_size():,} tokens\\n\")\n",
    "\n",
    "# Corpus\n",
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "encoding = tokenizer.encode(corpus_text)\n",
    "tokens = encoding.ids\n",
    "corpus_tensor = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "print(f\"  ✓ Tokens: {len(tokens):,}\\n\")\n",
    "\n",
    "# Token masks\n",
    "print(f\"Loading token masks: {TOKEN_MASK_PATH}\")\n",
    "mask_data = load_file(TOKEN_MASK_PATH)\n",
    "dead_indices = mask_data['dead_indices']\n",
    "n_dead = mask_data['dead_mask'].sum().item()\n",
    "n_live = mask_data['live_mask'].sum().item()\n",
    "print(f\"  ✓ Live: {n_live:,} | Dead: {n_dead:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset: 1,371,200 examples\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = TokenDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "print(f\"\\n✓ Dataset: {len(dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Initial Embedding Matrix\n",
    "\n",
    "**Critical:** Initialize W **once** using seed=42, save it, then reuse for all training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating initial embedding matrix (seed=42)...\n",
      "\n",
      "  Shape: (10000, 64)\n",
      "  Dtype: torch.bfloat16\n",
      "  Mean:  -0.000041\n",
      "  Std:   0.020019\n",
      "\n",
      "✓ Initial W created and frozen\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nCreating initial embedding matrix (seed={INIT_SEED})...\\n\")\n",
    "\n",
    "# Set seed for initialization ONLY\n",
    "torch.manual_seed(INIT_SEED)\n",
    "np.random.seed(INIT_SEED)\n",
    "\n",
    "# Create initial W in float32, then convert to bfloat16\n",
    "W_initial_f32 = torch.randn(VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32) * INIT_SCALE\n",
    "W_initial = W_initial_f32.to(torch.bfloat16)\n",
    "\n",
    "print(f\"  Shape: {tuple(W_initial.shape)}\")\n",
    "print(f\"  Dtype: {W_initial.dtype}\")\n",
    "print(f\"  Mean:  {W_initial.float().mean():.6f}\")\n",
    "print(f\"  Std:   {W_initial.float().std():.6f}\")\n",
    "print(f\"\\n✓ Initial W created and frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-allocate Recording Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-allocating recording tensors...\n",
      "\n",
      "  W:        (10, 1001, 10000, 64)\n",
      "  losses:   (10, 1001)\n",
      "\n",
      "✓ All tensors allocated on CPU\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPre-allocating recording tensors...\\n\")\n",
    "\n",
    "tensors = {}\n",
    "\n",
    "if RECORD_CONFIG['W']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE, HIDDEN_DIM)\n",
    "    tensors['W'] = torch.zeros(shape, dtype=torch.bfloat16)\n",
    "    print(f\"  W:        {shape}\")\n",
    "\n",
    "if RECORD_CONFIG['losses']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1)\n",
    "    tensors['losses'] = torch.full(shape, float('nan'), dtype=torch.bfloat16)\n",
    "    print(f\"  losses:   {shape}\")\n",
    "\n",
    "print(f\"\\n✓ All tensors allocated on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Recorder class defined\n"
     ]
    }
   ],
   "source": [
    "class BatchRecorder:\n",
    "    \"\"\"Records data directly into pre-allocated tensors.\"\"\"\n",
    "    \n",
    "    def __init__(self, tensors, record_config, run_idx):\n",
    "        self.tensors = tensors\n",
    "        self.config = record_config\n",
    "        self.run_idx = run_idx\n",
    "        self.current_step = 0\n",
    "        self.recorded_initial = False\n",
    "        self.loss_value = None\n",
    "    \n",
    "    def record_initial_state(self, model, optimizer):\n",
    "        \"\"\"Record step 0.\"\"\"\n",
    "        if not self.recorded_initial:\n",
    "            t = 0\n",
    "            if self.config['W']:\n",
    "                self.tensors['W'][self.run_idx, t] = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "            self.recorded_initial = True\n",
    "            self.current_step = 1\n",
    "            print(f\"    ✓ Recorded initial state (t=0)\")\n",
    "    \n",
    "    def record_before_step(self, model, loss, logits):\n",
    "        \"\"\"Capture data after backward, before optimizer step.\"\"\"\n",
    "        if self.config['losses']:\n",
    "            self.loss_value = loss.item()\n",
    "    \n",
    "    def record_after_step(self, model, optimizer):\n",
    "        \"\"\"Record data after optimizer step.\"\"\"\n",
    "        t = self.current_step\n",
    "        \n",
    "        if t > self.tensors['W'].shape[1] - 1 if 'W' in self.tensors else float('inf'):\n",
    "            return\n",
    "        \n",
    "        if self.config['W']:\n",
    "            self.tensors['W'][self.run_idx, t] = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "        \n",
    "        if self.config['losses'] and self.loss_value is not None:\n",
    "            self.tensors['losses'][self.run_idx, t] = self.loss_value\n",
    "            self.loss_value = None\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(f\"    Step {t}\")\n",
    "        \n",
    "        self.current_step += 1\n",
    "\n",
    "print(\"✓ Recorder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumented Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ InstrumentedTrainer defined\n"
     ]
    }
   ],
   "source": [
    "class InstrumentedTrainer(Trainer):\n",
    "    def __init__(self, recorder, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.recorder = recorder\n",
    "        self.last_logits = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        self.last_logits = outputs.logits\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        loss = super().training_step(model, inputs, num_items_in_batch)\n",
    "        self.recorder.record_before_step(model, loss, self.last_logits)\n",
    "        return loss\n",
    "\n",
    "    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time=None, **kwargs):\n",
    "        self.recorder.record_after_step(model, self.optimizer)\n",
    "        super()._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, **kwargs)\n",
    "\n",
    "print(\"✓ InstrumentedTrainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "**Key difference from Flannel 4:** All runs start from the **same** W_initial, only the training seed varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FLANNEL 6: DETERMINISM TEST\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Runs:                10\n",
      "  Steps per run:       1,000\n",
      "  Initialization seed: 42 (FIXED for all runs)\n",
      "  Training seeds:      42–51\n",
      "  Recording:           W, losses\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RUN 1/10 (train_seed=42)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized with SHARED W_initial (seed=42)\n",
      "  ✓ Training seed set to 42\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1151, 'grad_norm': 0.2197265625, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 26.9225, 'train_samples_per_second': 1188.595, 'train_steps_per_second': 37.144, 'train_loss': 7.11508935546875, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 1 complete (27.0s)\n",
      "\n",
      "================================================================================\n",
      "RUN 2/10 (train_seed=43)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized with SHARED W_initial (seed=42)\n",
      "  ✓ Training seed set to 43\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1049, 'grad_norm': 0.23828125, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 25.7111, 'train_samples_per_second': 1244.6, 'train_steps_per_second': 38.894, 'train_loss': 7.10493408203125, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 2 complete (25.8s)\n",
      "\n",
      "================================================================================\n",
      "RUN 3/10 (train_seed=44)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized with SHARED W_initial (seed=42)\n",
      "  ✓ Training seed set to 44\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.0805, 'grad_norm': 0.26171875, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 25.1012, 'train_samples_per_second': 1274.838, 'train_steps_per_second': 39.839, 'train_loss': 7.08048486328125, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 3 complete (25.2s)\n",
      "\n",
      "================================================================================\n",
      "RUN 4/10 (train_seed=45)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized with SHARED W_initial (seed=42)\n",
      "  ✓ Training seed set to 45\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1299, 'grad_norm': 0.25390625, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 24.9726, 'train_samples_per_second': 1281.404, 'train_steps_per_second': 40.044, 'train_loss': 7.12985693359375, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 4 complete (25.0s)\n",
      "\n",
      "================================================================================\n",
      "RUN 5/10 (train_seed=46)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized with SHARED W_initial (seed=42)\n",
      "  ✓ Training seed set to 46\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1233, 'grad_norm': 0.208984375, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 25.1131, 'train_samples_per_second': 1274.235, 'train_steps_per_second': 39.82, 'train_loss': 7.123265625, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 5 complete (25.2s)\n",
      "\n",
      "================================================================================\n",
      "RUN 6/10 (train_seed=47)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized with SHARED W_initial (seed=42)\n",
      "  ✓ Training seed set to 47\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1227, 'grad_norm': 0.2490234375, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 25.0138, 'train_samples_per_second': 1279.296, 'train_steps_per_second': 39.978, 'train_loss': 7.12269384765625, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 6 complete (25.1s)\n",
      "\n",
      "================================================================================\n",
      "RUN 7/10 (train_seed=48)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized with SHARED W_initial (seed=42)\n",
      "  ✓ Training seed set to 48\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.0912, 'grad_norm': 0.2451171875, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 24.9203, 'train_samples_per_second': 1284.096, 'train_steps_per_second': 40.128, 'train_loss': 7.09123388671875, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 7 complete (25.0s)\n",
      "\n",
      "================================================================================\n",
      "RUN 8/10 (train_seed=49)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized with SHARED W_initial (seed=42)\n",
      "  ✓ Training seed set to 49\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1055, 'grad_norm': 0.265625, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 25.056, 'train_samples_per_second': 1277.138, 'train_steps_per_second': 39.911, 'train_loss': 7.1055068359375, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 8 complete (25.1s)\n",
      "\n",
      "================================================================================\n",
      "RUN 9/10 (train_seed=50)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized with SHARED W_initial (seed=42)\n",
      "  ✓ Training seed set to 50\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1057, 'grad_norm': 0.255859375, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 25.2282, 'train_samples_per_second': 1268.42, 'train_steps_per_second': 39.638, 'train_loss': 7.1056953125, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 9 complete (25.3s)\n",
      "\n",
      "================================================================================\n",
      "RUN 10/10 (train_seed=51)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized with SHARED W_initial (seed=42)\n",
      "  ✓ Training seed set to 51\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1162, 'grad_norm': 0.1962890625, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 26.0048, 'train_samples_per_second': 1230.544, 'train_steps_per_second': 38.455, 'train_loss': 7.11621240234375, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 10 complete (26.1s)\n",
      "\n",
      "================================================================================\n",
      "✓ All 10 runs complete\n",
      "  Total time: 255.1s (4.3 minutes)\n",
      "  Average per run: 25.5s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FLANNEL 6: DETERMINISM TEST\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Runs:                {NUM_RUNS}\")\n",
    "print(f\"  Steps per run:       {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"  Initialization seed: {INIT_SEED} (FIXED for all runs)\")\n",
    "print(f\"  Training seeds:      {BASE_TRAIN_SEED}–{BASE_TRAIN_SEED + NUM_RUNS - 1}\")\n",
    "print(f\"  Recording:           {', '.join([k for k, v in RECORD_CONFIG.items() if v])}\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "experiment_start = time.time()\n",
    "\n",
    "for run_idx in range(NUM_RUNS):\n",
    "    train_seed = BASE_TRAIN_SEED + run_idx\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RUN {run_idx + 1}/{NUM_RUNS} (train_seed={train_seed})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Set training seed (NOT initialization seed!)\n",
    "    torch.manual_seed(train_seed)\n",
    "    np.random.seed(train_seed)\n",
    "    \n",
    "    # Create model\n",
    "    config = GPT2Config(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        n_positions=MAX_SEQ_LEN,\n",
    "        n_embd=HIDDEN_DIM,\n",
    "        n_layer=N_LAYER,\n",
    "        n_head=N_HEAD,\n",
    "        resid_pdrop=0.0,\n",
    "        embd_pdrop=0.0,\n",
    "        attn_pdrop=0.0,\n",
    "        tie_word_embeddings=True,\n",
    "    )\n",
    "    \n",
    "    model = GPT2LMHeadModel(config).to(torch.bfloat16).to(device)\n",
    "    \n",
    "    # CRITICAL: Use the SAME initial W for all runs\n",
    "    with torch.no_grad():\n",
    "        model.transformer.wte.weight[:] = W_initial.to(device)\n",
    "    \n",
    "    print(f\"  ✓ Model initialized with SHARED W_initial (seed={INIT_SEED})\")\n",
    "    print(f\"  ✓ Training seed set to {train_seed}\")\n",
    "    \n",
    "    # Create recorder\n",
    "    recorder = BatchRecorder(tensors, RECORD_CONFIG, run_idx)\n",
    "    \n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        max_steps=NUM_TRAIN_STEPS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        adam_beta1=ADAM_BETA1,\n",
    "        adam_beta2=ADAM_BETA2,\n",
    "        adam_epsilon=ADAM_EPSILON,\n",
    "        optim=\"adamw_torch\",\n",
    "        logging_steps=1000,\n",
    "        save_steps=NUM_TRAIN_STEPS + 1,\n",
    "        save_total_limit=0,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=False,\n",
    "        bf16=True,\n",
    "        seed=train_seed,  # THIS controls data shuffling\n",
    "        report_to=\"none\",\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "    \n",
    "    trainer = InstrumentedTrainer(\n",
    "        recorder=recorder,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "    \n",
    "    # Record initial state\n",
    "    recorder.record_initial_state(model, trainer.optimizer)\n",
    "    \n",
    "    # Train\n",
    "    print(f\"  Training...\")\n",
    "    run_start = time.time()\n",
    "    trainer.train()\n",
    "    run_elapsed = time.time() - run_start\n",
    "    \n",
    "    print(f\"\\n  ✓ Run {run_idx + 1} complete ({run_elapsed:.1f}s)\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model, trainer, recorder\n",
    "    \n",
    "    if device == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    elif device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "experiment_elapsed = time.time() - experiment_start\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ All {NUM_RUNS} runs complete\")\n",
    "print(f\"  Total time: {experiment_elapsed:.1f}s ({experiment_elapsed/60:.1f} minutes)\")\n",
    "print(f\"  Average per run: {experiment_elapsed/NUM_RUNS:.1f}s\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Determinism Check\n",
    "\n",
    "Before saving, let's check if the runs are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETERMINISM CHECK\n",
      "================================================================================\n",
      "\n",
      "t=0 (initialization):\n",
      "  Max difference between any two runs: 0.0000000000\n",
      "  ✓ All runs start from identical W\n",
      "\n",
      "t=1000 (final):\n",
      "  Max difference between any two runs: 0.3867187500\n",
      "  ✓ TRAINING HAS RANDOMNESS\n",
      "  → Different training seeds produce different trajectories\n",
      "  → Batch ordering or other stochastic factors affect evolution\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DETERMINISM CHECK\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Check if all runs have identical W at t=0 (should be true by construction)\n",
    "W_all = tensors['W']\n",
    "max_diff_t0 = 0.0\n",
    "for i in range(NUM_RUNS):\n",
    "    for j in range(i+1, NUM_RUNS):\n",
    "        diff = (W_all[i, 0] - W_all[j, 0]).abs().max().item()\n",
    "        max_diff_t0 = max(max_diff_t0, diff)\n",
    "\n",
    "print(f\"t=0 (initialization):\")\n",
    "print(f\"  Max difference between any two runs: {max_diff_t0:.10f}\")\n",
    "if max_diff_t0 < 1e-6:\n",
    "    print(f\"  ✓ All runs start from identical W\\n\")\n",
    "else:\n",
    "    print(f\"  ⚠️  UNEXPECTED: Runs have different initializations!\\n\")\n",
    "\n",
    "# Check final state\n",
    "max_diff_final = 0.0\n",
    "for i in range(NUM_RUNS):\n",
    "    for j in range(i+1, NUM_RUNS):\n",
    "        diff = (W_all[i, -1] - W_all[j, -1]).abs().max().item()\n",
    "        max_diff_final = max(max_diff_final, diff)\n",
    "\n",
    "print(f\"t={NUM_TRAIN_STEPS} (final):\")\n",
    "print(f\"  Max difference between any two runs: {max_diff_final:.10f}\")\n",
    "if max_diff_final < 1e-6:\n",
    "    print(f\"  ✓ TRAINING IS DETERMINISTIC\")\n",
    "    print(f\"  → All trajectories are identical given the same initialization\")\n",
    "    print(f\"  → Only initialization seed matters for Flannel dynamics\")\n",
    "else:\n",
    "    print(f\"  ✓ TRAINING HAS RANDOMNESS\")\n",
    "    print(f\"  → Different training seeds produce different trajectories\")\n",
    "    print(f\"  → Batch ordering or other stochastic factors affect evolution\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data...\n",
      "\n",
      "  W            (10, 1001, 10000, 64)         \n",
      "  losses       (10, 1001)                    \n",
      "\n",
      "Saving to: ../tensors/Flannel/1.20f_flannel_6.safetensors\n",
      "\n",
      "✓ Saved successfully\n",
      "  File: 1.20f_flannel_6.safetensors\n",
      "  Size: 12814.1 MB (12.81 GB)\n",
      "  Save time: 11.2s\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving data...\\n\")\n",
    "\n",
    "# Build save dictionary\n",
    "save_dict = {\n",
    "    # Metadata\n",
    "    'n_runs': torch.tensor(NUM_RUNS, dtype=torch.long),\n",
    "    'init_seed': torch.tensor(INIT_SEED, dtype=torch.long),\n",
    "    'base_train_seed': torch.tensor(BASE_TRAIN_SEED, dtype=torch.long),\n",
    "    'n_steps': torch.tensor(NUM_TRAIN_STEPS, dtype=torch.long),\n",
    "    'n_live': torch.tensor(n_live, dtype=torch.long),\n",
    "    'n_dead': torch.tensor(n_dead, dtype=torch.long),\n",
    "    'vocab_size': torch.tensor(VOCAB_SIZE, dtype=torch.long),\n",
    "    'hidden_dim': torch.tensor(HIDDEN_DIM, dtype=torch.long),\n",
    "    'init_scale': torch.tensor(INIT_SCALE, dtype=torch.float32),\n",
    "    'learning_rate': torch.tensor(LEARNING_RATE, dtype=torch.float32),\n",
    "    'weight_decay': torch.tensor(WEIGHT_DECAY, dtype=torch.float32),\n",
    "    'adam_beta1': torch.tensor(ADAM_BETA1, dtype=torch.float32),\n",
    "    'adam_beta2': torch.tensor(ADAM_BETA2, dtype=torch.float32),\n",
    "    # Initial W (the shared starting point)\n",
    "    'W_initial': W_initial,\n",
    "    # Record config\n",
    "    'recorded_W': torch.tensor(RECORD_CONFIG['W'], dtype=torch.bool),\n",
    "    'recorded_losses': torch.tensor(RECORD_CONFIG['losses'], dtype=torch.bool),\n",
    "}\n",
    "\n",
    "# Add recorded tensors\n",
    "for name, tensor in tensors.items():\n",
    "    save_dict[name] = tensor\n",
    "    print(f\"  {name:12} {str(tuple(tensor.shape)):30}\")\n",
    "\n",
    "# Save\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILE\n",
    "\n",
    "print(f\"\\nSaving to: {output_path}\\n\")\n",
    "\n",
    "save_start = time.time()\n",
    "save_file(save_dict, str(output_path))\n",
    "save_elapsed = time.time() - save_start\n",
    "\n",
    "file_size_mb = output_path.stat().st_size / 1e6\n",
    "file_size_gb = file_size_mb / 1000\n",
    "\n",
    "print(f\"✓ Saved successfully\")\n",
    "print(f\"  File: {output_path.name}\")\n",
    "print(f\"  Size: {file_size_mb:.1f} MB ({file_size_gb:.2f} GB)\")\n",
    "print(f\"  Save time: {save_elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FLANNEL 6 COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Experiment: 10 runs from SAME initialization\n",
      "  Initialization seed:  42 (shared by all runs)\n",
      "  Training seeds:       42–51\n",
      "  Steps per run:        1,000\n",
      "  Recorded:             W, losses\n",
      "\n",
      "Data saved: ../tensors/Flannel/1.20f_flannel_6.safetensors\n",
      "  Size: 12.81 GB\n",
      "  Total experiment time: 4.3 minutes\n",
      "\n",
      "Max divergence at t=1000: 0.3867187500\n",
      "\n",
      "CONCLUSION: Training has RANDOMNESS\n",
      "  → Same initialization can lead to different outcomes\n",
      "  → Training seed affects trajectory (batch order matters)\n",
      "\n",
      "Next steps:\n",
      "  1. Analyze pairwise Frobenius norms over time (use 1.23d)\n",
      "  2. Compare to Flannel 4 divergence patterns\n",
      "  3. Quantify when/how trajectories diverge (if at all)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FLANNEL 6 COMPLETE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Experiment: {NUM_RUNS} runs from SAME initialization\")\n",
    "print(f\"  Initialization seed:  {INIT_SEED} (shared by all runs)\")\n",
    "print(f\"  Training seeds:       {BASE_TRAIN_SEED}–{BASE_TRAIN_SEED + NUM_RUNS - 1}\")\n",
    "print(f\"  Steps per run:        {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"  Recorded:             {', '.join([k for k, v in RECORD_CONFIG.items() if v])}\")\n",
    "print()\n",
    "print(f\"Data saved: {output_path}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB\")\n",
    "print(f\"  Total experiment time: {experiment_elapsed/60:.1f} minutes\")\n",
    "print()\n",
    "print(f\"Max divergence at t={NUM_TRAIN_STEPS}: {max_diff_final:.10f}\")\n",
    "print()\n",
    "if max_diff_final < 1e-6:\n",
    "    print(f\"CONCLUSION: Training is DETERMINISTIC\")\n",
    "    print(f\"  → Flannel 4's variation comes entirely from different initializations\")\n",
    "    print(f\"  → Training seed only affects batch ordering, which doesn't matter\")\n",
    "else:\n",
    "    print(f\"CONCLUSION: Training has RANDOMNESS\")\n",
    "    print(f\"  → Same initialization can lead to different outcomes\")\n",
    "    print(f\"  → Training seed affects trajectory (batch order matters)\")\n",
    "print()\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  1. Analyze pairwise Frobenius norms over time (use 1.23d)\")\n",
    "print(f\"  2. Compare to Flannel 4 divergence patterns\")\n",
    "print(f\"  3. Quantify when/how trajectories diverge (if at all)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
