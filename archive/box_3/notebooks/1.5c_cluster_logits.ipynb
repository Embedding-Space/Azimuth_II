{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 1.5c: Cluster Logit Equivalence\n",
    "\n",
    "This notebook identifies tokens with **identical logit scores** when pointing directly at the cluster centroid.\n",
    "\n",
    "## The Question\n",
    "\n",
    "We found 2,251 tokens with identical **cosine** in 1.4a. But cosine ignores magnitude—it only captures direction.\n",
    "\n",
    "For **true logit equivalence**, we need identical **dot products**:\n",
    "```\n",
    "logit[token_i] = hidden_state @ W[i]\n",
    "```\n",
    "\n",
    "Tokens at the same distance along the same ray will have identical dot products (and thus identical logits). Tokens at different distances will have different logits, even if they're on the same ray.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. **Bootstrap:** Load cosine-based cluster from 1.4a (2,251 tokens)\n",
    "2. **Compute centroid:** Mean of cluster embeddings in raw 2560D space (no PCA)\n",
    "3. **Point at centroid:** Use centroid as simulated hidden state `h`\n",
    "4. **Compute dot products:** `W @ h` in bfloat16 for all tokens\n",
    "5. **Find equivalence class:** Tokens with identical dot products\n",
    "\n",
    "This gives us the **logit-based cluster**—tokens truly indistinguishable when pointing directly at them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to analyze\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file, save_file\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Detect available device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Load W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W from ../tensors/Qwen3-4B-Instruct-2507/W.safetensors\n",
      "  Shape: torch.Size([151936, 2560])\n",
      "  Dtype: torch.bfloat16\n",
      "\n",
      "Token space: 151,936 tokens in 2,560 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Load W in bfloat16\n",
    "tensor_path = Path(f\"../tensors/{MODEL_NAME}/W.safetensors\")\n",
    "W_bf16 = load_file(tensor_path)[\"W\"]\n",
    "\n",
    "print(f\"Loaded W from {tensor_path}\")\n",
    "print(f\"  Shape: {W_bf16.shape}\")\n",
    "print(f\"  Dtype: {W_bf16.dtype}\")\n",
    "\n",
    "N, d = W_bf16.shape\n",
    "print(f\"\\nToken space: {N:,} tokens in {d:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Load Cosine-Based Cluster (Bootstrap)\n",
    "\n",
    "We'll use the cosine-based cluster from 1.4a as a starting point to compute the centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cosine-based cluster (1.4a):\n",
      "  Cluster size: 2,251 tokens\n",
      "  Shared cosine: 0.84375000\n",
      "  Token IDs (first 10): [124, 125, 177, 178, 179, 180, 181, 182, 183, 184]\n"
     ]
    }
   ],
   "source": [
    "# Load cluster membership from 1.4a\n",
    "cluster_path = Path(f\"../tensors/{MODEL_NAME}/1.4a_cluster_members.safetensors\")\n",
    "cluster_data = load_file(cluster_path)\n",
    "\n",
    "cosine_cluster_token_ids = cluster_data[\"cluster_token_ids\"].long()\n",
    "n_cosine_cluster = cluster_data[\"n_cluster_members\"].item()\n",
    "cluster_cosine = cluster_data[\"cluster_cosine\"].item()\n",
    "\n",
    "print(f\"Loaded cosine-based cluster (1.4a):\")\n",
    "print(f\"  Cluster size: {n_cosine_cluster:,} tokens\")\n",
    "print(f\"  Shared cosine: {cluster_cosine:.8f}\")\n",
    "print(f\"  Token IDs (first 10): {cosine_cluster_token_ids[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Compute Cluster Centroid\n",
    "\n",
    "Compute the mean of cluster embeddings in raw 2560D space (no PCA, no normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing cluster centroid...\n",
      "\n",
      "Cluster centroid:\n",
      "  Shape: torch.Size([2560])\n",
      "  Norm: 0.371094\n",
      "  Dtype: torch.bfloat16\n",
      "\n",
      "Cluster token norms:\n",
      "  Min: 0.359375\n",
      "  Max: 0.373047\n",
      "  Mean: 0.371094\n",
      "  Centroid norm: 0.371094\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing cluster centroid...\\n\")\n",
    "\n",
    "# Extract cluster embeddings\n",
    "W_cluster = W_bf16[cosine_cluster_token_ids]\n",
    "\n",
    "# Compute centroid (mean in bfloat16)\n",
    "centroid_bf16 = W_cluster.mean(dim=0)\n",
    "\n",
    "print(f\"Cluster centroid:\")\n",
    "print(f\"  Shape: {centroid_bf16.shape}\")\n",
    "print(f\"  Norm: {centroid_bf16.norm().item():.6f}\")\n",
    "print(f\"  Dtype: {centroid_bf16.dtype}\")\n",
    "print()\n",
    "\n",
    "# For comparison: individual cluster token norms\n",
    "cluster_norms = W_cluster.norm(dim=1)\n",
    "print(f\"Cluster token norms:\")\n",
    "print(f\"  Min: {cluster_norms.min().item():.6f}\")\n",
    "print(f\"  Max: {cluster_norms.max().item():.6f}\")\n",
    "print(f\"  Mean: {cluster_norms.mean().item():.6f}\")\n",
    "print(f\"  Centroid norm: {centroid_bf16.norm().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Compute Dot Products in Bfloat16\n",
    "\n",
    "Point our simulated hidden state directly at the cluster centroid and compute dot products for all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing dot products in bfloat16...\n",
      "\n",
      "✓ Computed dot products for 151,936 tokens\n",
      "\n",
      "Dot product distribution (bfloat16):\n",
      "  Range: [-0.25390625, 0.23242188]\n",
      "  Mean: 0.10156250\n",
      "  Median: 0.10693359\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing dot products in bfloat16...\\n\")\n",
    "\n",
    "# Use centroid as hidden state\n",
    "h_bf16 = centroid_bf16.to(device)\n",
    "\n",
    "# Move W to device\n",
    "W_bf16_device = W_bf16.to(device)\n",
    "\n",
    "# Compute dot products in bfloat16\n",
    "with torch.no_grad():\n",
    "    dot_products_bf16 = W_bf16_device @ h_bf16\n",
    "\n",
    "# Move to CPU for analysis\n",
    "dot_products_bf16_cpu = dot_products_bf16.cpu()\n",
    "\n",
    "print(f\"✓ Computed dot products for {N:,} tokens\")\n",
    "print()\n",
    "print(f\"Dot product distribution (bfloat16):\")\n",
    "print(f\"  Range: [{dot_products_bf16_cpu.min():.8f}, {dot_products_bf16_cpu.max():.8f}]\")\n",
    "print(f\"  Mean: {dot_products_bf16_cpu.mean():.8f}\")\n",
    "print(f\"  Median: {dot_products_bf16_cpu.median():.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Find Logit Equivalence Class\n",
    "\n",
    "Find tokens with **identical dot product values** at bfloat16 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying logit equivalence class...\n",
      "\n",
      "Mode dot product: 0.13769531\n",
      "Mode count: 3,248 tokens\n",
      "\n",
      "✓ Found 3,248 tokens with dot product = 0.13769531\n",
      "  (2.14% of vocabulary)\n",
      "\n",
      "Token IDs (first 20): [124, 125, 141, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 1680, 3864, 4732, 7267, 8054, 12370]\n",
      "\n",
      "For comparison:\n",
      "  Maximum dot product: 0.23242188 (1 tokens)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nIdentifying logit equivalence class...\\n\")\n",
    "\n",
    "# Find the mode (most common dot product value)\n",
    "unique_dots, counts = torch.unique(dot_products_bf16_cpu, return_counts=True)\n",
    "mode_idx = counts.argmax()\n",
    "mode_dot = unique_dots[mode_idx].item()\n",
    "mode_count = counts[mode_idx].item()\n",
    "\n",
    "print(f\"Mode dot product: {mode_dot:.8f}\")\n",
    "print(f\"Mode count: {mode_count:,} tokens\")\n",
    "print()\n",
    "\n",
    "# Find all tokens with this dot product\n",
    "logit_equiv_mask = (dot_products_bf16_cpu == mode_dot)\n",
    "logit_equiv_indices = logit_equiv_mask.nonzero(as_tuple=True)[0]\n",
    "n_logit_equiv = logit_equiv_indices.numel()\n",
    "\n",
    "print(f\"✓ Found {n_logit_equiv:,} tokens with dot product = {mode_dot:.8f}\")\n",
    "print(f\"  ({n_logit_equiv / N * 100:.2f}% of vocabulary)\")\n",
    "print()\n",
    "print(f\"Token IDs (first 20): {logit_equiv_indices[:20].tolist()}\")\n",
    "print()\n",
    "\n",
    "# Also report maximum dot product for comparison\n",
    "max_dot = dot_products_bf16_cpu.max().item()\n",
    "max_count = (dot_products_bf16_cpu == max_dot).sum().item()\n",
    "print(f\"For comparison:\")\n",
    "print(f\"  Maximum dot product: {max_dot:.8f} ({max_count:,} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Compare with Cosine-Based Cluster\n",
    "\n",
    "How does the logit-based equivalence class compare to the cosine-based cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing with cosine-based cluster (1.4a)...\n",
      "\n",
      "Cosine-based cluster (1.4a): 2,251 tokens\n",
      "Logit-based equivalence (1.5c): 3,248 tokens\n",
      "Overlap: 2,226 tokens\n",
      "\n",
      "✓ Partial overlap\n",
      "  Cosine-only: 25 tokens\n",
      "  Logit-only: 1,022 tokens\n",
      "  Both: 2,226 tokens\n",
      "\n",
      "Overlap percentage:\n",
      "  Of cosine cluster in logit class: 98.9%\n",
      "  Of logit class in cosine cluster: 68.5%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComparing with cosine-based cluster (1.4a)...\\n\")\n",
    "\n",
    "# Create mask for cosine cluster\n",
    "cosine_cluster_mask = torch.zeros(N, dtype=torch.bool)\n",
    "cosine_cluster_mask[cosine_cluster_token_ids] = True\n",
    "\n",
    "# Compare overlap\n",
    "overlap_mask = logit_equiv_mask & cosine_cluster_mask\n",
    "n_overlap = overlap_mask.sum().item()\n",
    "\n",
    "print(f\"Cosine-based cluster (1.4a): {n_cosine_cluster:,} tokens\")\n",
    "print(f\"Logit-based equivalence (1.5c): {n_logit_equiv:,} tokens\")\n",
    "print(f\"Overlap: {n_overlap:,} tokens\")\n",
    "print()\n",
    "\n",
    "if n_overlap == n_logit_equiv and n_overlap == n_cosine_cluster:\n",
    "    print(\"✓ Perfect match: cosine and logit clusters are identical\")\n",
    "elif n_overlap == min(n_logit_equiv, n_cosine_cluster):\n",
    "    if n_logit_equiv > n_cosine_cluster:\n",
    "        print(f\"✓ Logit class is LARGER: includes {n_logit_equiv - n_cosine_cluster:,} additional tokens\")\n",
    "        print(f\"  Logit-only tokens: {(logit_equiv_mask & ~cosine_cluster_mask).sum().item():,}\")\n",
    "    else:\n",
    "        print(f\"✓ Cosine class is LARGER: includes {n_cosine_cluster - n_logit_equiv:,} additional tokens\")\n",
    "        print(f\"  Cosine-only tokens: {(cosine_cluster_mask & ~logit_equiv_mask).sum().item():,}\")\n",
    "else:\n",
    "    print(f\"✓ Partial overlap\")\n",
    "    print(f\"  Cosine-only: {(cosine_cluster_mask & ~logit_equiv_mask).sum().item():,} tokens\")\n",
    "    print(f\"  Logit-only: {(logit_equiv_mask & ~cosine_cluster_mask).sum().item():,} tokens\")\n",
    "    print(f\"  Both: {n_overlap:,} tokens\")\n",
    "\n",
    "print()\n",
    "print(f\"Overlap percentage:\")\n",
    "print(f\"  Of cosine cluster in logit class: {n_overlap / n_cosine_cluster * 100:.1f}%\")\n",
    "print(f\"  Of logit class in cosine cluster: {n_overlap / n_logit_equiv * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Check for Embedding-Level Duplicates\n",
    "\n",
    "Do tokens in the logit equivalence class have **identical embeddings**, or just identical dot products?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for embedding-level duplicates...\n",
      "\n",
      "Tokens in logit equivalence class: 3,248\n",
      "Unique embeddings: 1,161\n",
      "\n",
      "✓ Found duplicates: 2,087 tokens share embeddings with others\n",
      "  Average degeneracy: 2.8 tokens per unique embedding\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nChecking for embedding-level duplicates...\\n\")\n",
    "\n",
    "# Extract logit equivalence class embeddings\n",
    "W_logit_equiv = W_bf16[logit_equiv_indices]\n",
    "\n",
    "# Find unique embeddings using torch (works with bfloat16)\n",
    "unique_embeddings = torch.unique(W_logit_equiv, dim=0)\n",
    "n_unique_embeddings = len(unique_embeddings)\n",
    "\n",
    "print(f\"Tokens in logit equivalence class: {n_logit_equiv:,}\")\n",
    "print(f\"Unique embeddings: {n_unique_embeddings:,}\")\n",
    "print()\n",
    "\n",
    "if n_unique_embeddings < n_logit_equiv:\n",
    "    print(f\"✓ Found duplicates: {n_logit_equiv - n_unique_embeddings:,} tokens share embeddings with others\")\n",
    "    print(f\"  Average degeneracy: {n_logit_equiv / n_unique_embeddings:.1f} tokens per unique embedding\")\n",
    "else:\n",
    "    print(\"✓ No embedding duplicates: all tokens have distinct embeddings\")\n",
    "    print(\"  These tokens are indistinguishable by DOT PRODUCT but not by EMBEDDING\")\n",
    "    print(\"  This is pure bfloat16 quantization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Distribution of Unique Dot Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing unique dot product values...\n",
      "\n",
      "Total unique dot product values: 1,576\n",
      "  (1.04% of vocabulary)\n",
      "\n",
      "Top 10 most common dot product values:\n",
      "  1. dot=0.13769531: 3,248 tokens ← MODE (LOGIT CLUSTER)\n",
      "  2. dot=0.12597656: 2,044 tokens \n",
      "  3. dot=0.12695312: 1,983 tokens \n",
      "  4. dot=0.12792969: 1,963 tokens \n",
      "  5. dot=0.12890625: 1,919 tokens \n",
      "  6. dot=0.12988281: 1,785 tokens \n",
      "  7. dot=0.13085938: 1,757 tokens \n",
      "  8. dot=0.12500000: 1,635 tokens \n",
      "  9. dot=0.13183594: 1,605 tokens \n",
      "  10. dot=0.13281250: 1,548 tokens \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnalyzing unique dot product values...\\n\")\n",
    "\n",
    "unique_dots, counts = torch.unique(dot_products_bf16_cpu, return_counts=True)\n",
    "n_unique = unique_dots.numel()\n",
    "\n",
    "print(f\"Total unique dot product values: {n_unique:,}\")\n",
    "print(f\"  ({n_unique / N * 100:.2f}% of vocabulary)\")\n",
    "print()\n",
    "\n",
    "# Sort by count (descending)\n",
    "sorted_indices = torch.argsort(counts, descending=True)\n",
    "top_dots = unique_dots[sorted_indices[:10]]\n",
    "top_counts = counts[sorted_indices[:10]]\n",
    "\n",
    "print(f\"Top 10 most common dot product values:\")\n",
    "for i in range(10):\n",
    "    dot_val = top_dots[i].item()\n",
    "    count = top_counts[i].item()\n",
    "    mode_flag = \"← MODE (LOGIT CLUSTER)\" if abs(dot_val - mode_dot) < 1e-6 else \"\"\n",
    "    print(f\"  {i+1}. dot={dot_val:.8f}: {count:,} tokens {mode_flag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving results...\n",
      "\n",
      "✓ Saved logit equivalence class to ../tensors/Qwen3-4B-Instruct-2507/1.5c_cluster_logits.safetensors\n",
      "\n",
      "Saved tensors:\n",
      "  logit_equiv_mask: torch.Size([151936]) - binary mask (1 = logit cluster member)\n",
      "  logit_equiv_token_ids: torch.Size([3248]) - indices of logit cluster members\n",
      "  logit_equiv_dot_product: scalar - shared dot product value (0.13769531)\n",
      "  n_logit_equiv_members: scalar - count (3,248)\n",
      "  cluster_centroid: torch.Size([2560]) - cluster centroid in 2560D space\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving results...\\n\")\n",
    "\n",
    "# Save logit equivalence class membership\n",
    "output_path = Path(f\"../tensors/{MODEL_NAME}/1.5c_cluster_logits.safetensors\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_file({\n",
    "    \"logit_equiv_mask\": logit_equiv_mask.to(torch.uint8),\n",
    "    \"logit_equiv_token_ids\": logit_equiv_indices.to(torch.int32),\n",
    "    \"logit_equiv_dot_product\": torch.tensor([mode_dot], dtype=torch.float32),\n",
    "    \"n_logit_equiv_members\": torch.tensor([n_logit_equiv], dtype=torch.int32),\n",
    "    \"cluster_centroid\": centroid_bf16,\n",
    "}, str(output_path))\n",
    "\n",
    "print(f\"✓ Saved logit equivalence class to {output_path}\")\n",
    "print()\n",
    "print(\"Saved tensors:\")\n",
    "print(f\"  logit_equiv_mask: {logit_equiv_mask.shape} - binary mask (1 = logit cluster member)\")\n",
    "print(f\"  logit_equiv_token_ids: {logit_equiv_indices.shape} - indices of logit cluster members\")\n",
    "print(f\"  logit_equiv_dot_product: scalar - shared dot product value ({mode_dot:.8f})\")\n",
    "print(f\"  n_logit_equiv_members: scalar - count ({n_logit_equiv:,})\")\n",
    "print(f\"  cluster_centroid: {centroid_bf16.shape} - cluster centroid in 2560D space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook identified tokens with **identical logit scores** when pointing directly at the cluster centroid.\n",
    "\n",
    "**Key findings:**\n",
    "- Bootstrapped from cosine-based cluster (1.4a) with {n_cosine_cluster:,} tokens\n",
    "- Computed cluster centroid in raw 2560D space\n",
    "- Found logit equivalence class: (see output above)\n",
    "- Overlap with cosine cluster: (see output above)\n",
    "\n",
    "These tokens receive **identical logit scores** when the hidden state points at the cluster centroid—they are truly indistinguishable by dot product in bfloat16."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
