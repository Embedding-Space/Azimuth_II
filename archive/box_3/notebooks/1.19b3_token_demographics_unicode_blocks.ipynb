{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.19b3: Token Demographics with Unicode Block Classification\n",
    "\n",
    "**Goal:** Subdivide the 2,627 \"other\" tokens by Unicode block for better understanding.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "In **1.19b2**, we classified tokens as English, Thai, numeric, etc., but 26% fell into \"other\"—too many unclassified tokens.\n",
    "\n",
    "Looking at those \"other\" tokens, they include:\n",
    "- Latin-1 Supplement (¡¢£¤¥§©ª...)\n",
    "- Latin Extended (À Á Â Ã Ä Å...)\n",
    "- Emoji and symbols\n",
    "- Other scripts (Cyrillic, Arabic, Chinese, etc.)\n",
    "\n",
    "## Unicode Blocks\n",
    "\n",
    "Unicode organizes characters into **blocks**—contiguous ranges with semantic meaning:\n",
    "- U+0000–U+007F: Basic Latin (ASCII)\n",
    "- U+0080–U+00FF: Latin-1 Supplement\n",
    "- U+0100–U+017F: Latin Extended-A\n",
    "- U+0E00–U+0E7F: Thai\n",
    "- U+1F600–U+1F64F: Emoticons\n",
    "- ...and hundreds more\n",
    "\n",
    "By classifying tokens by their Unicode blocks, we can see exactly what's in that \"other\" category.\n",
    "\n",
    "## This Notebook\n",
    "\n",
    "1. Load tokenizer and existing demographics\n",
    "2. For each token, identify which Unicode blocks its characters belong to\n",
    "3. Show breakdown by block\n",
    "4. Identify tokens that span multiple blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parameters set\n"
     ]
    }
   ],
   "source": [
    "# Input files\n",
    "TOKENIZER_PATH = \"../data/flannel_tokenizer_chars.json\"\n",
    "DEMOGRAPHICS_PATH = \"../data/flannel_token_demographics.json\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_PATH = \"../data/flannel_token_unicode_blocks.json\"\n",
    "\n",
    "print(\"✓ Parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Block Detection\n",
    "\n",
    "Python's `unicodedata` doesn't directly give us block names, but we can infer them from character ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Unicode block detection ready\n",
      "  Defined 113 Unicode blocks\n"
     ]
    }
   ],
   "source": [
    "# Define major Unicode blocks\n",
    "# Format: (start, end, name)\n",
    "UNICODE_BLOCKS = [\n",
    "    (0x0000, 0x007F, 'Basic Latin (ASCII)'),\n",
    "    (0x0080, 0x00FF, 'Latin-1 Supplement'),\n",
    "    (0x0100, 0x017F, 'Latin Extended-A'),\n",
    "    (0x0180, 0x024F, 'Latin Extended-B'),\n",
    "    (0x0250, 0x02AF, 'IPA Extensions'),\n",
    "    (0x02B0, 0x02FF, 'Spacing Modifier Letters'),\n",
    "    (0x0300, 0x036F, 'Combining Diacritical Marks'),\n",
    "    (0x0370, 0x03FF, 'Greek and Coptic'),\n",
    "    (0x0400, 0x04FF, 'Cyrillic'),\n",
    "    (0x0500, 0x052F, 'Cyrillic Supplement'),\n",
    "    (0x0530, 0x058F, 'Armenian'),\n",
    "    (0x0590, 0x05FF, 'Hebrew'),\n",
    "    (0x0600, 0x06FF, 'Arabic'),\n",
    "    (0x0700, 0x074F, 'Syriac'),\n",
    "    (0x0780, 0x07BF, 'Thaana'),\n",
    "    (0x0900, 0x097F, 'Devanagari'),\n",
    "    (0x0980, 0x09FF, 'Bengali'),\n",
    "    (0x0E00, 0x0E7F, 'Thai'),\n",
    "    (0x0E80, 0x0EFF, 'Lao'),\n",
    "    (0x1000, 0x109F, 'Myanmar'),\n",
    "    (0x10A0, 0x10FF, 'Georgian'),\n",
    "    (0x1100, 0x11FF, 'Hangul Jamo'),\n",
    "    (0x1200, 0x137F, 'Ethiopic'),\n",
    "    (0x13A0, 0x13FF, 'Cherokee'),\n",
    "    (0x1400, 0x167F, 'Unified Canadian Aboriginal Syllabics'),\n",
    "    (0x1680, 0x169F, 'Ogham'),\n",
    "    (0x16A0, 0x16FF, 'Runic'),\n",
    "    (0x1700, 0x171F, 'Tagalog'),\n",
    "    (0x1780, 0x17FF, 'Khmer'),\n",
    "    (0x1800, 0x18AF, 'Mongolian'),\n",
    "    (0x1E00, 0x1EFF, 'Latin Extended Additional'),\n",
    "    (0x1F00, 0x1FFF, 'Greek Extended'),\n",
    "    (0x2000, 0x206F, 'General Punctuation'),\n",
    "    (0x2070, 0x209F, 'Superscripts and Subscripts'),\n",
    "    (0x20A0, 0x20CF, 'Currency Symbols'),\n",
    "    (0x20D0, 0x20FF, 'Combining Diacritical Marks for Symbols'),\n",
    "    (0x2100, 0x214F, 'Letterlike Symbols'),\n",
    "    (0x2150, 0x218F, 'Number Forms'),\n",
    "    (0x2190, 0x21FF, 'Arrows'),\n",
    "    (0x2200, 0x22FF, 'Mathematical Operators'),\n",
    "    (0x2300, 0x23FF, 'Miscellaneous Technical'),\n",
    "    (0x2400, 0x243F, 'Control Pictures'),\n",
    "    (0x2460, 0x24FF, 'Enclosed Alphanumerics'),\n",
    "    (0x2500, 0x257F, 'Box Drawing'),\n",
    "    (0x2580, 0x259F, 'Block Elements'),\n",
    "    (0x25A0, 0x25FF, 'Geometric Shapes'),\n",
    "    (0x2600, 0x26FF, 'Miscellaneous Symbols'),\n",
    "    (0x2700, 0x27BF, 'Dingbats'),\n",
    "    (0x27C0, 0x27EF, 'Miscellaneous Mathematical Symbols-A'),\n",
    "    (0x2800, 0x28FF, 'Braille Patterns'),\n",
    "    (0x2E80, 0x2EFF, 'CJK Radicals Supplement'),\n",
    "    (0x2F00, 0x2FDF, 'Kangxi Radicals'),\n",
    "    (0x3000, 0x303F, 'CJK Symbols and Punctuation'),\n",
    "    (0x3040, 0x309F, 'Hiragana'),\n",
    "    (0x30A0, 0x30FF, 'Katakana'),\n",
    "    (0x3100, 0x312F, 'Bopomofo'),\n",
    "    (0x3130, 0x318F, 'Hangul Compatibility Jamo'),\n",
    "    (0x3200, 0x32FF, 'Enclosed CJK Letters and Months'),\n",
    "    (0x3300, 0x33FF, 'CJK Compatibility'),\n",
    "    (0x4E00, 0x9FFF, 'CJK Unified Ideographs'),\n",
    "    (0xA000, 0xA48F, 'Yi Syllables'),\n",
    "    (0xA490, 0xA4CF, 'Yi Radicals'),\n",
    "    (0xAC00, 0xD7AF, 'Hangul Syllables'),\n",
    "    (0xE000, 0xF8FF, 'Private Use Area'),\n",
    "    (0xF900, 0xFAFF, 'CJK Compatibility Ideographs'),\n",
    "    (0xFB00, 0xFB4F, 'Alphabetic Presentation Forms'),\n",
    "    (0xFB50, 0xFDFF, 'Arabic Presentation Forms-A'),\n",
    "    (0xFE00, 0xFE0F, 'Variation Selectors'),\n",
    "    (0xFE10, 0xFE1F, 'Vertical Forms'),\n",
    "    (0xFE20, 0xFE2F, 'Combining Half Marks'),\n",
    "    (0xFE30, 0xFE4F, 'CJK Compatibility Forms'),\n",
    "    (0xFE50, 0xFE6F, 'Small Form Variants'),\n",
    "    (0xFE70, 0xFEFF, 'Arabic Presentation Forms-B'),\n",
    "    (0xFF00, 0xFFEF, 'Halfwidth and Fullwidth Forms'),\n",
    "    (0xFFF0, 0xFFFF, 'Specials'),\n",
    "    (0x10000, 0x1007F, 'Linear B Syllabary'),\n",
    "    (0x10080, 0x100FF, 'Linear B Ideograms'),\n",
    "    (0x10100, 0x1013F, 'Aegean Numbers'),\n",
    "    (0x10300, 0x1032F, 'Old Italic'),\n",
    "    (0x10330, 0x1034F, 'Gothic'),\n",
    "    (0x10380, 0x1039F, 'Ugaritic'),\n",
    "    (0x103A0, 0x103DF, 'Old Persian'),\n",
    "    (0x10400, 0x1044F, 'Deseret'),\n",
    "    (0x10450, 0x1047F, 'Shavian'),\n",
    "    (0x10480, 0x104AF, 'Osmanya'),\n",
    "    (0x10800, 0x1083F, 'Cypriot Syllabary'),\n",
    "    (0x10900, 0x1091F, 'Phoenician'),\n",
    "    (0x10A00, 0x10A5F, 'Kharoshthi'),\n",
    "    (0x12000, 0x123FF, 'Cuneiform'),\n",
    "    (0x12400, 0x1247F, 'Cuneiform Numbers and Punctuation'),\n",
    "    (0x1D000, 0x1D0FF, 'Byzantine Musical Symbols'),\n",
    "    (0x1D100, 0x1D1FF, 'Musical Symbols'),\n",
    "    (0x1D200, 0x1D24F, 'Ancient Greek Musical Notation'),\n",
    "    (0x1D300, 0x1D35F, 'Tai Xuan Jing Symbols'),\n",
    "    (0x1D360, 0x1D37F, 'Counting Rod Numerals'),\n",
    "    (0x1D400, 0x1D7FF, 'Mathematical Alphanumeric Symbols'),\n",
    "    (0x1F000, 0x1F02F, 'Mahjong Tiles'),\n",
    "    (0x1F030, 0x1F09F, 'Domino Tiles'),\n",
    "    (0x1F0A0, 0x1F0FF, 'Playing Cards'),\n",
    "    (0x1F100, 0x1F1FF, 'Enclosed Alphanumeric Supplement'),\n",
    "    (0x1F200, 0x1F2FF, 'Enclosed Ideographic Supplement'),\n",
    "    (0x1F300, 0x1F5FF, 'Miscellaneous Symbols and Pictographs'),\n",
    "    (0x1F600, 0x1F64F, 'Emoticons'),\n",
    "    (0x1F650, 0x1F67F, 'Ornamental Dingbats'),\n",
    "    (0x1F680, 0x1F6FF, 'Transport and Map Symbols'),\n",
    "    (0x1F700, 0x1F77F, 'Alchemical Symbols'),\n",
    "    (0x1F780, 0x1F7FF, 'Geometric Shapes Extended'),\n",
    "    (0x1F800, 0x1F8FF, 'Supplemental Arrows-C'),\n",
    "    (0x1F900, 0x1F9FF, 'Supplemental Symbols and Pictographs'),\n",
    "    (0x20000, 0x2A6DF, 'CJK Unified Ideographs Extension B'),\n",
    "    (0x2A700, 0x2B73F, 'CJK Unified Ideographs Extension C'),\n",
    "    (0x2B740, 0x2B81F, 'CJK Unified Ideographs Extension D'),\n",
    "    (0x2B820, 0x2CEAF, 'CJK Unified Ideographs Extension E'),\n",
    "]\n",
    "\n",
    "def get_unicode_block(char):\n",
    "    \"\"\"Get the Unicode block name for a character\"\"\"\n",
    "    code = ord(char)\n",
    "    for start, end, name in UNICODE_BLOCKS:\n",
    "        if start <= code <= end:\n",
    "            return name\n",
    "    return 'Unknown'\n",
    "\n",
    "print(f\"✓ Unicode block detection ready\")\n",
    "print(f\"  Defined {len(UNICODE_BLOCKS)} Unicode blocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and demographics...\n",
      "\n",
      "✓ Loaded tokenizer (10,000 tokens)\n",
      "✓ Loaded demographics data\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer and demographics...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Load existing demographics\n",
    "with open(DEMOGRAPHICS_PATH, 'r', encoding='utf-8') as f:\n",
    "    demographics = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded tokenizer ({len(vocab):,} tokens)\")\n",
    "print(f\"✓ Loaded demographics data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Unicode Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Unicode blocks for all tokens...\n",
      "\n",
      "✓ Analysis complete\n",
      "  Found 55 different Unicode blocks represented\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAnalyzing Unicode blocks for all tokens...\\n\")\n",
    "\n",
    "# For each token, find which blocks it contains\n",
    "token_blocks = {}  # token_id -> set of block names\n",
    "block_token_counts = Counter()  # block_name -> count of tokens containing it\n",
    "pure_block_tokens = defaultdict(list)  # block_name -> [(token, id), ...] for pure tokens\n",
    "\n",
    "for token_str, token_id in vocab.items():\n",
    "    # Get blocks for all characters in this token\n",
    "    blocks = set(get_unicode_block(char) for char in token_str)\n",
    "    token_blocks[token_id] = blocks\n",
    "    \n",
    "    # Count occurrences\n",
    "    for block in blocks:\n",
    "        block_token_counts[block] += 1\n",
    "    \n",
    "    # If token is pure (single block), add to that block's list\n",
    "    if len(blocks) == 1:\n",
    "        block_name = list(blocks)[0]\n",
    "        pure_block_tokens[block_name].append((token_str, token_id))\n",
    "\n",
    "print(f\"✓ Analysis complete\")\n",
    "print(f\"  Found {len(block_token_counts)} different Unicode blocks represented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Block Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "UNICODE BLOCK DISTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "Tokens containing characters from each block:\n",
      "(A token may appear in multiple blocks if it contains mixed characters)\n",
      "\n",
      "  Basic Latin (ASCII)                          : 6,109 tokens (61.09%) - 6,100 pure\n",
      "  Thai                                         : 1,272 tokens (12.72%) - 1,272 pure\n",
      "  CJK Unified Ideographs                       : 1,035 tokens (10.35%) - 1,035 pure\n",
      "  Hangul Syllables                             :   385 tokens ( 3.85%) -   385 pure\n",
      "  Unknown                                      :   107 tokens ( 1.07%) -   107 pure\n",
      "  Latin-1 Supplement                           :    89 tokens ( 0.89%) -    89 pure\n",
      "  Cyrillic                                     :    83 tokens ( 0.83%) -    83 pure\n",
      "  Miscellaneous Symbols and Pictographs        :    70 tokens ( 0.70%) -    70 pure\n",
      "  Katakana                                     :    65 tokens ( 0.65%) -    65 pure\n",
      "  Latin Extended-A                             :    63 tokens ( 0.63%) -    63 pure\n",
      "  Arabic                                       :    61 tokens ( 0.61%) -    61 pure\n",
      "  Hiragana                                     :    59 tokens ( 0.59%) -    59 pure\n",
      "  Devanagari                                   :    55 tokens ( 0.55%) -    55 pure\n",
      "  Greek and Coptic                             :    47 tokens ( 0.47%) -    47 pure\n",
      "  Myanmar                                      :    41 tokens ( 0.41%) -    41 pure\n",
      "  General Punctuation                          :    35 tokens ( 0.35%) -    26 pure\n",
      "  Latin Extended Additional                    :    33 tokens ( 0.33%) -    33 pure\n",
      "  Hebrew                                       :    33 tokens ( 0.33%) -    33 pure\n",
      "  Emoticons                                    :    30 tokens ( 0.30%) -    30 pure\n",
      "  Bengali                                      :    29 tokens ( 0.29%) -    29 pure\n",
      "  Halfwidth and Fullwidth Forms                :    28 tokens ( 0.28%) -    28 pure\n",
      "  Miscellaneous Symbols                        :    28 tokens ( 0.28%) -    28 pure\n",
      "  Mathematical Operators                       :    21 tokens ( 0.21%) -    21 pure\n",
      "  Geometric Shapes                             :    20 tokens ( 0.20%) -    20 pure\n",
      "  Supplemental Symbols and Pictographs         :    19 tokens ( 0.19%) -    19 pure\n",
      "  Dingbats                                     :    19 tokens ( 0.19%) -    19 pure\n",
      "  Greek Extended                               :    18 tokens ( 0.18%) -    18 pure\n",
      "  Spacing Modifier Letters                     :    15 tokens ( 0.15%) -    15 pure\n",
      "  IPA Extensions                               :    15 tokens ( 0.15%) -    15 pure\n",
      "  Enclosed Alphanumeric Supplement             :    12 tokens ( 0.12%) -    12 pure\n",
      "  CJK Symbols and Punctuation                  :    11 tokens ( 0.11%) -    11 pure\n",
      "  Arrows                                       :    11 tokens ( 0.11%) -    11 pure\n",
      "  Combining Diacritical Marks                  :     9 tokens ( 0.09%) -     9 pure\n",
      "  Latin Extended-B                             :     8 tokens ( 0.08%) -     8 pure\n",
      "  Georgian                                     :     8 tokens ( 0.08%) -     8 pure\n",
      "  Currency Symbols                             :     7 tokens ( 0.07%) -     7 pure\n",
      "  Transport and Map Symbols                    :     7 tokens ( 0.07%) -     7 pure\n",
      "  Letterlike Symbols                           :     6 tokens ( 0.06%) -     6 pure\n",
      "  Number Forms                                 :     6 tokens ( 0.06%) -     6 pure\n",
      "  Superscripts and Subscripts                  :     6 tokens ( 0.06%) -     6 pure\n",
      "  Hangul Compatibility Jamo                    :     4 tokens ( 0.04%) -     4 pure\n",
      "  Enclosed Alphanumerics                       :     4 tokens ( 0.04%) -     4 pure\n",
      "  Box Drawing                                  :     4 tokens ( 0.04%) -     4 pure\n",
      "  Unified Canadian Aboriginal Syllabics        :     3 tokens ( 0.03%) -     3 pure\n",
      "  Alphabetic Presentation Forms                :     3 tokens ( 0.03%) -     3 pure\n",
      "  Miscellaneous Technical                      :     3 tokens ( 0.03%) -     3 pure\n",
      "  Specials                                     :     2 tokens ( 0.02%) -     2 pure\n",
      "  Miscellaneous Mathematical Symbols-A         :     2 tokens ( 0.02%) -     2 pure\n",
      "  Variation Selectors                          :     2 tokens ( 0.02%) -     2 pure\n",
      "  Arabic Presentation Forms-A                  :     2 tokens ( 0.02%) -     2 pure\n",
      "  CJK Compatibility Forms                      :     1 tokens ( 0.01%) -     1 pure\n",
      "  Combining Diacritical Marks for Symbols      :     1 tokens ( 0.01%) -     1 pure\n",
      "  Cyrillic Supplement                          :     1 tokens ( 0.01%) -     1 pure\n",
      "  Mathematical Alphanumeric Symbols            :     1 tokens ( 0.01%) -     1 pure\n",
      "  Braille Patterns                             :     1 tokens ( 0.01%) -     1 pure\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"UNICODE BLOCK DISTRIBUTION\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Tokens containing characters from each block:\")\n",
    "print(f\"(A token may appear in multiple blocks if it contains mixed characters)\\n\")\n",
    "\n",
    "# Sort by count, descending\n",
    "sorted_blocks = sorted(block_token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for block_name, count in sorted_blocks:\n",
    "    pct = 100 * count / len(vocab)\n",
    "    pure_count = len(pure_block_tokens[block_name])\n",
    "    print(f\"  {block_name:45s}: {count:5,} tokens ({pct:5.2f}%) - {pure_count:5,} pure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on \"Other\" Category\n",
    "\n",
    "Let's see what Unicode blocks are in the 2,627 \"other\" tokens from 1.19b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BREAKDOWN OF 'OTHER' CATEGORY\n",
      "======================================================================\n",
      "\n",
      "Total 'other' tokens: 2,627\n",
      "\n",
      "Unicode blocks in 'other' category:\n",
      "  CJK Unified Ideographs                       : 1,035 tokens ( 39.4%)\n",
      "  Hangul Syllables                             :   385 tokens ( 14.7%)\n",
      "  Unknown                                      :   107 tokens (  4.1%)\n",
      "  Latin-1 Supplement                           :    88 tokens (  3.3%)\n",
      "  Cyrillic                                     :    83 tokens (  3.2%)\n",
      "  Miscellaneous Symbols and Pictographs        :    70 tokens (  2.7%)\n",
      "  Katakana                                     :    65 tokens (  2.5%)\n",
      "  Latin Extended-A                             :    63 tokens (  2.4%)\n",
      "  Arabic                                       :    61 tokens (  2.3%)\n",
      "  Hiragana                                     :    59 tokens (  2.2%)\n",
      "  Devanagari                                   :    55 tokens (  2.1%)\n",
      "  Greek and Coptic                             :    47 tokens (  1.8%)\n",
      "  Myanmar                                      :    41 tokens (  1.6%)\n",
      "  General Punctuation                          :    35 tokens (  1.3%)\n",
      "  Latin Extended Additional                    :    33 tokens (  1.3%)\n",
      "  Hebrew                                       :    33 tokens (  1.3%)\n",
      "  Emoticons                                    :    30 tokens (  1.1%)\n",
      "  Bengali                                      :    29 tokens (  1.1%)\n",
      "  Miscellaneous Symbols                        :    28 tokens (  1.1%)\n",
      "  Halfwidth and Fullwidth Forms                :    28 tokens (  1.1%)\n",
      "  Mathematical Operators                       :    21 tokens (  0.8%)\n",
      "  Geometric Shapes                             :    20 tokens (  0.8%)\n",
      "  Supplemental Symbols and Pictographs         :    19 tokens (  0.7%)\n",
      "  Dingbats                                     :    19 tokens (  0.7%)\n",
      "  Greek Extended                               :    18 tokens (  0.7%)\n",
      "  Spacing Modifier Letters                     :    15 tokens (  0.6%)\n",
      "  IPA Extensions                               :    15 tokens (  0.6%)\n",
      "  Enclosed Alphanumeric Supplement             :    12 tokens (  0.5%)\n",
      "  Arrows                                       :    11 tokens (  0.4%)\n",
      "  CJK Symbols and Punctuation                  :    11 tokens (  0.4%)\n",
      "  Basic Latin (ASCII)                          :     9 tokens (  0.3%)\n",
      "  Combining Diacritical Marks                  :     9 tokens (  0.3%)\n",
      "  Latin Extended-B                             :     8 tokens (  0.3%)\n",
      "  Georgian                                     :     8 tokens (  0.3%)\n",
      "  Transport and Map Symbols                    :     7 tokens (  0.3%)\n",
      "  Currency Symbols                             :     7 tokens (  0.3%)\n",
      "  Superscripts and Subscripts                  :     6 tokens (  0.2%)\n",
      "  Number Forms                                 :     6 tokens (  0.2%)\n",
      "  Letterlike Symbols                           :     6 tokens (  0.2%)\n",
      "  Box Drawing                                  :     4 tokens (  0.2%)\n",
      "  Enclosed Alphanumerics                       :     4 tokens (  0.2%)\n",
      "  Hangul Compatibility Jamo                    :     4 tokens (  0.2%)\n",
      "  Alphabetic Presentation Forms                :     3 tokens (  0.1%)\n",
      "  Miscellaneous Technical                      :     3 tokens (  0.1%)\n",
      "  Unified Canadian Aboriginal Syllabics        :     3 tokens (  0.1%)\n",
      "  Miscellaneous Mathematical Symbols-A         :     2 tokens (  0.1%)\n",
      "  Arabic Presentation Forms-A                  :     2 tokens (  0.1%)\n",
      "  Variation Selectors                          :     2 tokens (  0.1%)\n",
      "  Specials                                     :     2 tokens (  0.1%)\n",
      "  Braille Patterns                             :     1 tokens (  0.0%)\n",
      "  Mathematical Alphanumeric Symbols            :     1 tokens (  0.0%)\n",
      "  Combining Diacritical Marks for Symbols      :     1 tokens (  0.0%)\n",
      "  Cyrillic Supplement                          :     1 tokens (  0.0%)\n",
      "  CJK Compatibility Forms                      :     1 tokens (  0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BREAKDOWN OF 'OTHER' CATEGORY\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Get list of \"other\" tokens from demographics\n",
    "other_tokens = [(t, i) for t, i in demographics['tokens_by_category']['other']]\n",
    "other_token_ids = set(i for _, i in other_tokens)\n",
    "\n",
    "print(f\"Total 'other' tokens: {len(other_tokens):,}\\n\")\n",
    "\n",
    "# Count blocks within \"other\" category\n",
    "other_block_counts = Counter()\n",
    "for token_str, token_id in other_tokens:\n",
    "    blocks = token_blocks[token_id]\n",
    "    for block in blocks:\n",
    "        other_block_counts[block] += 1\n",
    "\n",
    "print(f\"Unicode blocks in 'other' category:\")\n",
    "sorted_other_blocks = sorted(other_block_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for block_name, count in sorted_other_blocks:\n",
    "    pct = 100 * count / len(other_tokens)\n",
    "    print(f\"  {block_name:45s}: {count:5,} tokens ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Examples from Top Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLES FROM TOP UNICODE BLOCKS\n",
      "======================================================================\n",
      "\n",
      "Basic Latin (ASCII) (pure tokens: 6,100):\n",
      "      0: '<|endoftext|>'\n",
      "      1: '\\n'\n",
      "      2: ' '\n",
      "      3: '!'\n",
      "      4: '\"'\n",
      "      5: '#'\n",
      "      6: '$'\n",
      "      7: '%'\n",
      "      8: '&'\n",
      "      9: \"'\"\n",
      "     10: '('\n",
      "     11: ')'\n",
      "     12: '*'\n",
      "     13: '+'\n",
      "     14: ','\n",
      "     15: '-'\n",
      "     16: '.'\n",
      "     17: '/'\n",
      "     18: '0'\n",
      "     19: '1'\n",
      "  ... and 6,080 more\n",
      "\n",
      "Thai (pure tokens: 1,272):\n",
      "    673: 'ก'\n",
      "    674: 'ข'\n",
      "    675: 'ฃ'\n",
      "    676: 'ค'\n",
      "    677: 'ฅ'\n",
      "    678: 'ฆ'\n",
      "    679: 'ง'\n",
      "    680: 'จ'\n",
      "    681: 'ฉ'\n",
      "    682: 'ช'\n",
      "    683: 'ซ'\n",
      "    684: 'ฌ'\n",
      "    685: 'ญ'\n",
      "    686: 'ฎ'\n",
      "    687: 'ฏ'\n",
      "    688: 'ฐ'\n",
      "    689: 'ฑ'\n",
      "    690: 'ฒ'\n",
      "    691: 'ณ'\n",
      "    692: 'ด'\n",
      "  ... and 1,252 more\n",
      "\n",
      "CJK Unified Ideographs (pure tokens: 1,035):\n",
      "   1202: '一'\n",
      "   1203: '七'\n",
      "   1204: '万'\n",
      "   1205: '丈'\n",
      "   1206: '三'\n",
      "   1207: '上'\n",
      "   1208: '下'\n",
      "   1209: '不'\n",
      "   1210: '与'\n",
      "   1211: '世'\n",
      "   1212: '両'\n",
      "   1213: '並'\n",
      "   1214: '个'\n",
      "   1215: '中'\n",
      "   1216: '为'\n",
      "   1217: '主'\n",
      "   1218: '乃'\n",
      "   1219: '久'\n",
      "   1220: '义'\n",
      "   1221: '之'\n",
      "  ... and 1,015 more\n",
      "\n",
      "Hangul Syllables (pure tokens: 385):\n",
      "   2239: '가'\n",
      "   2240: '각'\n",
      "   2241: '간'\n",
      "   2242: '갈'\n",
      "   2243: '감'\n",
      "   2244: '강'\n",
      "   2245: '같'\n",
      "   2246: '개'\n",
      "   2247: '거'\n",
      "   2248: '건'\n",
      "   2249: '걷'\n",
      "   2250: '걸'\n",
      "   2251: '것'\n",
      "   2252: '게'\n",
      "   2253: '겐'\n",
      "   2254: '겠'\n",
      "   2255: '계'\n",
      "   2256: '고'\n",
      "   2257: '곤'\n",
      "   2258: '공'\n",
      "  ... and 365 more\n",
      "\n",
      "Unknown (pure tokens: 107):\n",
      "    521: '࠰'\n",
      "    522: '࠲'\n",
      "    523: '࠸'\n",
      "    608: 'ં'\n",
      "    609: 'આ'\n",
      "    610: 'ઉ'\n",
      "    611: 'ક'\n",
      "    612: 'ગ'\n",
      "    613: 'ટ'\n",
      "    614: 'ડ'\n",
      "    615: 'ણ'\n",
      "    616: 'ત'\n",
      "    617: 'થ'\n",
      "    618: 'દ'\n",
      "    619: 'ન'\n",
      "    620: 'પ'\n",
      "    621: 'ફ'\n",
      "    622: 'બ'\n",
      "    623: 'મ'\n",
      "    624: 'ર'\n",
      "  ... and 87 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EXAMPLES FROM TOP UNICODE BLOCKS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Show top 5 blocks\n",
    "for block_name, _ in sorted_blocks[:5]:\n",
    "    pure_tokens = pure_block_tokens[block_name]\n",
    "    print(f\"{block_name} (pure tokens: {len(pure_tokens):,}):\")\n",
    "    \n",
    "    # Show first 20 examples\n",
    "    sorted_tokens = sorted(pure_tokens, key=lambda x: x[1])[:20]\n",
    "    for token_str, token_id in sorted_tokens:\n",
    "        print(f\"  {token_id:5d}: {repr(token_str)}\")\n",
    "    \n",
    "    if len(pure_tokens) > 20:\n",
    "        print(f\"  ... and {len(pure_tokens) - 20:,} more\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Mixed-Block Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MIXED-BLOCK TOKENS\n",
      "======================================================================\n",
      "\n",
      "Tokens spanning multiple Unicode blocks: 9\n",
      "\n",
      "Examples (first 20):\n",
      "   3378: '.”'                 - Basic Latin (ASCII), General Punctuation\n",
      "   3579: ',”'                 - Basic Latin (ASCII), General Punctuation\n",
      "   6005: '?”'                 - Basic Latin (ASCII), General Punctuation\n",
      "   6326: '”.'                 - Basic Latin (ASCII), General Punctuation\n",
      "   7299: '”,'                 - Basic Latin (ASCII), General Punctuation\n",
      "   8035: '!”'                 - Basic Latin (ASCII), General Punctuation\n",
      "   8761: '.’'                 - Basic Latin (ASCII), General Punctuation\n",
      "   9463: '’.'                 - Basic Latin (ASCII), General Punctuation\n",
      "   9712: '’,'                 - Basic Latin (ASCII), General Punctuation\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MIXED-BLOCK TOKENS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Find tokens that span multiple blocks\n",
    "mixed_block_tokens = [(token_str, token_id, token_blocks[token_id]) \n",
    "                      for token_str, token_id in vocab.items() \n",
    "                      if len(token_blocks[token_id]) > 1]\n",
    "\n",
    "print(f\"Tokens spanning multiple Unicode blocks: {len(mixed_block_tokens):,}\\n\")\n",
    "\n",
    "if mixed_block_tokens:\n",
    "    print(f\"Examples (first 20):\")\n",
    "    sorted_mixed = sorted(mixed_block_tokens, key=lambda x: x[1])[:20]\n",
    "    for token_str, token_id, blocks in sorted_mixed:\n",
    "        block_list = ', '.join(sorted(blocks))\n",
    "        print(f\"  {token_id:5d}: {repr(token_str):20s} - {block_list}\")\n",
    "    \n",
    "    if len(mixed_block_tokens) > 20:\n",
    "        print(f\"  ... and {len(mixed_block_tokens) - 20:,} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Unicode Block Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Unicode block data to ../data/flannel_token_unicode_blocks.json...\n",
      "\n",
      "✓ Saved Unicode block data\n",
      "  Path: ../data/flannel_token_unicode_blocks.json\n",
      "  Size: 461.3 KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving Unicode block data to {OUTPUT_PATH}...\\n\")\n",
    "\n",
    "# Prepare data\n",
    "unicode_block_data = {\n",
    "    'block_token_counts': dict(block_token_counts),\n",
    "    'token_blocks': {str(tid): list(blocks) for tid, blocks in token_blocks.items()},\n",
    "    'pure_block_token_counts': {block: len(tokens) for block, tokens in pure_block_tokens.items()},\n",
    "    'mixed_block_token_count': len(mixed_block_tokens),\n",
    "    'other_category_blocks': dict(other_block_counts)\n",
    "}\n",
    "\n",
    "# Save\n",
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(unicode_block_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "output_path = Path(OUTPUT_PATH)\n",
    "if output_path.exists():\n",
    "    output_kb = output_path.stat().st_size / 1024\n",
    "    print(f\"✓ Saved Unicode block data\")\n",
    "    print(f\"  Path: {OUTPUT_PATH}\")\n",
    "    print(f\"  Size: {output_kb:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "UNICODE BLOCK ANALYSIS COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Key Findings:\n",
      "  Total Unicode blocks represented: 55\n",
      "  Tokens spanning multiple blocks: 9\n",
      "  Tokens in single block: 9,991\n",
      "\n",
      "Top 5 blocks by token count:\n",
      "  1. Basic Latin (ASCII): 6,109 tokens (61.1%), 6,100 pure\n",
      "  2. Thai: 1,272 tokens (12.7%), 1,272 pure\n",
      "  3. CJK Unified Ideographs: 1,035 tokens (10.3%), 1,035 pure\n",
      "  4. Hangul Syllables: 385 tokens (3.9%), 385 pure\n",
      "  5. Unknown: 107 tokens (1.1%), 107 pure\n",
      "\n",
      "Mystery of 'other' category solved:\n",
      "  Largest component: CJK Unified Ideographs (1,035 tokens)\n",
      "  Plus 53 other Unicode blocks\n",
      "\n",
      "Output:\n",
      "  Unicode block data: ../data/flannel_token_unicode_blocks.json\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"UNICODE BLOCK ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(f\"Key Findings:\")\n",
    "print(f\"  Total Unicode blocks represented: {len(block_token_counts)}\")\n",
    "print(f\"  Tokens spanning multiple blocks: {len(mixed_block_tokens):,}\")\n",
    "print(f\"  Tokens in single block: {len(vocab) - len(mixed_block_tokens):,}\\n\")\n",
    "\n",
    "print(f\"Top 5 blocks by token count:\")\n",
    "for i, (block_name, count) in enumerate(sorted_blocks[:5], 1):\n",
    "    pct = 100 * count / len(vocab)\n",
    "    pure = len(pure_block_tokens[block_name])\n",
    "    print(f\"  {i}. {block_name}: {count:,} tokens ({pct:.1f}%), {pure:,} pure\")\n",
    "\n",
    "print(f\"\\nMystery of 'other' category solved:\")\n",
    "if sorted_other_blocks:\n",
    "    top_other = sorted_other_blocks[0]\n",
    "    print(f\"  Largest component: {top_other[0]} ({top_other[1]:,} tokens)\")\n",
    "    if len(sorted_other_blocks) > 1:\n",
    "        print(f\"  Plus {len(sorted_other_blocks) - 1} other Unicode blocks\")\n",
    "\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Unicode block data: {OUTPUT_PATH}\")\n",
    "print()\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
