{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thimble 1: Adam Accounting Validation\n",
    "\n",
    "**Question:** Can we reconstruct observed weight changes from recorded gradients and optimizer states?\n",
    "\n",
    "**Method:** Train a small language model for 1,000 steps using bare PyTorch (no Trainer framework). Record at every step:\n",
    "- W: embedding weights\n",
    "- grad_W: gradients\n",
    "- momentum_W: Adam momentum (exp_avg)\n",
    "- variance_W: Adam variance (exp_avg_sq)\n",
    "\n",
    "Then test: does the AdamW update formula reproduce the observed ΔW?\n",
    "\n",
    "$$\\Delta W(t) = -\\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} - \\lambda \\cdot W(t-1)$$\n",
    "\n",
    "where $\\hat{m}_t = m_t / (1 - \\beta_1^t)$ and $\\hat{v}_t = v_t / (1 - \\beta_2^t)$ are bias-corrected.\n",
    "\n",
    "**Why this matters:** If we can't validate the accounting, we can't trust any downstream analysis of forces and dynamics. This is the foundation for understanding dead token motion.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "Same as Flannel experiments:\n",
    "- 10,000 token vocabulary (6,301 live, 3,699 dead)\n",
    "- 64-dimensional embeddings\n",
    "- 2-layer transformer\n",
    "- 2 attention heads\n",
    "- Tied embeddings (input embedding = output unembedding transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYERS = 2\n",
    "N_HEADS = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# Training\n",
    "NUM_STEPS = 1000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.0  # Studying dynamics in \"flat spacetime\"\n",
    "\n",
    "# Optimizer (AdamW)\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "# Initialization\n",
    "INIT_SCALE = 0.02  # N(0, 0.02)\n",
    "SEED = 42\n",
    "\n",
    "# Paths\n",
    "TOKENIZER_PATH = \"../data/flannel_tokenizer_chars.json\"\n",
    "CORPUS_PATH = \"../data/flannel_model_corpus.txt\"\n",
    "TOKEN_MASK_PATH = \"../tensors/Flannel/live_dead_tokens.safetensors\"\n",
    "OUTPUT_PATH = \"../tensors/Thimble/thimble_1.safetensors\"\n",
    "\n",
    "print(\"✓ Parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from tokenizers import Tokenizer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file, load_file\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MEMORY & DISK SAFETY CHECK\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Recording tensors (stored on CPU during training)\n",
    "bytes_bf16 = 2  # bfloat16\n",
    "bytes_f32 = 4   # float32\n",
    "\n",
    "recording_w = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16\n",
    "recording_grad = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_bf16\n",
    "recording_momentum = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_f32\n",
    "recording_variance = (NUM_STEPS+1) * VOCAB_SIZE * HIDDEN_DIM * bytes_f32\n",
    "recording_losses = (NUM_STEPS+1) * bytes_f32\n",
    "\n",
    "total_recording = recording_w + recording_grad + recording_momentum + recording_variance + recording_losses\n",
    "\n",
    "print(f\"Recording tensors (CPU memory):\")\n",
    "print(f\"  W:         {recording_w/1e9:.2f} GB\")\n",
    "print(f\"  grad_W:    {recording_grad/1e9:.2f} GB\")\n",
    "print(f\"  momentum:  {recording_momentum/1e9:.2f} GB\")\n",
    "print(f\"  variance:  {recording_variance/1e9:.2f} GB\")\n",
    "print(f\"  losses:    {recording_losses/1e9:.4f} GB\")\n",
    "print(f\"  {'─'*40}\")\n",
    "print(f\"  Total:     {total_recording/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Model memory (on device during training)\n",
    "embedding_params = VOCAB_SIZE * HIDDEN_DIM\n",
    "params_per_layer = 12 * HIDDEN_DIM**2  # Rough estimate for transformer layer\n",
    "transformer_params = N_LAYERS * params_per_layer\n",
    "total_model_params = embedding_params + transformer_params\n",
    "\n",
    "model_memory = total_model_params * bytes_bf16\n",
    "optimizer_memory = 2 * total_model_params * bytes_f32  # Adam has 2 states (m, v) in fp32\n",
    "activation_memory = BATCH_SIZE * MAX_SEQ_LEN * HIDDEN_DIM * N_LAYERS * 2 * bytes_bf16\n",
    "\n",
    "print(f\"Model memory (device memory):\")\n",
    "print(f\"  Model weights: {model_memory/1e9:.2f} GB ({total_model_params:,} params)\")\n",
    "print(f\"  Optimizer:     {optimizer_memory/1e9:.2f} GB (Adam states)\")\n",
    "print(f\"  Activations:   {activation_memory/1e9:.2f} GB (batch={BATCH_SIZE})\")\n",
    "print(f\"  {'─'*40}\")\n",
    "print(f\"  Total:         {(model_memory + optimizer_memory + activation_memory)/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Peak RAM: recording tensors + model + optimizer + activations + corpus + misc\n",
    "corpus_memory = 1371328 * 8  # Approximate token count * bytes per long\n",
    "misc_overhead = 1e9  # 1 GB for Python, libraries, etc.\n",
    "peak_ram = total_recording + model_memory + optimizer_memory + activation_memory + corpus_memory + misc_overhead\n",
    "\n",
    "print(f\"Peak RAM estimate:\")\n",
    "print(f\"  Recording:     {total_recording/1e9:.2f} GB\")\n",
    "print(f\"  Model+opt+act: {(model_memory + optimizer_memory + activation_memory)/1e9:.2f} GB\")\n",
    "print(f\"  Corpus+misc:   {(corpus_memory + misc_overhead)/1e9:.2f} GB\")\n",
    "print(f\"  {'─'*40}\")\n",
    "print(f\"  Total:         {peak_ram/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Disk space: same as recording tensors (plus small overhead for metadata)\n",
    "metadata_overhead = 1e6  # ~1 MB for scalar tensors\n",
    "disk_needed = total_recording + metadata_overhead\n",
    "\n",
    "print(f\"Disk space needed:\")\n",
    "print(f\"  Safetensors:   {disk_needed/1e9:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Safety verdict\n",
    "print(f\"{'='*80}\")\n",
    "if peak_ram <= 24e9:\n",
    "    print(f\"✓ SAFE: Peak RAM ({peak_ram/1e9:.1f} GB) within 24 GB budget\")\n",
    "else:\n",
    "    print(f\"⚠️  WARNING: Peak RAM ({peak_ram/1e9:.1f} GB) exceeds 24 GB budget!\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"✓ Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "print(f\"Loading tokenizer: {TOKENIZER_PATH}\")\n",
    "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "print(f\"  ✓ Vocabulary: {tokenizer.get_vocab_size():,} tokens\\n\")\n",
    "\n",
    "# Corpus\n",
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "encoding = tokenizer.encode(corpus_text)\n",
    "tokens = encoding.ids\n",
    "corpus_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "print(f\"  ✓ Tokens: {len(tokens):,}\\n\")\n",
    "\n",
    "# Token masks (for analysis later)\n",
    "print(f\"Loading token masks: {TOKEN_MASK_PATH}\")\n",
    "mask_data = load_file(TOKEN_MASK_PATH)\n",
    "live_mask = mask_data['live_mask'].bool()\n",
    "dead_mask = mask_data['dead_mask'].bool()\n",
    "n_live = live_mask.sum().item()\n",
    "n_dead = dead_mask.sum().item()\n",
    "print(f\"  ✓ Live: {n_live:,} | Dead: {n_dead:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = TokenDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "\n",
    "# DataLoader with deterministic sampling\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    generator=g,\n",
    "    worker_init_fn=seed_worker,\n",
    "    num_workers=0,  # Single-threaded for reproducibility\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dataset: {len(dataset):,} examples\")\n",
    "print(f\"✓ DataLoader: {len(dataloader):,} batches per epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating model...\\n\")\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYERS,\n",
    "    n_head=N_HEADS,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Initialize embedding weights with N(0, 0.02)\n",
    "with torch.no_grad():\n",
    "    nn.init.normal_(model.transformer.wte.weight, mean=0.0, std=INIT_SCALE)\n",
    "\n",
    "# Move to device and convert to bfloat16\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"  Architecture: {N_LAYERS} layers, {N_HEADS} heads, {HIDDEN_DIM}d embeddings\")\n",
    "print(f\"  Parameters: {n_params:,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Dtype: {model.dtype}\")\n",
    "print(f\"\\n✓ Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(ADAM_BETA1, ADAM_BETA2),\n",
    "    eps=ADAM_EPSILON,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "print(f\"✓ Optimizer: AdamW\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Betas: ({ADAM_BETA1}, {ADAM_BETA2})\")\n",
    "print(f\"  Epsilon: {ADAM_EPSILON}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-allocate Recording Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPre-allocating recording tensors...\\n\")\n",
    "\n",
    "# Shape: (num_steps+1, vocab_size, hidden_dim)\n",
    "W_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "grad_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.bfloat16)\n",
    "\n",
    "# Optimizer states are kept in float32 (PyTorch's internal format)\n",
    "momentum_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32)\n",
    "variance_history = torch.zeros(NUM_STEPS+1, VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32)\n",
    "\n",
    "# Losses\n",
    "loss_history = torch.zeros(NUM_STEPS+1, dtype=torch.float32)\n",
    "\n",
    "# Memory calculation\n",
    "memory_w = W_history.numel() * 2  # bfloat16 = 2 bytes\n",
    "memory_grad = grad_history.numel() * 2\n",
    "memory_momentum = momentum_history.numel() * 4  # float32 = 4 bytes\n",
    "memory_variance = variance_history.numel() * 4\n",
    "memory_loss = loss_history.numel() * 4\n",
    "total_memory = memory_w + memory_grad + memory_momentum + memory_variance + memory_loss\n",
    "\n",
    "print(f\"  W:         {tuple(W_history.shape)} (bfloat16) = {memory_w/1e9:.2f} GB\")\n",
    "print(f\"  grad_W:    {tuple(grad_history.shape)} (bfloat16) = {memory_grad/1e9:.2f} GB\")\n",
    "print(f\"  momentum:  {tuple(momentum_history.shape)} (float32) = {memory_momentum/1e9:.2f} GB\")\n",
    "print(f\"  variance:  {tuple(variance_history.shape)} (float32) = {memory_variance/1e9:.2f} GB\")\n",
    "print(f\"  losses:    {tuple(loss_history.shape)} (float32) = {memory_loss/1e9:.4f} GB\")\n",
    "print(f\"\\n  Total: {total_memory/1e9:.2f} GB\")\n",
    "print(f\"\\n✓ Tensors allocated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"THIMBLE 1: TRAINING\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Record initial state (step 0)\n",
    "W_history[0] = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "loss_history[0] = float('nan')  # No loss before first step\n",
    "print(\"✓ Recorded initial state (t=0)\\n\")\n",
    "\n",
    "# Create infinite iterator over dataloader\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "\n",
    "for step in tqdm(range(1, NUM_STEPS+1), desc=\"Training\"):\n",
    "    # Get next batch (cycle through dataset if needed)\n",
    "    try:\n",
    "        batch = next(data_iter)\n",
    "    except StopIteration:\n",
    "        data_iter = iter(dataloader)\n",
    "        batch = next(data_iter)\n",
    "    \n",
    "    # Move batch to device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # === RECORD GRADIENTS (before optimizer.step) ===\n",
    "    grad_history[step] = model.transformer.wte.weight.grad.clone().cpu().bfloat16()\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # === RECORD WEIGHTS & OPTIMIZER STATE (after optimizer.step) ===\n",
    "    W_history[step] = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "    \n",
    "    # Get optimizer state for embedding weights\n",
    "    wte_param = model.transformer.wte.weight\n",
    "    if wte_param in optimizer.state:\n",
    "        opt_state = optimizer.state[wte_param]\n",
    "        momentum_history[step] = opt_state['exp_avg'].clone().cpu().float()\n",
    "        variance_history[step] = opt_state['exp_avg_sq'].clone().cpu().float()\n",
    "    \n",
    "    loss_history[step] = loss.item()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training complete\")\n",
    "print(f\"  Time: {elapsed:.1f}s ({elapsed/60:.1f} minutes)\")\n",
    "print(f\"  Final loss: {loss_history[-1]:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Validation: Test Adam Accounting\n",
    "\n",
    "Before saving, let's verify that we can reconstruct ΔW from our recorded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating Adam accounting...\n",
      "\n",
      "t=  1: ratio=1.012, cosine=+1.000\n",
      "t= 10: ratio=1.003, cosine=+0.999\n",
      "t= 50: ratio=0.987, cosine=+0.991\n",
      "t=100: ratio=0.957, cosine=+0.918\n",
      "t=200: ratio=1.048, cosine=+0.882\n",
      "t=500: ratio=0.917, cosine=+0.879\n",
      "t=800: ratio=1.086, cosine=+0.821\n",
      "\n",
      "(Ratio should be ~1.0, cosine should be ~1.0 for perfect match)\n",
      "\n",
      "✓ Validation complete\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nValidating Adam accounting...\\n\")\n",
    "\n",
    "# Test at several timesteps\n",
    "test_steps = [1, 10, 50, 100, 200, 500, 800]\n",
    "\n",
    "for t in test_steps:\n",
    "    if t > NUM_STEPS:\n",
    "        continue\n",
    "    \n",
    "    # Measured ΔW\n",
    "    delta_W_measured = W_history[t] - W_history[t-1]\n",
    "    measured_norm = torch.norm(delta_W_measured.float())\n",
    "    \n",
    "    # Compute ΔW from AdamW formula\n",
    "    m_t = momentum_history[t]\n",
    "    v_t = variance_history[t]\n",
    "    \n",
    "    # Bias correction\n",
    "    m_hat = m_t / (1 - ADAM_BETA1**t)\n",
    "    v_hat = v_t / (1 - ADAM_BETA2**t)\n",
    "    \n",
    "    # AdamW update\n",
    "    adam_term = LEARNING_RATE * m_hat / (torch.sqrt(v_hat) + ADAM_EPSILON)\n",
    "    decay_term = WEIGHT_DECAY * W_history[t-1].float()\n",
    "    \n",
    "    delta_W_computed = -adam_term - decay_term\n",
    "    computed_norm = torch.norm(delta_W_computed)\n",
    "    \n",
    "    # Compare\n",
    "    ratio = computed_norm / measured_norm\n",
    "    \n",
    "    # Cosine similarity\n",
    "    cosine = (delta_W_computed.flatten() @ delta_W_measured.float().flatten()) / (computed_norm * measured_norm)\n",
    "    \n",
    "    print(f\"t={t:3d}: ratio={ratio:.3f}, cosine={cosine:+.3f}\")\n",
    "\n",
    "print(\"\\n(Ratio should be ~1.0, cosine should be ~1.0 for perfect match)\")\n",
    "print(\"\\n✓ Validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSaving data to {OUTPUT_PATH}...\\n\")\n",
    "\n",
    "# Create output directory if needed\n",
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build save dictionary\n",
    "save_dict = {\n",
    "    # Training trajectories\n",
    "    'W': W_history,\n",
    "    'grad_W': grad_history,\n",
    "    'momentum_W': momentum_history,\n",
    "    'variance_W': variance_history,\n",
    "    'losses': loss_history,\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    'vocab_size': torch.tensor(VOCAB_SIZE, dtype=torch.long),\n",
    "    'hidden_dim': torch.tensor(HIDDEN_DIM, dtype=torch.long),\n",
    "    'n_layers': torch.tensor(N_LAYERS, dtype=torch.long),\n",
    "    'n_heads': torch.tensor(N_HEADS, dtype=torch.long),\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'num_steps': torch.tensor(NUM_STEPS, dtype=torch.long),\n",
    "    'batch_size': torch.tensor(BATCH_SIZE, dtype=torch.long),\n",
    "    'learning_rate': torch.tensor(LEARNING_RATE, dtype=torch.float32),\n",
    "    'weight_decay': torch.tensor(WEIGHT_DECAY, dtype=torch.float32),\n",
    "    'adam_beta1': torch.tensor(ADAM_BETA1, dtype=torch.float32),\n",
    "    'adam_beta2': torch.tensor(ADAM_BETA2, dtype=torch.float32),\n",
    "    'adam_epsilon': torch.tensor(ADAM_EPSILON, dtype=torch.float32),\n",
    "    'init_scale': torch.tensor(INIT_SCALE, dtype=torch.float32),\n",
    "    'seed': torch.tensor(SEED, dtype=torch.long),\n",
    "    \n",
    "    # Token counts\n",
    "    'n_live': torch.tensor(n_live, dtype=torch.long),\n",
    "    'n_dead': torch.tensor(n_dead, dtype=torch.long),\n",
    "}\n",
    "\n",
    "# Save\n",
    "save_start = time.time()\n",
    "save_file(save_dict, str(OUTPUT_PATH))\n",
    "save_elapsed = time.time() - save_start\n",
    "\n",
    "# File size\n",
    "file_size_bytes = Path(OUTPUT_PATH).stat().st_size\n",
    "file_size_gb = file_size_bytes / 1e9\n",
    "\n",
    "print(f\"✓ Saved successfully\")\n",
    "print(f\"  File: {Path(OUTPUT_PATH).name}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB\")\n",
    "print(f\"  Save time: {save_elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"THIMBLE 1 COMPLETE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Trained small language model for {NUM_STEPS:,} steps\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print()\n",
    "print(f\"Recorded at every step:\")\n",
    "print(f\"  • W: embedding weights (bfloat16)\")\n",
    "print(f\"  • grad_W: gradients (bfloat16)\")\n",
    "print(f\"  • momentum_W: Adam exp_avg (float32)\")\n",
    "print(f\"  • variance_W: Adam exp_avg_sq (float32)\")\n",
    "print(f\"  • losses: training loss\")\n",
    "print()\n",
    "print(f\"Data saved: {OUTPUT_PATH}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB\")\n",
    "print(f\"  Training time: {elapsed/60:.1f} minutes\")\n",
    "print()\n",
    "print(f\"Next step: Analyze in separate notebook to fully validate Adam accounting.\")\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
