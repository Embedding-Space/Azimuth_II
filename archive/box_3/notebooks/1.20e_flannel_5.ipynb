{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.20e: Flannel 5 - Initialization Scale Sweep\n",
    "\n",
    "**Purpose:** Test whether the Inhale-Sneeze-Fimbulwinter epochs depend on initialization scale σ.\n",
    "\n",
    "## Research Question\n",
    "\n",
    "Flannel 4 confirmed that the five epochs are reproducible across different random seeds at σ=0.02.\n",
    "\n",
    "**But:** Are these dynamics universal, or specific to σ=0.02?\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "- **Fixed:** Random seed (42), all other hyperparameters\n",
    "- **Swept:** Initialization scale σ ∈ [0.005, 0.010, 0.015, 0.020, 0.025, 0.030, 0.035, 0.040, 0.045]\n",
    "- **Control:** σ=0.020 (run 4, our known reference point)\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "**Hypothesis 1:** Epoch structure persists, but timing/magnitude scale with σ\n",
    "**Hypothesis 2:** Phase transition at some critical σ where dynamics change qualitatively\n",
    "**Hypothesis 3:** Fimbulwinter timing depends on σ (gradient magnitudes → quantization freeze)\n",
    "\n",
    "## Data\n",
    "\n",
    "- 9 runs × 1000 steps each\n",
    "- Recording: W + losses\n",
    "- Storage: ~11.5 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parameters set\n"
     ]
    }
   ],
   "source": [
    "# === SWEEP CONFIG ===\n",
    "# Sweep initialization scale, fixed seed\n",
    "INIT_SCALES = [0.005, 0.010, 0.015, 0.020, 0.025, 0.030, 0.035, 0.040, 0.045]\n",
    "NUM_RUNS = len(INIT_SCALES)  # 9 runs\n",
    "FIXED_SEED = 42  # Same seed for all runs\n",
    "\n",
    "# === RECORDING CONFIG ===\n",
    "RECORD_CONFIG = {\n",
    "    'W': True,           # Embedding matrix (ALWAYS RECOMMENDED)\n",
    "    'grads': False,      # Gradients ∂L/∂W\n",
    "    'momentum': False,   # Adam exp_avg\n",
    "    'variance': False,   # Adam exp_avg_sq\n",
    "    'logits': False,     # Model outputs (large!)\n",
    "    'losses': True,      # Loss per step (tiny, always useful)\n",
    "}\n",
    "\n",
    "# === MODEL ARCHITECTURE ===\n",
    "VOCAB_SIZE = 10000\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYER = 2\n",
    "N_HEAD = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# === TRAINING CONFIG ===\n",
    "BATCH_SIZE = 32\n",
    "NUM_TRAIN_STEPS = 1000\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "# Optimizer: Adam\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_EPSILON = 1e-8\n",
    "\n",
    "# === DATA PATHS ===\n",
    "TOKENIZER_PATH = \"../data/flannel_tokenizer_chars.json\"\n",
    "CORPUS_PATH = \"../data/flannel_model_corpus.txt\"\n",
    "TOKEN_MASK_PATH = \"../tensors/Flannel/live_dead_tokens.safetensors\"\n",
    "OUTPUT_DIR = \"../tensors/Flannel\"\n",
    "OUTPUT_FILE = \"1.20e_flannel_5.safetensors\"\n",
    "\n",
    "print(\"✓ Parameters set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file, load_file\n",
    "import time\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory & Disk Requirements Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MEMORY & DISK REQUIREMENTS\n",
      "================================================================================\n",
      "\n",
      "Experiment configuration:\n",
      "  Sweep type: Initialization scale σ\n",
      "  σ values: [0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045]\n",
      "  Runs: 9\n",
      "  Steps per run: 1,000\n",
      "  Fixed seed: 42\n",
      "\n",
      "Recording 2 item(s): W, losses\n",
      "\n",
      "  W            (9, 1001, 10000, 64)              11.53 GB\n",
      "  losses       (9, 1001)                          0.00 GB\n",
      "\n",
      "Model parameters: 738,304\n",
      "  Model memory (bf16):         0.00 GB\n",
      "  Optimizer state (fp32):      0.01 GB\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "PEAK RAM ESTIMATE:            11.54 GB\n",
      "DISK SPACE NEEDED:            11.53 GB\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "✓ Resources within budget. Ready to proceed.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MEMORY & DISK REQUIREMENTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "bytes_per_element = 2  # bfloat16\n",
    "\n",
    "# Calculate size for each enabled recording\n",
    "tensor_sizes = {}\n",
    "tensor_shapes = {}\n",
    "\n",
    "if RECORD_CONFIG['W']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE, HIDDEN_DIM)\n",
    "    tensor_shapes['W'] = shape\n",
    "    tensor_sizes['W'] = np.prod(shape) * bytes_per_element\n",
    "\n",
    "if RECORD_CONFIG['grads']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE, HIDDEN_DIM)\n",
    "    tensor_shapes['grads'] = shape\n",
    "    tensor_sizes['grads'] = np.prod(shape) * bytes_per_element\n",
    "\n",
    "if RECORD_CONFIG['momentum']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE, HIDDEN_DIM)\n",
    "    tensor_shapes['momentum'] = shape\n",
    "    tensor_sizes['momentum'] = np.prod(shape) * bytes_per_element\n",
    "\n",
    "if RECORD_CONFIG['variance']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE, HIDDEN_DIM)\n",
    "    tensor_shapes['variance'] = shape\n",
    "    tensor_sizes['variance'] = np.prod(shape) * bytes_per_element\n",
    "\n",
    "if RECORD_CONFIG['logits']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE)\n",
    "    tensor_shapes['logits'] = shape\n",
    "    tensor_sizes['logits'] = np.prod(shape) * bytes_per_element\n",
    "\n",
    "if RECORD_CONFIG['losses']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1)\n",
    "    tensor_shapes['losses'] = shape\n",
    "    tensor_sizes['losses'] = np.prod(shape) * bytes_per_element\n",
    "\n",
    "total_recorded_data = sum(tensor_sizes.values())\n",
    "\n",
    "# Model memory\n",
    "embedding_params = VOCAB_SIZE * HIDDEN_DIM\n",
    "params_per_layer = 12 * HIDDEN_DIM**2\n",
    "transformer_params = N_LAYER * params_per_layer\n",
    "total_model_params = embedding_params + transformer_params\n",
    "\n",
    "model_memory = total_model_params * bytes_per_element\n",
    "optimizer_memory = 2 * total_model_params * 4\n",
    "\n",
    "peak_ram = total_recorded_data + model_memory + optimizer_memory\n",
    "disk_space = total_recorded_data\n",
    "\n",
    "# Display\n",
    "print(f\"Experiment configuration:\")\n",
    "print(f\"  Sweep type: Initialization scale σ\")\n",
    "print(f\"  σ values: {INIT_SCALES}\")\n",
    "print(f\"  Runs: {NUM_RUNS}\")\n",
    "print(f\"  Steps per run: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"  Fixed seed: {FIXED_SEED}\")\n",
    "print()\n",
    "\n",
    "enabled_items = [k for k, v in RECORD_CONFIG.items() if v]\n",
    "print(f\"Recording {len(enabled_items)} item(s): {', '.join(enabled_items)}\")\n",
    "print()\n",
    "\n",
    "for name, size_bytes in tensor_sizes.items():\n",
    "    size_gb = size_bytes / 1e9\n",
    "    shape = tensor_shapes[name]\n",
    "    print(f\"  {name:12} {str(shape):30} {size_gb:8.2f} GB\")\n",
    "\n",
    "print()\n",
    "print(f\"Model parameters: {total_model_params:,}\")\n",
    "print(f\"  Model memory (bf16):     {model_memory/1e9:8.2f} GB\")\n",
    "print(f\"  Optimizer state (fp32):  {optimizer_memory/1e9:8.2f} GB\")\n",
    "print()\n",
    "print(f\"{'─'*80}\")\n",
    "print(f\"PEAK RAM ESTIMATE:         {peak_ram/1e9:8.2f} GB\")\n",
    "print(f\"DISK SPACE NEEDED:         {disk_space/1e9:8.2f} GB\")\n",
    "print(f\"{'─'*80}\")\n",
    "\n",
    "# Warnings\n",
    "RAM_BUDGET = 24\n",
    "DISK_BUDGET = 50\n",
    "\n",
    "if peak_ram > RAM_BUDGET * 1e9:\n",
    "    print(f\"\\n⚠️  WARNING: Peak RAM ({peak_ram/1e9:.1f} GB) exceeds {RAM_BUDGET} GB budget!\")\n",
    "    print(f\"   Consider reducing NUM_TRAIN_STEPS or disabling expensive recordings.\")\n",
    "\n",
    "if disk_space > DISK_BUDGET * 1e9:\n",
    "    print(f\"\\n⚠️  WARNING: Disk space ({disk_space/1e9:.1f} GB) exceeds {DISK_BUDGET} GB budget!\")\n",
    "\n",
    "if peak_ram <= RAM_BUDGET * 1e9 and disk_space <= DISK_BUDGET * 1e9:\n",
    "    print(f\"\\n✓ Resources within budget. Ready to proceed.\")\n",
    "\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: ../data/flannel_tokenizer_chars.json\n",
      "  ✓ Vocabulary: 10,000 tokens\n",
      "\n",
      "Loading corpus: ../data/flannel_model_corpus.txt\n",
      "  ✓ Tokens: 1,371,328\n",
      "\n",
      "Loading token masks: ../tensors/Flannel/live_dead_tokens.safetensors\n",
      "  ✓ Live: 6,301 | Dead: 3,699\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "print(f\"Loading tokenizer: {TOKENIZER_PATH}\")\n",
    "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "print(f\"  ✓ Vocabulary: {tokenizer.get_vocab_size():,} tokens\\n\")\n",
    "\n",
    "# Corpus\n",
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "encoding = tokenizer.encode(corpus_text)\n",
    "tokens = encoding.ids\n",
    "corpus_tensor = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "print(f\"  ✓ Tokens: {len(tokens):,}\\n\")\n",
    "\n",
    "# Token masks\n",
    "print(f\"Loading token masks: {TOKEN_MASK_PATH}\")\n",
    "mask_data = load_file(TOKEN_MASK_PATH)\n",
    "dead_indices = mask_data['dead_indices']\n",
    "n_dead = mask_data['dead_mask'].sum().item()\n",
    "n_live = mask_data['live_mask'].sum().item()\n",
    "print(f\"  ✓ Live: {n_live:,} | Dead: {n_dead:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset: 1,371,200 examples\n"
     ]
    }
   ],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = TokenDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "print(f\"\\n✓ Dataset: {len(dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-allocate Recording Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-allocating recording tensors...\n",
      "\n",
      "  W:        (9, 1001, 10000, 64)\n",
      "  losses:   (9, 1001)\n",
      "\n",
      "✓ All tensors allocated on CPU\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPre-allocating recording tensors...\\n\")\n",
    "\n",
    "tensors = {}\n",
    "\n",
    "if RECORD_CONFIG['W']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE, HIDDEN_DIM)\n",
    "    tensors['W'] = torch.zeros(shape, dtype=torch.bfloat16)\n",
    "    print(f\"  W:        {shape}\")\n",
    "\n",
    "if RECORD_CONFIG['grads']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE, HIDDEN_DIM)\n",
    "    tensors['grads'] = torch.zeros(shape, dtype=torch.bfloat16)\n",
    "    print(f\"  grads:    {shape}\")\n",
    "\n",
    "if RECORD_CONFIG['momentum']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE, HIDDEN_DIM)\n",
    "    tensors['momentum'] = torch.zeros(shape, dtype=torch.bfloat16)\n",
    "    print(f\"  momentum: {shape}\")\n",
    "\n",
    "if RECORD_CONFIG['variance']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE, HIDDEN_DIM)\n",
    "    tensors['variance'] = torch.zeros(shape, dtype=torch.bfloat16)\n",
    "    print(f\"  variance: {shape}\")\n",
    "\n",
    "if RECORD_CONFIG['logits']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1, VOCAB_SIZE)\n",
    "    tensors['logits'] = torch.zeros(shape, dtype=torch.bfloat16)\n",
    "    print(f\"  logits:   {shape}\")\n",
    "\n",
    "if RECORD_CONFIG['losses']:\n",
    "    shape = (NUM_RUNS, NUM_TRAIN_STEPS+1)\n",
    "    tensors['losses'] = torch.full(shape, float('nan'), dtype=torch.bfloat16)\n",
    "    print(f\"  losses:   {shape}\")\n",
    "\n",
    "print(f\"\\n✓ All tensors allocated on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Recorder class defined\n"
     ]
    }
   ],
   "source": [
    "class BatchRecorder:\n",
    "    \"\"\"Records data directly into pre-allocated tensors.\"\"\"\n",
    "    \n",
    "    def __init__(self, tensors, record_config, run_idx):\n",
    "        self.tensors = tensors\n",
    "        self.config = record_config\n",
    "        self.run_idx = run_idx\n",
    "        self.current_step = 0\n",
    "        self.recorded_initial = False\n",
    "        \n",
    "        self.grad_before = None\n",
    "        self.loss_value = None\n",
    "        self.logits_sample = None\n",
    "    \n",
    "    def record_initial_state(self, model, optimizer):\n",
    "        \"\"\"Record step 0.\"\"\"\n",
    "        if not self.recorded_initial:\n",
    "            t = 0\n",
    "            \n",
    "            if self.config['W']:\n",
    "                self.tensors['W'][self.run_idx, t] = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "            \n",
    "            self.recorded_initial = True\n",
    "            self.current_step = 1\n",
    "            print(f\"    ✓ Recorded initial state (t=0)\")\n",
    "    \n",
    "    def record_before_step(self, model, loss, logits):\n",
    "        \"\"\"Capture data after backward, before optimizer step.\"\"\"\n",
    "        if self.config['grads'] and model.transformer.wte.weight.grad is not None:\n",
    "            self.grad_before = model.transformer.wte.weight.grad.clone().cpu().bfloat16()\n",
    "        \n",
    "        if self.config['losses']:\n",
    "            self.loss_value = loss.item()\n",
    "        \n",
    "        if self.config['logits']:\n",
    "            self.logits_sample = logits[0, -1, :].detach().cpu().bfloat16()\n",
    "    \n",
    "    def record_after_step(self, model, optimizer):\n",
    "        \"\"\"Record data after optimizer step.\"\"\"\n",
    "        t = self.current_step\n",
    "        \n",
    "        if t > self.tensors['W'].shape[1] - 1 if 'W' in self.tensors else float('inf'):\n",
    "            return\n",
    "        \n",
    "        if self.config['W']:\n",
    "            self.tensors['W'][self.run_idx, t] = model.transformer.wte.weight.data.clone().cpu().bfloat16()\n",
    "        \n",
    "        if self.config['grads'] and self.grad_before is not None:\n",
    "            self.tensors['grads'][self.run_idx, t] = self.grad_before\n",
    "            self.grad_before = None\n",
    "        \n",
    "        if self.config['momentum']:\n",
    "            param = model.transformer.wte.weight\n",
    "            if param in optimizer.state and 'exp_avg' in optimizer.state[param]:\n",
    "                self.tensors['momentum'][self.run_idx, t] = optimizer.state[param]['exp_avg'].clone().cpu().bfloat16()\n",
    "        \n",
    "        if self.config['variance']:\n",
    "            param = model.transformer.wte.weight\n",
    "            if param in optimizer.state and 'exp_avg_sq' in optimizer.state[param]:\n",
    "                self.tensors['variance'][self.run_idx, t] = optimizer.state[param]['exp_avg_sq'].clone().cpu().bfloat16()\n",
    "        \n",
    "        if self.config['logits'] and self.logits_sample is not None:\n",
    "            self.tensors['logits'][self.run_idx, t] = self.logits_sample\n",
    "            self.logits_sample = None\n",
    "        \n",
    "        if self.config['losses'] and self.loss_value is not None:\n",
    "            self.tensors['losses'][self.run_idx, t] = self.loss_value\n",
    "            self.loss_value = None\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(f\"    Step {t}\")\n",
    "        \n",
    "        self.current_step += 1\n",
    "\n",
    "print(\"✓ Recorder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumented Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ InstrumentedTrainer defined\n"
     ]
    }
   ],
   "source": [
    "class InstrumentedTrainer(Trainer):\n",
    "    def __init__(self, recorder, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.recorder = recorder\n",
    "        self.last_logits = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        self.last_logits = outputs.logits\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        loss = super().training_step(model, inputs, num_items_in_batch)\n",
    "        self.recorder.record_before_step(model, loss, self.last_logits)\n",
    "        return loss\n",
    "\n",
    "    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time=None, **kwargs):\n",
    "        self.recorder.record_after_step(model, self.optimizer)\n",
    "        super()._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, **kwargs)\n",
    "\n",
    "print(\"✓ InstrumentedTrainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization Scale Sweep Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FLANNEL 5: INITIALIZATION SCALE SWEEP\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Runs: 9\n",
      "  Steps per run: 1,000\n",
      "  Fixed seed: 42\n",
      "  σ values: [0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045]\n",
      "  Recording: W, losses\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RUN 1/9 (σ=0.005)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized (seed=42, σ=0.005)\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1415, 'grad_norm': 0.2158203125, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 26.2006, 'train_samples_per_second': 1221.348, 'train_steps_per_second': 38.167, 'train_loss': 7.14151318359375, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 1 complete (26.3s)\n",
      "\n",
      "================================================================================\n",
      "RUN 2/9 (σ=0.010)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized (seed=42, σ=0.010)\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1265, 'grad_norm': 0.2412109375, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 25.247, 'train_samples_per_second': 1267.479, 'train_steps_per_second': 39.609, 'train_loss': 7.12645361328125, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 2 complete (25.3s)\n",
      "\n",
      "================================================================================\n",
      "RUN 3/9 (σ=0.015)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized (seed=42, σ=0.015)\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1365, 'grad_norm': 0.2099609375, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 25.5252, 'train_samples_per_second': 1253.661, 'train_steps_per_second': 39.177, 'train_loss': 7.13654931640625, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 3 complete (25.6s)\n",
      "\n",
      "================================================================================\n",
      "RUN 4/9 (σ=0.020)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized (seed=42, σ=0.020)\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1299, 'grad_norm': 0.2177734375, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 24.9447, 'train_samples_per_second': 1282.837, 'train_steps_per_second': 40.089, 'train_loss': 7.12989404296875, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 4 complete (25.0s)\n",
      "\n",
      "================================================================================\n",
      "RUN 5/9 (σ=0.025)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized (seed=42, σ=0.025)\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.1044, 'grad_norm': 0.2294921875, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 24.9601, 'train_samples_per_second': 1282.045, 'train_steps_per_second': 40.064, 'train_loss': 7.104359375, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 5 complete (25.0s)\n",
      "\n",
      "================================================================================\n",
      "RUN 6/9 (σ=0.030)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized (seed=42, σ=0.030)\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.0936, 'grad_norm': 0.2451171875, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 24.5729, 'train_samples_per_second': 1302.246, 'train_steps_per_second': 40.695, 'train_loss': 7.09358056640625, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 6 complete (24.6s)\n",
      "\n",
      "================================================================================\n",
      "RUN 7/9 (σ=0.035)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized (seed=42, σ=0.035)\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.0824, 'grad_norm': 0.271484375, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 25.1051, 'train_samples_per_second': 1274.642, 'train_steps_per_second': 39.833, 'train_loss': 7.082412109375, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 7 complete (25.2s)\n",
      "\n",
      "================================================================================\n",
      "RUN 8/9 (σ=0.040)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized (seed=42, σ=0.040)\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.0945, 'grad_norm': 0.23046875, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 24.9436, 'train_samples_per_second': 1282.893, 'train_steps_per_second': 40.09, 'train_loss': 7.0944677734375, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 8 complete (25.0s)\n",
      "\n",
      "================================================================================\n",
      "RUN 9/9 (σ=0.045)\n",
      "================================================================================\n",
      "\n",
      "  ✓ Model initialized (seed=42, σ=0.045)\n",
      "    ✓ Recorded initial state (t=0)\n",
      "  Training...\n",
      "    Step 100\n",
      "    Step 200\n",
      "    Step 300\n",
      "    Step 400\n",
      "    Step 500\n",
      "    Step 600\n",
      "    Step 700\n",
      "    Step 800\n",
      "    Step 900\n",
      "    Step 1000\n",
      "{'loss': 7.0718, 'grad_norm': 0.283203125, 'learning_rate': 1e-06, 'epoch': 0.023337222870478413}\n",
      "{'train_runtime': 24.7774, 'train_samples_per_second': 1291.501, 'train_steps_per_second': 40.359, 'train_loss': 7.07181982421875, 'epoch': 0.023337222870478413}\n",
      "\n",
      "  ✓ Run 9 complete (24.8s)\n",
      "\n",
      "================================================================================\n",
      "✓ All 9 runs complete\n",
      "  Total time: 227.2s (3.8 minutes)\n",
      "  Average per run: 25.2s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FLANNEL 5: INITIALIZATION SCALE SWEEP\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Runs: {NUM_RUNS}\")\n",
    "print(f\"  Steps per run: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"  Fixed seed: {FIXED_SEED}\")\n",
    "print(f\"  σ values: {INIT_SCALES}\")\n",
    "print(f\"  Recording: {', '.join([k for k, v in RECORD_CONFIG.items() if v])}\")\n",
    "print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "experiment_start = time.time()\n",
    "\n",
    "for run_idx, init_scale in enumerate(INIT_SCALES):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RUN {run_idx + 1}/{NUM_RUNS} (σ={init_scale:.3f})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Set seed (same for all runs)\n",
    "    torch.manual_seed(FIXED_SEED)\n",
    "    np.random.seed(FIXED_SEED)\n",
    "    \n",
    "    # Create model\n",
    "    config = GPT2Config(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        n_positions=MAX_SEQ_LEN,\n",
    "        n_embd=HIDDEN_DIM,\n",
    "        n_layer=N_LAYER,\n",
    "        n_head=N_HEAD,\n",
    "        resid_pdrop=0.0,\n",
    "        embd_pdrop=0.0,\n",
    "        attn_pdrop=0.0,\n",
    "        tie_word_embeddings=True,\n",
    "    )\n",
    "    \n",
    "    model = GPT2LMHeadModel(config).to(torch.bfloat16).to(device)\n",
    "    \n",
    "    # Initialize with current σ\n",
    "    init_f32 = torch.randn(VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32, device=device) * init_scale\n",
    "    with torch.no_grad():\n",
    "        model.transformer.wte.weight[:] = init_f32.to(torch.bfloat16)\n",
    "    \n",
    "    print(f\"  ✓ Model initialized (seed={FIXED_SEED}, σ={init_scale:.3f})\")\n",
    "    \n",
    "    # Create recorder\n",
    "    recorder = BatchRecorder(tensors, RECORD_CONFIG, run_idx)\n",
    "    \n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        max_steps=NUM_TRAIN_STEPS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        adam_beta1=ADAM_BETA1,\n",
    "        adam_beta2=ADAM_BETA2,\n",
    "        adam_epsilon=ADAM_EPSILON,\n",
    "        optim=\"adamw_torch\",\n",
    "        logging_steps=1000,\n",
    "        save_steps=NUM_TRAIN_STEPS + 1,\n",
    "        save_total_limit=0,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=False,\n",
    "        bf16=True,\n",
    "        seed=FIXED_SEED,\n",
    "        report_to=\"none\",\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "    \n",
    "    trainer = InstrumentedTrainer(\n",
    "        recorder=recorder,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "    \n",
    "    # Record initial state\n",
    "    recorder.record_initial_state(model, trainer.optimizer)\n",
    "    \n",
    "    # Train\n",
    "    print(f\"  Training...\")\n",
    "    run_start = time.time()\n",
    "    trainer.train()\n",
    "    run_elapsed = time.time() - run_start\n",
    "    \n",
    "    print(f\"\\n  ✓ Run {run_idx + 1} complete ({run_elapsed:.1f}s)\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model, trainer, recorder\n",
    "    \n",
    "    if device == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    elif device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "experiment_elapsed = time.time() - experiment_start\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ All {NUM_RUNS} runs complete\")\n",
    "print(f\"  Total time: {experiment_elapsed:.1f}s ({experiment_elapsed/60:.1f} minutes)\")\n",
    "print(f\"  Average per run: {experiment_elapsed/NUM_RUNS:.1f}s\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data...\n",
      "\n",
      "  W            (9, 1001, 10000, 64)          \n",
      "  losses       (9, 1001)                     \n",
      "\n",
      "Saving to: ../tensors/Flannel/1.20e_flannel_5.safetensors\n",
      "\n",
      "✓ Saved successfully\n",
      "  File: 1.20e_flannel_5.safetensors\n",
      "  Size: 11531.5 MB (11.53 GB)\n",
      "  Save time: 6.1s\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving data...\\n\")\n",
    "\n",
    "# Build save dictionary\n",
    "save_dict = {\n",
    "    # Metadata\n",
    "    'n_runs': torch.tensor(NUM_RUNS, dtype=torch.long),\n",
    "    'fixed_seed': torch.tensor(FIXED_SEED, dtype=torch.long),\n",
    "    'init_scales': torch.tensor(INIT_SCALES, dtype=torch.float32),\n",
    "    'n_steps': torch.tensor(NUM_TRAIN_STEPS, dtype=torch.long),\n",
    "    'n_live': torch.tensor(n_live, dtype=torch.long),\n",
    "    'n_dead': torch.tensor(n_dead, dtype=torch.long),\n",
    "    'vocab_size': torch.tensor(VOCAB_SIZE, dtype=torch.long),\n",
    "    'hidden_dim': torch.tensor(HIDDEN_DIM, dtype=torch.long),\n",
    "    'learning_rate': torch.tensor(LEARNING_RATE, dtype=torch.float32),\n",
    "    'weight_decay': torch.tensor(WEIGHT_DECAY, dtype=torch.float32),\n",
    "    'adam_beta1': torch.tensor(ADAM_BETA1, dtype=torch.float32),\n",
    "    'adam_beta2': torch.tensor(ADAM_BETA2, dtype=torch.float32),\n",
    "    # Record config\n",
    "    'recorded_W': torch.tensor(RECORD_CONFIG['W'], dtype=torch.bool),\n",
    "    'recorded_grads': torch.tensor(RECORD_CONFIG['grads'], dtype=torch.bool),\n",
    "    'recorded_momentum': torch.tensor(RECORD_CONFIG['momentum'], dtype=torch.bool),\n",
    "    'recorded_variance': torch.tensor(RECORD_CONFIG['variance'], dtype=torch.bool),\n",
    "    'recorded_logits': torch.tensor(RECORD_CONFIG['logits'], dtype=torch.bool),\n",
    "    'recorded_losses': torch.tensor(RECORD_CONFIG['losses'], dtype=torch.bool),\n",
    "}\n",
    "\n",
    "# Add all recorded tensors\n",
    "for name, tensor in tensors.items():\n",
    "    save_dict[name] = tensor\n",
    "    print(f\"  {name:12} {str(tuple(tensor.shape)):30}\")\n",
    "\n",
    "# Save\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILE\n",
    "\n",
    "print(f\"\\nSaving to: {output_path}\\n\")\n",
    "\n",
    "save_start = time.time()\n",
    "save_file(save_dict, str(output_path))\n",
    "save_elapsed = time.time() - save_start\n",
    "\n",
    "file_size_mb = output_path.stat().st_size / 1e6\n",
    "file_size_gb = file_size_mb / 1000\n",
    "\n",
    "print(f\"✓ Saved successfully\")\n",
    "print(f\"  File: {output_path.name}\")\n",
    "print(f\"  Size: {file_size_mb:.1f} MB ({file_size_gb:.2f} GB)\")\n",
    "print(f\"  Save time: {save_elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FLANNEL 5 COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Experiment: Initialization scale sweep\n",
      "  Runs: 9\n",
      "  Steps per run: 1,000\n",
      "  Fixed seed: 42\n",
      "  σ values: [0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045]\n",
      "  Control (σ=0.020): run 3\n",
      "  Recorded: W, losses\n",
      "\n",
      "Data saved: ../tensors/Flannel/1.20e_flannel_5.safetensors\n",
      "  Size: 11.53 GB\n",
      "  Total experiment time: 3.8 minutes\n",
      "\n",
      "Data structure:\n",
      "  W: (9, 1001, 10000, 64)\n",
      "  losses: (9, 1001)\n",
      "\n",
      "Next steps:\n",
      "  1. Create 1.23b analysis notebook (adapt 1.23a)\n",
      "  2. Plot mean radius trajectories for all σ values\n",
      "  3. Compare epoch timing across scales\n",
      "  4. Check for phase transitions or scaling laws\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FLANNEL 5 COMPLETE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Experiment: Initialization scale sweep\")\n",
    "print(f\"  Runs: {NUM_RUNS}\")\n",
    "print(f\"  Steps per run: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"  Fixed seed: {FIXED_SEED}\")\n",
    "print(f\"  σ values: {INIT_SCALES}\")\n",
    "print(f\"  Control (σ=0.020): run {INIT_SCALES.index(0.020)}\")\n",
    "print(f\"  Recorded: {', '.join([k for k, v in RECORD_CONFIG.items() if v])}\")\n",
    "print()\n",
    "print(f\"Data saved: {output_path}\")\n",
    "print(f\"  Size: {file_size_gb:.2f} GB\")\n",
    "print(f\"  Total experiment time: {experiment_elapsed/60:.1f} minutes\")\n",
    "print()\n",
    "print(f\"Data structure:\")\n",
    "for name in tensors.keys():\n",
    "    print(f\"  {name}: {tuple(tensors[name].shape)}\")\n",
    "print()\n",
    "print(f\"Next steps:\")\n",
    "print(f\"  1. Create 1.23b analysis notebook (adapt 1.23a)\")\n",
    "print(f\"  2. Plot mean radius trajectories for all σ values\")\n",
    "print(f\"  3. Compare epoch timing across scales\")\n",
    "print(f\"  4. Check for phase transitions or scaling laws\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
