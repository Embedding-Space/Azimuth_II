{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 1.5a: Logit Equivalence Classes\n",
    "\n",
    "This notebook identifies tokens that produce **identical logit scores** for a given hidden state due to bfloat16 quantization.\n",
    "\n",
    "## The Question\n",
    "\n",
    "We know from 1.4a that 2,251 tokens share identical **cosine** (0.84375) to a reference direction. But cosine only captures direction, not magnitude.\n",
    "\n",
    "**Logits are computed via dot products:**\n",
    "```\n",
    "logit[token_i] = hidden_state @ W[i]\n",
    "```\n",
    "\n",
    "The dot product depends on both direction AND magnitude. Due to bfloat16's limited precision, tokens with **different embeddings** may produce **identical dot products** when both are rounded to bfloat16.\n",
    "\n",
    "This notebook finds the **logit equivalence classes**: sets of tokens that are indistinguishable by dot product at bfloat16 precision.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. Define a simulated hidden state `h` using spherical coordinates (lat/lon/distance)\n",
    "2. Compute dot products `W @ h` in bfloat16\n",
    "3. Find tokens with identical dot product values\n",
    "4. Report the largest equivalence class (mode of dot product distribution)\n",
    "5. Compare to embedding-based equivalence (tokens with literally identical W[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to analyze\n",
    "MODEL_NAME = \"Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# PCA basis (same as 1.4a)\n",
    "NORTH_PC = 2      # North pole (+90° latitude)\n",
    "MERIDIAN_PC = 1   # Prime meridian (0° longitude)\n",
    "EQUINOX_PC = 3    # Equinox (+90° longitude)\n",
    "\n",
    "# Simulated hidden state (spherical coordinates)\n",
    "REF_LAT = -7.288   # Latitude (degrees) - pointing at cluster\n",
    "REF_LON = 6.941    # Longitude (degrees)\n",
    "REF_DIST = None    # Distance from origin (gamma units) - None = use cluster mean norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file, save_file\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Device Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Detect available device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Load W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W from ../tensors/Qwen3-4B-Instruct-2507/W.safetensors\n",
      "  Shape: torch.Size([151936, 2560])\n",
      "  Dtype: torch.bfloat16\n",
      "\n",
      "Token space: 151,936 tokens in 2,560 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Load W in bfloat16\n",
    "tensor_path = Path(f\"../tensors/{MODEL_NAME}/W.safetensors\")\n",
    "W_bf16 = load_file(tensor_path)[\"W\"]\n",
    "\n",
    "print(f\"Loaded W from {tensor_path}\")\n",
    "print(f\"  Shape: {W_bf16.shape}\")\n",
    "print(f\"  Dtype: {W_bf16.dtype}\")\n",
    "\n",
    "N, d = W_bf16.shape\n",
    "print(f\"\\nToken space: {N:,} tokens in {d:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Compute PCA Basis\n",
    "\n",
    "We need the PCA basis to convert lat/lon/distance to a 2560D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PCA...\n",
      "  Computing covariance matrix...\n",
      "  Computing eigendecomposition...\n",
      "\n",
      "✓ PCA computed\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing PCA...\")\n",
    "\n",
    "# Work in float32 for PCA (more stable)\n",
    "W = W_bf16.to(torch.float32)\n",
    "\n",
    "# Center the data\n",
    "W_centered = W - W.mean(dim=0)\n",
    "\n",
    "# Compute covariance matrix\n",
    "print(\"  Computing covariance matrix...\")\n",
    "cov = (W_centered.T @ W_centered) / N\n",
    "\n",
    "# Eigendecomposition\n",
    "print(\"  Computing eigendecomposition...\")\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
    "\n",
    "# Sort by descending eigenvalue\n",
    "idx = torch.argsort(eigenvalues, descending=True)\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "print(f\"\\n✓ PCA computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Define Simulated Hidden State\n",
    "\n",
    "Convert lat/lon/distance to a 2560D vector representing our simulated hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spherical coordinate basis:\n",
      "  North (+Z):    PC2\n",
      "  Meridian (+X): PC1\n",
      "  Equinox (+Y):  PC3\n",
      "\n",
      "Direction (unit sphere):\n",
      "  Lat: -7.288°, Lon: 6.941°\n",
      "  Cartesian: x=0.984651, y=0.119871, z=-0.126857\n",
      "\n",
      "Distance: 0.371094 (cluster mean norm)\n",
      "\n",
      "Simulated hidden state in 2560D space:\n",
      "  Norm: 0.371094\n",
      "  Direction: lat=-7.288°, lon=6.941°\n"
     ]
    }
   ],
   "source": [
    "def get_pc_vector(pcs, index):\n",
    "    \"\"\"Get PC vector by index (1-indexed), with sign flip for negative indices.\"\"\"\n",
    "    pc_num = abs(index) - 1\n",
    "    vector = pcs[:, pc_num].clone()\n",
    "    if index < 0:\n",
    "        vector = -vector\n",
    "    return vector\n",
    "\n",
    "\n",
    "# Extract basis vectors\n",
    "north = get_pc_vector(eigenvectors, NORTH_PC)\n",
    "meridian = get_pc_vector(eigenvectors, MERIDIAN_PC)\n",
    "equinox = get_pc_vector(eigenvectors, EQUINOX_PC)\n",
    "\n",
    "print(\"Spherical coordinate basis:\")\n",
    "print(f\"  North (+Z):    PC{NORTH_PC}\")\n",
    "print(f\"  Meridian (+X): PC{MERIDIAN_PC}\")\n",
    "print(f\"  Equinox (+Y):  PC{EQUINOX_PC}\")\n",
    "print()\n",
    "\n",
    "# Convert lat/long to Cartesian coordinates on unit sphere\n",
    "lat_rad = np.deg2rad(REF_LAT)\n",
    "lon_rad = np.deg2rad(REF_LON)\n",
    "\n",
    "# Spherical to Cartesian: (r=1, lat, lon) -> (x, y, z)\n",
    "x = np.cos(lat_rad) * np.cos(lon_rad)  # Meridian component\n",
    "y = np.cos(lat_rad) * np.sin(lon_rad)  # Equinox component\n",
    "z = np.sin(lat_rad)                     # North component\n",
    "\n",
    "print(f\"Direction (unit sphere):\")\n",
    "print(f\"  Lat: {REF_LAT:.3f}°, Lon: {REF_LON:.3f}°\")\n",
    "print(f\"  Cartesian: x={x:.6f}, y={y:.6f}, z={z:.6f}\")\n",
    "print()\n",
    "\n",
    "# Convert to direction in full 2560D space\n",
    "direction = x * meridian + y * equinox + z * north\n",
    "direction = direction / direction.norm()  # Normalize\n",
    "\n",
    "# Determine distance (default to cluster mean norm if not specified)\n",
    "if REF_DIST is None:\n",
    "    # Load cluster membership to get mean norm\n",
    "    cluster_path = Path(f\"../tensors/{MODEL_NAME}/1.4a_cluster_members.safetensors\")\n",
    "    cluster_data = load_file(cluster_path)\n",
    "    cluster_token_ids = cluster_data[\"cluster_token_ids\"].long()\n",
    "    \n",
    "    # Compute mean norm of cluster embeddings\n",
    "    cluster_norms = W_bf16[cluster_token_ids].norm(dim=1)\n",
    "    ref_dist = cluster_norms.mean().item()\n",
    "    print(f\"Distance: {ref_dist:.6f} (cluster mean norm)\")\n",
    "else:\n",
    "    ref_dist = REF_DIST\n",
    "    print(f\"Distance: {ref_dist:.6f} (user specified)\")\n",
    "\n",
    "# Construct hidden state\n",
    "h = direction * ref_dist\n",
    "\n",
    "print(f\"\\nSimulated hidden state in {d}D space:\")\n",
    "print(f\"  Norm: {h.norm().item():.6f}\")\n",
    "print(f\"  Direction: lat={REF_LAT:.3f}°, lon={REF_LON:.3f}°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Compute Dot Products in Bfloat16\n",
    "\n",
    "This is the key step: compute `W @ h` in bfloat16 to match the model's logit computation precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing dot products in bfloat16...\n",
      "\n",
      "✓ Computed dot products for 151,936 tokens\n",
      "\n",
      "Dot product distribution (bfloat16):\n",
      "  Range: [-0.28320312, 0.25781250]\n",
      "  Mean: 0.09179688\n",
      "  Median: 0.09912109\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing dot products in bfloat16...\\n\")\n",
    "\n",
    "# Convert h to bfloat16\n",
    "h_bf16 = h.to(torch.bfloat16).to(device)\n",
    "\n",
    "# Move W to device\n",
    "W_bf16_device = W_bf16.to(device)\n",
    "\n",
    "# Compute dot products in bfloat16\n",
    "with torch.no_grad():\n",
    "    dot_products_bf16 = W_bf16_device @ h_bf16\n",
    "\n",
    "# Move to CPU for analysis\n",
    "dot_products_bf16_cpu = dot_products_bf16.cpu()\n",
    "\n",
    "print(f\"✓ Computed dot products for {N:,} tokens\")\n",
    "print()\n",
    "print(f\"Dot product distribution (bfloat16):\")\n",
    "print(f\"  Range: [{dot_products_bf16_cpu.min():.8f}, {dot_products_bf16_cpu.max():.8f}]\")\n",
    "print(f\"  Mean: {dot_products_bf16_cpu.mean():.8f}\")\n",
    "print(f\"  Median: {dot_products_bf16_cpu.median():.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Find Logit Equivalence Classes\n",
    "\n",
    "Find tokens with **identical dot product values** at bfloat16 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying logit equivalence classes...\n",
      "\n",
      "Mode dot product: 0.11621094\n",
      "Mode count: 3,281 tokens\n",
      "\n",
      "✓ Found 3,281 tokens with dot product = 0.11621094\n",
      "  (2.16% of vocabulary)\n",
      "\n",
      "Token IDs (first 20): [124, 125, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 3402, 4902, 7392, 7550, 9083, 10092, 12813]\n",
      "\n",
      "For comparison:\n",
      "  Maximum dot product: 0.25781250 (1 tokens)\n"
     ]
    }
   ],
   "source": [
    "print(\"Identifying logit equivalence classes...\\n\")\n",
    "\n",
    "# Find the mode (most common dot product value)\n",
    "unique_dots, counts = torch.unique(dot_products_bf16_cpu, return_counts=True)\n",
    "mode_idx = counts.argmax()\n",
    "mode_dot = unique_dots[mode_idx].item()\n",
    "mode_count = counts[mode_idx].item()\n",
    "\n",
    "print(f\"Mode dot product: {mode_dot:.8f}\")\n",
    "print(f\"Mode count: {mode_count:,} tokens\")\n",
    "print()\n",
    "\n",
    "# Find all tokens with this dot product\n",
    "equiv_mask = (dot_products_bf16_cpu == mode_dot)\n",
    "equiv_indices = equiv_mask.nonzero(as_tuple=True)[0]\n",
    "n_equiv = equiv_indices.numel()\n",
    "\n",
    "print(f\"✓ Found {n_equiv:,} tokens with dot product = {mode_dot:.8f}\")\n",
    "print(f\"  ({n_equiv / N * 100:.2f}% of vocabulary)\")\n",
    "print()\n",
    "print(f\"Token IDs (first 20): {equiv_indices[:20].tolist()}\")\n",
    "print()\n",
    "\n",
    "# Also report maximum dot product for comparison\n",
    "max_dot = dot_products_bf16_cpu.max().item()\n",
    "max_count = (dot_products_bf16_cpu == max_dot).sum().item()\n",
    "print(f\"For comparison:\")\n",
    "print(f\"  Maximum dot product: {max_dot:.8f} ({max_count:,} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Distribution of Unique Dot Products\n",
    "\n",
    "How many unique dot product values exist at bfloat16 precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing unique dot product values...\n",
      "\n",
      "Total unique dot product values: 2,033\n",
      "  (1.34% of vocabulary)\n",
      "\n",
      "Top 10 most common dot product values:\n",
      "  1. dot=0.11621094: 3,281 tokens\n",
      "  2. dot=0.12597656: 1,410 tokens\n",
      "  3. dot=0.12792969: 1,395 tokens\n",
      "  4. dot=0.12695312: 1,325 tokens\n",
      "  5. dot=0.12890625: 1,174 tokens\n",
      "  6. dot=0.12988281: 1,174 tokens\n",
      "  7. dot=0.11279297: 1,173 tokens\n",
      "  8. dot=0.11132812: 1,145 tokens\n",
      "  9. dot=0.13085938: 1,140 tokens\n",
      "  10. dot=0.11083984: 1,126 tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"Analyzing unique dot product values...\\n\")\n",
    "\n",
    "unique_dots, counts = torch.unique(dot_products_bf16_cpu, return_counts=True)\n",
    "n_unique = unique_dots.numel()\n",
    "\n",
    "print(f\"Total unique dot product values: {n_unique:,}\")\n",
    "print(f\"  ({n_unique / N * 100:.2f}% of vocabulary)\")\n",
    "print()\n",
    "\n",
    "# Sort by count (descending)\n",
    "sorted_indices = torch.argsort(counts, descending=True)\n",
    "top_dots = unique_dots[sorted_indices[:10]]\n",
    "top_counts = counts[sorted_indices[:10]]\n",
    "\n",
    "print(f\"Top 10 most common dot product values:\")\n",
    "for i in range(10):\n",
    "    dot_val = top_dots[i].item()\n",
    "    count = top_counts[i].item()\n",
    "    print(f\"  {i+1}. dot={dot_val:.8f}: {count:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Check for Embedding-Level Duplicates\n",
    "\n",
    "Do these tokens with identical dot products also have **identical embeddings**?\n",
    "\n",
    "Or are some truly distinct embeddings that just happen to round to the same dot product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for embedding-level duplicates...\n",
      "\n",
      "Tokens in equivalence class: 3,281\n",
      "Unique embeddings: 1,194\n",
      "\n",
      "✓ Found duplicates: 2,087 tokens share embeddings with others\n",
      "  Average degeneracy: 2.7 tokens per unique embedding\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for embedding-level duplicates...\\n\")\n",
    "\n",
    "# Extract equivalence class embeddings\n",
    "W_equiv = W_bf16[equiv_indices]\n",
    "\n",
    "# Find unique embeddings using torch (works with bfloat16)\n",
    "# torch.unique with dim=0 finds unique rows (unique 2560D embeddings)\n",
    "unique_embeddings = torch.unique(W_equiv, dim=0)\n",
    "n_unique_embeddings = len(unique_embeddings)\n",
    "\n",
    "print(f\"Tokens in equivalence class: {n_equiv:,}\")\n",
    "print(f\"Unique embeddings: {n_unique_embeddings:,}\")\n",
    "print()\n",
    "\n",
    "if n_unique_embeddings < n_equiv:\n",
    "    print(f\"✓ Found duplicates: {n_equiv - n_unique_embeddings:,} tokens share embeddings with others\")\n",
    "    print(f\"  Average degeneracy: {n_equiv / n_unique_embeddings:.1f} tokens per unique embedding\")\n",
    "else:\n",
    "    print(\"✓ No embedding duplicates: all tokens have distinct embeddings\")\n",
    "    print(\"  These tokens are indistinguishable by DOT PRODUCT but not by EMBEDDING\")\n",
    "    print(\"  This is pure bfloat16 quantization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Comparison with Cosine-Based Cluster\n",
    "\n",
    "How does this logit-based equivalence class compare to the cosine-based cluster from 1.4a?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing with cosine-based cluster (1.4a)...\n",
      "\n",
      "Cosine-based cluster (1.4a): 2,251 tokens\n",
      "Dot-product-based equivalence (1.5a): 3,281 tokens\n",
      "Overlap: 2,227 tokens\n",
      "\n",
      "✓ Partial overlap: 2,227 tokens in both\n",
      "  Cosine-only: 24 tokens\n",
      "  Dot-product-only: 1,054 tokens\n",
      "  These are different geometric structures!\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparing with cosine-based cluster (1.4a)...\\n\")\n",
    "\n",
    "# Load cosine cluster\n",
    "cluster_path = Path(f\"../tensors/{MODEL_NAME}/1.4a_cluster_members.safetensors\")\n",
    "cluster_data = load_file(cluster_path)\n",
    "cluster_mask = cluster_data[\"cluster_mask\"].bool()\n",
    "n_cluster = cluster_data[\"n_cluster_members\"].item()\n",
    "\n",
    "# Compare overlap\n",
    "overlap_mask = equiv_mask & cluster_mask\n",
    "n_overlap = overlap_mask.sum().item()\n",
    "\n",
    "print(f\"Cosine-based cluster (1.4a): {n_cluster:,} tokens\")\n",
    "print(f\"Dot-product-based equivalence (1.5a): {n_equiv:,} tokens\")\n",
    "print(f\"Overlap: {n_overlap:,} tokens\")\n",
    "print()\n",
    "\n",
    "if n_overlap == n_equiv and n_overlap == n_cluster:\n",
    "    print(\"✓ Perfect match: cosine and dot product identify the same tokens\")\n",
    "elif n_overlap == min(n_equiv, n_cluster):\n",
    "    if n_equiv > n_cluster:\n",
    "        print(f\"✓ Dot-product class is LARGER: includes {n_equiv - n_cluster:,} additional tokens\")\n",
    "        print(\"  These tokens have different directions but same dot product (varying magnitudes)\")\n",
    "    else:\n",
    "        print(f\"✓ Cosine class is LARGER: includes {n_cluster - n_equiv:,} additional tokens\")\n",
    "        print(\"  These tokens have same direction but different magnitudes (different dot products)\")\n",
    "else:\n",
    "    print(f\"✓ Partial overlap: {n_overlap:,} tokens in both\")\n",
    "    print(f\"  Cosine-only: {n_cluster - n_overlap:,} tokens\")\n",
    "    print(f\"  Dot-product-only: {n_equiv - n_overlap:,} tokens\")\n",
    "    print(\"  These are different geometric structures!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving results...\n",
      "\n",
      "✓ Saved logit equivalence class to ../tensors/Qwen3-4B-Instruct-2507/1.5a_logit_equivalence.safetensors\n",
      "\n",
      "Saved tensors:\n",
      "  equiv_mask: torch.Size([151936]) - binary mask (1 = equivalence class member)\n",
      "  equiv_token_ids: torch.Size([3281]) - indices of equivalence class members\n",
      "  equiv_dot_product: scalar - shared dot product value (0.11621094)\n",
      "  n_equiv_members: scalar - count (3,281)\n",
      "  ref_lat, ref_lon, ref_dist: scalars - simulated hidden state\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving results...\\n\")\n",
    "\n",
    "# Save equivalence class membership\n",
    "output_path = Path(f\"../tensors/{MODEL_NAME}/1.5a_logit_equivalence.safetensors\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_file({\n",
    "    \"equiv_mask\": equiv_mask.to(torch.uint8),\n",
    "    \"equiv_token_ids\": equiv_indices.to(torch.int32),\n",
    "    \"equiv_dot_product\": torch.tensor([mode_dot], dtype=torch.float32),\n",
    "    \"n_equiv_members\": torch.tensor([n_equiv], dtype=torch.int32),\n",
    "    \"ref_lat\": torch.tensor([REF_LAT], dtype=torch.float32),\n",
    "    \"ref_lon\": torch.tensor([REF_LON], dtype=torch.float32),\n",
    "    \"ref_dist\": torch.tensor([ref_dist], dtype=torch.float32),\n",
    "}, str(output_path))\n",
    "\n",
    "print(f\"✓ Saved logit equivalence class to {output_path}\")\n",
    "print()\n",
    "print(\"Saved tensors:\")\n",
    "print(f\"  equiv_mask: {equiv_mask.shape} - binary mask (1 = equivalence class member)\")\n",
    "print(f\"  equiv_token_ids: {equiv_indices.shape} - indices of equivalence class members\")\n",
    "print(f\"  equiv_dot_product: scalar - shared dot product value ({mode_dot:.8f})\")\n",
    "print(f\"  n_equiv_members: scalar - count ({n_equiv:,})\")\n",
    "print(f\"  ref_lat, ref_lon, ref_dist: scalars - simulated hidden state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook identified tokens with **identical logit scores** for a given hidden state due to bfloat16 quantization.\n",
    "\n",
    "**Key findings:**\n",
    "- Hidden state: lat={REF_LAT:.3f}°, lon={REF_LON:.3f}°, dist={ref_dist:.3f}\n",
    "- Equivalence class dot product: (see output above)\n",
    "- Equivalence class size: (see output above)\n",
    "\n",
    "These tokens are **indistinguishable by the model** during next-token prediction—they receive identical logit scores and thus identical probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
