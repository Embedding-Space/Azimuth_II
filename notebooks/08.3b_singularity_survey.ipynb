{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 08.3b: Singularity Survey for Trained Embeddings\n",
    "\n",
    "**Search embedding matrix for singularities (bit-for-bit identical vectors)**\n",
    "\n",
    "After training, we expect:\n",
    "- Tokens that appeared in training: unique vectors (moved by gradients)\n",
    "- Dead tokens (never appeared): collapsed to 1-5 unique vectors (frozen at initialization ± quantization noise)\n",
    "\n",
    "This notebook uses `torch.unique()` for O(N log N) hash-based deduplication to find groups of tokens sharing identical vectors.\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: embedding matrix to analyze\n",
    "TENSOR_DIR = \"../data/embeddings_128vocab_qweninit\"\n",
    "EMBEDDING_FILE = \"step_0005000.safetensors\"\n",
    "EMBEDDING_KEY = \"embeddings\"\n",
    "\n",
    "# Display options\n",
    "MAX_TOKENS_PER_GROUP = 20  # Limit display for very large singularity groups\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: ../data/embeddings_128vocab_qweninit/step_0005000.safetensors\n",
      "\n",
      "✓ Embeddings loaded\n",
      "Shape: torch.Size([128, 64])\n",
      "Vocabulary size: 128\n",
      "Hidden dimension: 64\n",
      "Total parameters: 8,192\n",
      "Memory footprint: 0.03 MB\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(TENSOR_DIR)\n",
    "embedding_path = data_dir / EMBEDDING_FILE\n",
    "\n",
    "print(f\"Loading embeddings from: {embedding_path}\\n\")\n",
    "\n",
    "data = load_file(embedding_path)\n",
    "gamma = data[EMBEDDING_KEY]\n",
    "vocab_size, hidden_dim = gamma.shape\n",
    "\n",
    "print(f\"✓ Embeddings loaded\")\n",
    "print(f\"Shape: {gamma.shape}\")\n",
    "print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"Hidden dimension: {hidden_dim:,}\")\n",
    "print(f\"Total parameters: {vocab_size * hidden_dim:,}\")\n",
    "print(f\"Memory footprint: {gamma.element_size() * gamma.numel() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Find Singularities\n",
    "\n",
    "Use `torch.unique()` to find all unique vectors and identify groups of tokens that share identical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching for singularities...\n",
      "\n",
      "Total tokens: 128\n",
      "Unique vectors: 78\n",
      "Duplicate tokens: 50\n",
      "Uniqueness: 60.94%\n",
      "\n",
      "⚠ Found 50 duplicate tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSearching for singularities...\\n\")\n",
    "\n",
    "# Find unique vectors\n",
    "unique_vectors, inverse_indices, counts = torch.unique(\n",
    "    gamma,\n",
    "    dim=0,\n",
    "    return_inverse=True,\n",
    "    return_counts=True\n",
    ")\n",
    "\n",
    "n_unique = len(unique_vectors)\n",
    "n_total = vocab_size\n",
    "n_duplicate = n_total - n_unique\n",
    "\n",
    "print(f\"Total tokens: {n_total:,}\")\n",
    "print(f\"Unique vectors: {n_unique:,}\")\n",
    "print(f\"Duplicate tokens: {n_duplicate:,}\")\n",
    "print(f\"Uniqueness: {100 * n_unique / n_total:.2f}%\\n\")\n",
    "\n",
    "if n_duplicate == 0:\n",
    "    print(\"✓ No singularities found. Every token has a unique vector.\")\n",
    "else:\n",
    "    print(f\"⚠ Found {n_duplicate:,} duplicate tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Analyze Singularity Groups\n",
    "\n",
    "Group tokens by their shared vector and report statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Singularity groups: 1\n",
      "Largest group: 51 tokens\n",
      "Smallest group: 51 tokens\n",
      "Mean group size: 51.0 tokens\n",
      "Median group size: 51.0 tokens\n",
      "\n",
      "Group size distribution:\n",
      "    51 tokens:    1 groups\n"
     ]
    }
   ],
   "source": [
    "if n_duplicate > 0:\n",
    "    # Build map from unique vector index to list of token IDs\n",
    "    singularity_groups = defaultdict(list)\n",
    "    \n",
    "    for token_id, unique_idx in enumerate(inverse_indices.tolist()):\n",
    "        if counts[unique_idx] > 1:  # Only include vectors shared by 2+ tokens\n",
    "            singularity_groups[unique_idx].append(token_id)\n",
    "    \n",
    "    n_groups = len(singularity_groups)\n",
    "    group_sizes = [len(tokens) for tokens in singularity_groups.values()]\n",
    "    \n",
    "    print(f\"\\nSingularity groups: {n_groups:,}\")\n",
    "    print(f\"Largest group: {max(group_sizes):,} tokens\")\n",
    "    print(f\"Smallest group: {min(group_sizes):,} tokens\")\n",
    "    print(f\"Mean group size: {np.mean(group_sizes):.1f} tokens\")\n",
    "    print(f\"Median group size: {np.median(group_sizes):.1f} tokens\")\n",
    "    \n",
    "    # Histogram of group sizes\n",
    "    size_counts = defaultdict(int)\n",
    "    for size in group_sizes:\n",
    "        size_counts[size] += 1\n",
    "    \n",
    "    print(\"\\nGroup size distribution:\")\n",
    "    for size in sorted(size_counts.keys()):\n",
    "        count = size_counts[size]\n",
    "        print(f\"  {size:4d} tokens: {count:4d} groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Display Singularity Token IDs\n",
    "\n",
    "Show which token IDs are in each singularity group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Displaying up to 20 tokens per group\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Group 1/1: 51 tokens sharing vector #67\n",
      "--------------------------------------------------------------------------------\n",
      "  Token IDs (byte values): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "  Characters: ['\\\\x00', '\\\\x01', '\\\\x02', '\\\\x03', '\\\\x04', '\\\\x05', '\\\\x06', '\\\\x07', '\\\\x08', '\\\\x09', '\\\\x0b', '\\\\x0c', '\\\\x0d', '\\\\x0e', '\\\\x0f', '\\\\x10', '\\\\x11', '\\\\x12', '\\\\x13', '\\\\x14']\n",
      "  ... and 31 more tokens\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if n_duplicate > 0:\n",
    "    # Sort groups by size (largest first)\n",
    "    sorted_groups = sorted(\n",
    "        singularity_groups.items(),\n",
    "        key=lambda x: len(x[1]),\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDisplaying up to {MAX_TOKENS_PER_GROUP} tokens per group\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for group_idx, (unique_idx, token_ids) in enumerate(sorted_groups, 1):\n",
    "        n_tokens = len(token_ids)\n",
    "        print(f\"\\nGroup {group_idx}/{n_groups}: {n_tokens} tokens sharing vector #{unique_idx}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Show first N tokens\n",
    "        display_tokens = token_ids[:MAX_TOKENS_PER_GROUP]\n",
    "        \n",
    "        # Show as byte values and ASCII chars where printable\n",
    "        print(f\"  Token IDs (byte values): {display_tokens}\")\n",
    "        \n",
    "        # Show ASCII characters\n",
    "        chars = []\n",
    "        for token_id in display_tokens:\n",
    "            if 32 <= token_id < 127:  # Printable ASCII\n",
    "                chars.append(chr(token_id))\n",
    "            else:\n",
    "                chars.append(f\"\\\\x{token_id:02x}\")\n",
    "        \n",
    "        print(f\"  Characters: {chars}\")\n",
    "        \n",
    "        if n_tokens > MAX_TOKENS_PER_GROUP:\n",
    "            print(f\"  ... and {n_tokens - MAX_TOKENS_PER_GROUP} more tokens\")\n",
    "        \n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Compute Black Hole Geometry\n",
    "\n",
    "If singularities exist, compute their geometric properties:\n",
    "- L2 norms (distance from origin)\n",
    "- Pairwise distances (how spread out are they?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BLACK HOLE GEOMETRY\n",
      "================================================================================\n",
      "\n",
      "Black hole L2 norms:\n",
      "  Min: 7.529632\n",
      "  Max: 7.529632\n",
      "  Mean: 7.529632\n",
      "  Std: nan\n",
      "\n",
      "Comparison to full cloud:\n",
      "  Full cloud L2 norms: mean=7.749794, std=0.274956\n",
      "  Centroid L2 norm: 7.731306\n",
      "  Black holes vs full cloud: 0.9716× mean norm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k5/44vd1ct56xj4y9h7x213kvjr0000gn/T/ipykernel_48189/3219163171.py:16: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  print(f\"  Std: {norms.std().item():.6f}\")\n"
     ]
    }
   ],
   "source": [
    "if n_duplicate > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BLACK HOLE GEOMETRY\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Get one representative from each singularity group\n",
    "    black_hole_vectors = unique_vectors[counts > 1]\n",
    "    \n",
    "    # Compute L2 norms\n",
    "    norms = torch.norm(black_hole_vectors, p=2, dim=1)\n",
    "    \n",
    "    print(f\"Black hole L2 norms:\")\n",
    "    print(f\"  Min: {norms.min().item():.6f}\")\n",
    "    print(f\"  Max: {norms.max().item():.6f}\")\n",
    "    print(f\"  Mean: {norms.mean().item():.6f}\")\n",
    "    print(f\"  Std: {norms.std().item():.6f}\")\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    if len(black_hole_vectors) > 1:\n",
    "        v1 = black_hole_vectors.unsqueeze(1)  # (n, 1, d)\n",
    "        v2 = black_hole_vectors.unsqueeze(0)  # (1, n, d)\n",
    "        diffs = v1 - v2  # (n, n, d)\n",
    "        \n",
    "        # L2 distances\n",
    "        l2_distances = torch.norm(diffs, p=2, dim=2)\n",
    "        \n",
    "        # Mask out diagonal\n",
    "        mask = ~torch.eye(len(black_hole_vectors), dtype=torch.bool)\n",
    "        l2_nonzero = l2_distances[mask]\n",
    "        \n",
    "        print(f\"\\nPairwise L2 distances between black holes:\")\n",
    "        print(f\"  Min: {l2_nonzero.min().item():.6e}\")\n",
    "        print(f\"  Max: {l2_nonzero.max().item():.6e}\")\n",
    "        print(f\"  Mean: {l2_nonzero.mean().item():.6e}\")\n",
    "        print(f\"  Median: {l2_nonzero.median().item():.6e}\")\n",
    "    \n",
    "    # Compare to full cloud\n",
    "    full_norms = torch.norm(gamma, p=2, dim=1)\n",
    "    centroid = gamma.mean(dim=0)\n",
    "    centroid_norm = centroid.norm().item()\n",
    "    \n",
    "    print(f\"\\nComparison to full cloud:\")\n",
    "    print(f\"  Full cloud L2 norms: mean={full_norms.mean().item():.6f}, std={full_norms.std().item():.6f}\")\n",
    "    print(f\"  Centroid L2 norm: {centroid_norm:.6f}\")\n",
    "    print(f\"  Black holes vs full cloud: {norms.mean().item() / full_norms.mean().item():.4f}× mean norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Embedding file: step_0005000.safetensors\n",
      "Vocabulary size: 128\n",
      "Unique vectors: 78\n",
      "Duplicate tokens: 50 (39.06%)\n",
      "Singularity groups: 1\n",
      "Largest singularity: 51 tokens\n",
      "Deduplication ratio: 1.64x\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Embedding file: {EMBEDDING_FILE}\")\n",
    "print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"Unique vectors: {n_unique:,}\")\n",
    "print(f\"Duplicate tokens: {n_duplicate:,} ({100 * n_duplicate / n_total:.2f}%)\")\n",
    "\n",
    "if n_duplicate > 0:\n",
    "    print(f\"Singularity groups: {n_groups:,}\")\n",
    "    print(f\"Largest singularity: {max(group_sizes):,} tokens\")\n",
    "    print(f\"Deduplication ratio: {n_total / n_unique:.2f}x\")\n",
    "else:\n",
    "    print(\"No singularities detected.\")\n",
    "\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
