{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 12.2c: Generate Synthetic Snowball Dataset\n",
    "\n",
    "**Goal:** Generate 1,000 synthetic snowballs at σ = 1.5×10⁻⁹ and save to disk for reusable analysis.\n",
    "\n",
    "## Rationale\n",
    "\n",
    "Generate once, analyze many ways:\n",
    "- Topology analysis (12.3c)\n",
    "- Population statistics (12.3b)\n",
    "- Future questions we haven't thought of yet\n",
    "\n",
    "This notebook streams data directly to disk using HDF5, keeping peak RAM usage at ~2.6 GB instead of 10+ GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "SIGMA = 1.5e-9         # Initialization noise scale\n",
    "N_TRIALS = 10000       # Number of independent snowballs to generate\n",
    "N_TOKENS = 2100        # Qwen's dead token count\n",
    "HIDDEN_DIM = 2560      # Qwen's embedding dimension\n",
    "BATCH_SIZE = 256       # Trials per batch (memory constraint)\n",
    "\n",
    "# Output\n",
    "OUTPUT_FILE = \"../data/tensors/synthetic_snowballs_n10000_sigma1.5e-9.h5\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MPS (Metal Performance Shaders) available\n",
      "  Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "from safetensors.torch import load_file\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Setup device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"✓ MPS (Metal Performance Shaders) available\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"⚠ MPS not available, using CPU only\")\n",
    "\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Load Qwen Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Qwen black hole centroid...\n",
      "\n",
      "✓ Centroid loaded to mps\n",
      "  Shape: torch.Size([2560])\n",
      "  Norm: 0.166061\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading Qwen black hole centroid...\\n\")\n",
    "\n",
    "centroid_data = load_file(\"../data/tensors/black_hole_centroid_qwen3_4b.safetensors\")\n",
    "qwen_centroid = centroid_data['centroid'].to(torch.float32).to(device)\n",
    "\n",
    "print(f\"✓ Centroid loaded to {device}\")\n",
    "print(f\"  Shape: {qwen_centroid.shape}\")\n",
    "print(f\"  Norm: {qwen_centroid.norm().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Create HDF5 File and Stream Snowballs to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 10,000 synthetic snowballs (streaming to disk)...\n",
      "  σ = 1.50e-09\n",
      "  Batch size: 256\n",
      "  Shape per trial: [2100, 2560]\n",
      "  Total shape: [10000, 2100, 2560]\n",
      "  Total dataset size: 102539.1 MB (100.14 GB)\n",
      "  Peak RAM usage: ~5250.0 MB (one batch in float32)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3602669166d4444fbc4f2ae4d606475a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating & writing batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Generated and saved 10,000 snowballs in 217.2s\n",
      "  File: ../data/tensors/synthetic_snowballs_n1000_sigma1.5e-9.h5\n",
      "  Size: 3970.2 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nGenerating {N_TRIALS:,} synthetic snowballs (streaming to disk)...\")\n",
    "print(f\"  σ = {SIGMA:.2e}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Shape per trial: [{N_TOKENS}, {HIDDEN_DIM}]\")\n",
    "print(f\"  Total shape: [{N_TRIALS}, {N_TOKENS}, {HIDDEN_DIM}]\")\n",
    "\n",
    "# Calculate memory\n",
    "total_elements = N_TRIALS * N_TOKENS * HIDDEN_DIM\n",
    "total_mb = total_elements * 2 / (1024 * 1024)  # float16 = 2 bytes\n",
    "print(f\"  Total dataset size: {total_mb:.1f} MB ({total_mb/1024:.2f} GB)\")\n",
    "print(f\"  Peak RAM usage: ~{BATCH_SIZE * N_TOKENS * HIDDEN_DIM * 4 / (1024**2):.1f} MB (one batch in float32)\\n\")\n",
    "\n",
    "output_path = Path(OUTPUT_FILE)\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "n_batches = (N_TRIALS + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create HDF5 file with chunked dataset (allows incremental writes)\n",
    "with h5py.File(str(output_path), 'w') as f:\n",
    "    # Create dataset with chunking (one batch per chunk for efficient I/O)\n",
    "    dataset = f.create_dataset(\n",
    "        'embeddings',\n",
    "        shape=(N_TRIALS, N_TOKENS, HIDDEN_DIM),\n",
    "        dtype='float16',\n",
    "        chunks=(BATCH_SIZE, N_TOKENS, HIDDEN_DIM),\n",
    "        compression='gzip',\n",
    "        compression_opts=1  # Light compression for speed\n",
    "    )\n",
    "    \n",
    "    # Save centroid as separate dataset\n",
    "    f.create_dataset('centroid', data=qwen_centroid.cpu().numpy().astype('float32'))\n",
    "    \n",
    "    # Save metadata as attributes\n",
    "    f.attrs['sigma'] = SIGMA\n",
    "    f.attrs['n_trials'] = N_TRIALS\n",
    "    f.attrs['n_tokens'] = N_TOKENS\n",
    "    f.attrs['hidden_dim'] = HIDDEN_DIM\n",
    "    f.attrs['random_seed'] = RANDOM_SEED\n",
    "    f.attrs['description'] = 'Synthetic snowballs: centroid + Gaussian(0, sigma) quantized to bfloat16'\n",
    "    \n",
    "    # Generate and write batches\n",
    "    for batch_idx in tqdm(range(n_batches), desc=\"Generating & writing batches\"):\n",
    "        # Determine batch size (last batch might be smaller)\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = min(start_idx + BATCH_SIZE, N_TRIALS)\n",
    "        current_batch_size = end_idx - start_idx\n",
    "        \n",
    "        # Generate batch on MPS\n",
    "        noise = torch.randn(current_batch_size, N_TOKENS, HIDDEN_DIM, dtype=torch.float32, device=device) * SIGMA\n",
    "        embeddings_batch = qwen_centroid.unsqueeze(0).unsqueeze(0) + noise\n",
    "        \n",
    "        # Quantize to bfloat16 then back to float32 (simulates bfloat16 storage)\n",
    "        embeddings_batch = embeddings_batch.to(torch.bfloat16).to(torch.float32)\n",
    "        \n",
    "        # Write directly to disk (no RAM accumulation!)\n",
    "        dataset[start_idx:end_idx] = embeddings_batch.cpu().numpy().astype('float16')\n",
    "        \n",
    "        # Cleanup\n",
    "        del noise, embeddings_batch\n",
    "        gc.collect()\n",
    "\n",
    "gen_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Generated and saved {N_TRIALS:,} snowballs in {gen_time:.1f}s\")\n",
    "print(f\"  File: {output_path}\")\n",
    "print(f\"  Size: {output_path.stat().st_size / (1024**2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying saved file...\n",
      "\n",
      "✓ File loads successfully\n",
      "  Datasets: ['centroid', 'embeddings']\n",
      "  Embeddings shape: (10000, 2100, 2560)\n",
      "  Embeddings dtype: float16\n",
      "  Centroid shape: (2560,)\n",
      "\n",
      "Metadata:\n",
      "  description: Synthetic snowballs: centroid + Gaussian(0, sigma) quantized to bfloat16\n",
      "  hidden_dim: 2560\n",
      "  n_tokens: 2100\n",
      "  n_trials: 10000\n",
      "  random_seed: 42\n",
      "  sigma: 1.5e-09\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nVerifying saved file...\")\n",
    "\n",
    "with h5py.File(str(output_path), 'r') as f:\n",
    "    print(f\"\\n✓ File loads successfully\")\n",
    "    print(f\"  Datasets: {list(f.keys())}\")\n",
    "    print(f\"  Embeddings shape: {f['embeddings'].shape}\")\n",
    "    print(f\"  Embeddings dtype: {f['embeddings'].dtype}\")\n",
    "    print(f\"  Centroid shape: {f['centroid'].shape}\")\n",
    "    print(f\"\\nMetadata:\")\n",
    "    for key, value in f.attrs.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity check on first trial...\n",
      "\n",
      "Trial 0 statistics:\n",
      "  Total tokens: 2100\n",
      "  Unique vectors: 10\n",
      "  Black holes (C): 10\n",
      "  Black hole population (P): 2100\n",
      "  Singletons: 0\n",
      "\n",
      "✓ Results look reasonable (matches expected ranges)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSanity check on first trial...\")\n",
    "\n",
    "with h5py.File(str(output_path), 'r') as f:\n",
    "    # Load just first trial (efficient, doesn't load whole dataset!)\n",
    "    trial_0 = torch.from_numpy(f['embeddings'][0]).to(torch.float32)\n",
    "\n",
    "# Find unique vectors\n",
    "unique_vectors, inverse_indices, counts = torch.unique(\n",
    "    trial_0,\n",
    "    dim=0,\n",
    "    return_inverse=True,\n",
    "    return_counts=True\n",
    ")\n",
    "\n",
    "# Black holes\n",
    "black_hole_mask = counts >= 2\n",
    "n_black_holes = black_hole_mask.sum().item()\n",
    "black_hole_population = counts[black_hole_mask].sum().item()\n",
    "n_unique = len(unique_vectors)\n",
    "n_singletons = n_unique - n_black_holes\n",
    "\n",
    "print(f\"\\nTrial 0 statistics:\")\n",
    "print(f\"  Total tokens: {len(trial_0)}\")\n",
    "print(f\"  Unique vectors: {n_unique}\")\n",
    "print(f\"  Black holes (C): {n_black_holes}\")\n",
    "print(f\"  Black hole population (P): {black_hole_population}\")\n",
    "print(f\"  Singletons: {n_singletons}\")\n",
    "\n",
    "if 10 <= n_black_holes <= 17 and 2090 <= black_hole_population <= 2100:\n",
    "    print(f\"\\n✓ Results look reasonable (matches expected ranges)\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Results outside expected ranges - check generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET GENERATION COMPLETE\n",
      "============================================================\n",
      "Trials: 10,000\n",
      "Tokens per trial: 2,100\n",
      "Dimensions: 2,560\n",
      "σ = 1.50e-09\n",
      "\n",
      "File: ../data/tensors/synthetic_snowballs_n1000_sigma1.5e-9.h5\n",
      "Size: 3970.2 MB (3.88 GB)\n",
      "\n",
      "Generation time: 217.2s\n",
      "\n",
      "Usage in other notebooks:\n",
      "  import h5py\n",
      "  import torch\n",
      "  with h5py.File('../data/tensors/synthetic_snowballs_n1000_sigma1.5e-9.h5', 'r') as f:\n",
      "      trial_42 = torch.from_numpy(f['embeddings'][42])\n",
      "      subset = torch.from_numpy(f['embeddings'][:100])\n",
      "      centroid = torch.from_numpy(f['centroid'][:])\n",
      "      sigma = f.attrs['sigma']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DATASET GENERATION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Trials: {N_TRIALS:,}\")\n",
    "print(f\"Tokens per trial: {N_TOKENS:,}\")\n",
    "print(f\"Dimensions: {HIDDEN_DIM:,}\")\n",
    "print(f\"σ = {SIGMA:.2e}\")\n",
    "print(f\"\\nFile: {output_path}\")\n",
    "print(f\"Size: {file_size_mb:.1f} MB ({file_size_mb/1024:.2f} GB)\")\n",
    "print(f\"\\nGeneration time: {gen_time:.1f}s\")\n",
    "print(f\"\\nUsage in other notebooks:\")\n",
    "print(f\"  import h5py\")\n",
    "print(f\"  import torch\")\n",
    "print(f\"  with h5py.File('{OUTPUT_FILE}', 'r') as f:\")\n",
    "print(f\"      trial_42 = torch.from_numpy(f['embeddings'][42])\")\n",
    "print(f\"      subset = torch.from_numpy(f['embeddings'][:100])\")\n",
    "print(f\"      centroid = torch.from_numpy(f['centroid'][:])\")\n",
    "print(f\"      sigma = f.attrs['sigma']\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
