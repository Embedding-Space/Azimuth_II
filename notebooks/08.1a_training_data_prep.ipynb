{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 08.1a: Training Data Preparation\n",
    "\n",
    "**Download and prepare ASCII training corpus for embedding evolution experiment**\n",
    "\n",
    "We're testing two hypotheses about embedding matrix evolution during training:\n",
    "\n",
    "1. **Normal initialization**: Soft explosion, cloud grows, centroid random-walks\n",
    "2. **Qwen initialization**: Violent explosion from singular point, dead tokens left as black holes\n",
    "\n",
    "To test this, we'll train a tiny transformer (256-token byte-level vocab, 64-dim hidden space) on pure ASCII text. This guarantees ~50% dead tokens (bytes 128-255 never appear in training).\n",
    "\n",
    "## Corpus Choice\n",
    "\n",
    "**The Great Gatsby** by F. Scott Fitzgerald\n",
    "- Public domain (published 1925)\n",
    "- ~250 KB of text\n",
    "- Rich vocabulary and varied sentence structure\n",
    "- Available from Project Gutenberg\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "GUTENBERG_URL = \"https://www.gutenberg.org/cache/epub/64317/pg64317.txt\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_PATH = \"../data/training_corpus.txt\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Download Gatsby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from Project Gutenberg...\n",
      "\n",
      "URL: https://www.gutenberg.org/cache/epub/64317/pg64317.txt\n",
      "\n",
      "✓ Downloaded successfully\n",
      "Raw size: 296,858 characters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Downloading from Project Gutenberg...\\n\")\n",
    "print(f\"URL: {GUTENBERG_URL}\")\n",
    "\n",
    "response = requests.get(GUTENBERG_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "raw_text = response.text\n",
    "\n",
    "print(f\"\\n✓ Downloaded successfully\")\n",
    "print(f\"Raw size: {len(raw_text):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Strip Gutenberg Header/Footer\n",
    "\n",
    "Project Gutenberg files include licensing headers and footers. We'll remove them to get just the novel text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripped header/footer\n",
      "Clean text size: 277,090 characters\n",
      "Removed: 19,768 characters\n"
     ]
    }
   ],
   "source": [
    "# Find start marker (typical Gutenberg format)\n",
    "start_markers = [\n",
    "    \"*** START OF THE PROJECT GUTENBERG EBOOK\",\n",
    "    \"*** START OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "    \"***START OF THE PROJECT GUTENBERG EBOOK\"\n",
    "]\n",
    "\n",
    "start_idx = 0\n",
    "for marker in start_markers:\n",
    "    idx = raw_text.find(marker)\n",
    "    if idx != -1:\n",
    "        # Skip past the marker line\n",
    "        start_idx = raw_text.find('\\n', idx) + 1\n",
    "        break\n",
    "\n",
    "# Find end marker\n",
    "end_markers = [\n",
    "    \"*** END OF THE PROJECT GUTENBERG EBOOK\",\n",
    "    \"*** END OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "    \"***END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "]\n",
    "\n",
    "end_idx = len(raw_text)\n",
    "for marker in end_markers:\n",
    "    idx = raw_text.find(marker)\n",
    "    if idx != -1:\n",
    "        end_idx = idx\n",
    "        break\n",
    "\n",
    "# Extract just the novel\n",
    "clean_text = raw_text[start_idx:end_idx].strip()\n",
    "\n",
    "print(f\"Stripped header/footer\")\n",
    "print(f\"Clean text size: {len(clean_text):,} characters\")\n",
    "print(f\"Removed: {len(raw_text) - len(clean_text):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Verify ASCII Purity\n",
    "\n",
    "Check that all characters are valid ASCII (bytes 0-127). If not, we'll convert or strip non-ASCII chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bytes: 286,640\n",
      "Non-ASCII bytes: 14,335\n",
      "\n",
      "Non-ASCII byte values: {128, 226, 195, 166, 167, 169, 138, 170, 148, 180, 152, 153, 156, 157}\n",
      "\n",
      "⚠ Text contains non-ASCII characters\n",
      "Converting to pure ASCII...\n",
      "✓ Converted to pure ASCII\n",
      "Final size: 272,305 bytes\n"
     ]
    }
   ],
   "source": [
    "# Convert to bytes and check\n",
    "text_bytes = clean_text.encode('utf-8', errors='replace')\n",
    "\n",
    "# Find non-ASCII bytes\n",
    "non_ascii = [b for b in text_bytes if b > 127]\n",
    "\n",
    "print(f\"Total bytes: {len(text_bytes):,}\")\n",
    "print(f\"Non-ASCII bytes: {len(non_ascii):,}\")\n",
    "\n",
    "if non_ascii:\n",
    "    print(f\"\\nNon-ASCII byte values: {set(non_ascii)}\")\n",
    "    print(f\"\\n⚠ Text contains non-ASCII characters\")\n",
    "    print(f\"Converting to pure ASCII...\")\n",
    "    \n",
    "    # Strip non-ASCII\n",
    "    ascii_text = clean_text.encode('ascii', errors='ignore').decode('ascii')\n",
    "    text_bytes = ascii_text.encode('ascii')\n",
    "    \n",
    "    print(f\"✓ Converted to pure ASCII\")\n",
    "    print(f\"Final size: {len(text_bytes):,} bytes\")\n",
    "else:\n",
    "    print(f\"\\n✓ Text is pure ASCII\")\n",
    "    ascii_text = clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Byte Statistics\n",
    "\n",
    "Analyze which bytes appear and their frequencies. This tells us:\n",
    "- How many unique bytes are in the corpus (should be <128)\n",
    "- Which ASCII bytes are dead (never appear)\n",
    "- Frequency distribution (some will be common, some rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte statistics:\n",
      "  Total bytes in corpus: 272,305\n",
      "  Unique bytes present: 78 / 128 ASCII\n",
      "  Dead ASCII bytes (0-127): 50\n",
      "  Dead high bytes (128-255): 128\n",
      "  Total dead in 256-byte vocab: 178 (69.5%)\n",
      "\n",
      "Most common bytes:\n",
      "   32 (    ): 44,251 times (16.25%)\n",
      "  101 (   e): 25,012 times ( 9.19%)\n",
      "  116 (   t): 18,098 times ( 6.65%)\n",
      "   97 (   a): 16,843 times ( 6.19%)\n",
      "  111 (   o): 15,739 times ( 5.78%)\n",
      "  110 (   n): 14,065 times ( 5.17%)\n",
      "  105 (   i): 12,532 times ( 4.60%)\n",
      "  115 (   s): 12,370 times ( 4.54%)\n",
      "  104 (   h): 12,240 times ( 4.49%)\n",
      "  114 (   r): 11,342 times ( 4.17%)\n",
      "\n",
      "Least common bytes:\n",
      "   81 (   Q):      4 times ( 0.00%)\n",
      "   50 (   2):      4 times ( 0.00%)\n",
      "   56 (   8):      3 times ( 0.00%)\n",
      "   88 (   X):      2 times ( 0.00%)\n",
      "   55 (   7):      2 times ( 0.00%)\n",
      "   52 (   4):      2 times ( 0.00%)\n",
      "   91 (   [):      2 times ( 0.00%)\n",
      "   93 (   ]):      2 times ( 0.00%)\n",
      "   36 (   $):      2 times ( 0.00%)\n",
      "   90 (   Z):      1 times ( 0.00%)\n"
     ]
    }
   ],
   "source": [
    "byte_counts = Counter(text_bytes)\n",
    "\n",
    "# Unique bytes that appear\n",
    "present_bytes = set(byte_counts.keys())\n",
    "n_present = len(present_bytes)\n",
    "\n",
    "# ASCII bytes that never appear\n",
    "all_ascii = set(range(128))\n",
    "dead_ascii = all_ascii - present_bytes\n",
    "n_dead_ascii = len(dead_ascii)\n",
    "\n",
    "# High bytes (128-255) never appear by construction\n",
    "n_dead_high = 128\n",
    "\n",
    "print(f\"Byte statistics:\")\n",
    "print(f\"  Total bytes in corpus: {len(text_bytes):,}\")\n",
    "print(f\"  Unique bytes present: {n_present} / 128 ASCII\")\n",
    "print(f\"  Dead ASCII bytes (0-127): {n_dead_ascii}\")\n",
    "print(f\"  Dead high bytes (128-255): {n_dead_high}\")\n",
    "print(f\"  Total dead in 256-byte vocab: {n_dead_ascii + n_dead_high} ({100 * (n_dead_ascii + n_dead_high) / 256:.1f}%)\")\n",
    "\n",
    "# Show most/least common\n",
    "most_common = byte_counts.most_common(10)\n",
    "least_common = byte_counts.most_common()[-10:]\n",
    "\n",
    "print(f\"\\nMost common bytes:\")\n",
    "for byte_val, count in most_common:\n",
    "    char = chr(byte_val) if 32 <= byte_val < 127 else f\"\\\\x{byte_val:02x}\"\n",
    "    print(f\"  {byte_val:3d} ({char:>4s}): {count:6,} times ({100 * count / len(text_bytes):5.2f}%)\")\n",
    "\n",
    "print(f\"\\nLeast common bytes:\")\n",
    "for byte_val, count in least_common:\n",
    "    char = chr(byte_val) if 32 <= byte_val < 127 else f\"\\\\x{byte_val:02x}\"\n",
    "    print(f\"  {byte_val:3d} ({char:>4s}): {count:6,} times ({100 * count / len(text_bytes):5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Show Dead ASCII Bytes\n",
    "\n",
    "Display which ASCII characters never appear in Gatsby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dead ASCII bytes (50 total):\n",
      "Values: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 35, 37, 38, 39, 43, 47, 60, 61, 62, 64, 92, 94, 95, 96, 123, 124, 125, 126, 127]\n",
      "Characters: ['\\\\x00', '\\\\x01', '\\\\x02', '\\\\x03', '\\\\x04', '\\\\x05', '\\\\x06', '\\\\x07', '\\\\x08', '\\\\x09', '\\\\x0b', '\\\\x0c', '\\\\x0e', '\\\\x0f', '\\\\x10', '\\\\x11', '\\\\x12', '\\\\x13', '\\\\x14', '\\\\x15', '\\\\x16', '\\\\x17', '\\\\x18', '\\\\x19', '\\\\x1a', '\\\\x1b', '\\\\x1c', '\\\\x1d', '\\\\x1e', '\\\\x1f', '\"', '#', '%', '&', \"'\", '+', '/', '<', '=', '>', '@', '\\\\', '^', '_', '`', '{', '|', '}', '~', '\\\\x7f']\n"
     ]
    }
   ],
   "source": [
    "if dead_ascii:\n",
    "    print(f\"Dead ASCII bytes ({len(dead_ascii)} total):\")\n",
    "    print(f\"Values: {sorted(dead_ascii)}\")\n",
    "    \n",
    "    # Show as characters where printable\n",
    "    printable = [chr(b) if 32 <= b < 127 else f\"\\\\x{b:02x}\" for b in sorted(dead_ascii)]\n",
    "    print(f\"Characters: {printable}\")\n",
    "else:\n",
    "    print(f\"No dead ASCII bytes - all 128 ASCII values appear in corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Save Corpus\n",
    "\n",
    "Write the clean ASCII text to disk for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved corpus to: ../data/training_corpus.txt\n",
      "File size: 272,305 bytes (265.9 KB)\n"
     ]
    }
   ],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# Write as ASCII bytes\n",
    "with open(OUTPUT_PATH, 'w', encoding='ascii') as f:\n",
    "    f.write(ascii_text)\n",
    "\n",
    "print(f\"✓ Saved corpus to: {OUTPUT_PATH}\")\n",
    "print(f\"File size: {len(ascii_text):,} bytes ({len(ascii_text) / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING CORPUS SUMMARY\n",
      "================================================================================\n",
      "Source: The Great Gatsby (F. Scott Fitzgerald)\n",
      "Corpus size: 272,305 bytes (265.9 KB)\n",
      "\n",
      "Vocabulary (256-byte tokenizer):\n",
      "  Present: 78 bytes\n",
      "  Dead ASCII (0-127): 50 bytes\n",
      "  Dead high (128-255): 128 bytes\n",
      "  Total dead: 178 (69.5%)\n",
      "\n",
      "Output: ../data/training_corpus.txt\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*80}\")\n",
    "print(\"TRAINING CORPUS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Source: The Great Gatsby (F. Scott Fitzgerald)\")\n",
    "print(f\"Corpus size: {len(text_bytes):,} bytes ({len(text_bytes) / 1024:.1f} KB)\")\n",
    "print(f\"\\nVocabulary (256-byte tokenizer):\")\n",
    "print(f\"  Present: {n_present} bytes\")\n",
    "print(f\"  Dead ASCII (0-127): {n_dead_ascii} bytes\")\n",
    "print(f\"  Dead high (128-255): {n_dead_high} bytes\")\n",
    "print(f\"  Total dead: {n_dead_ascii + n_dead_high} ({100 * (n_dead_ascii + n_dead_high) / 256:.1f}%)\")\n",
    "print(f\"\\nOutput: {OUTPUT_PATH}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
