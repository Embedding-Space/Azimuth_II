{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 12.2d: Comprehensive Synthetic Statistics\n",
    "\n",
    "**Goal:** Stream through all 10,000 synthetic snowball trials and collect EVERY statistic we care about. Save to One CSV To Rule Them All.\n",
    "\n",
    "## What We Compute (Per Trial)\n",
    "\n",
    "### Basic Counts\n",
    "- `n_tokens` - Total tokens (should always be 2,100)\n",
    "- `n_unique` - Number of unique vectors\n",
    "- `n_black_holes` - Vectors with count ≥ 2\n",
    "- `n_singletons` - Vectors with count = 1\n",
    "- `total_population` - Sum of all counts (= n_tokens)\n",
    "- `black_hole_population` - Sum of counts where count ≥ 2\n",
    "\n",
    "### Per-Black-Hole Statistics\n",
    "- `largest_bh` - Max population among black holes\n",
    "- `smallest_bh` - Min population among black holes\n",
    "- `mean_bh_size` - Mean population per black hole\n",
    "- `median_bh_size` - Median population per black hole\n",
    "- `top2_population` - Sum of two largest black holes (concentration metric)\n",
    "- `gini_coefficient` - Inequality measure of population distribution\n",
    "\n",
    "### Spatial Extent (L∞ Distances)\n",
    "- `max_l_inf` - Maximum pairwise Chebyshev distance (in units of ε)\n",
    "- `mean_l_inf` - Mean pairwise Chebyshev distance\n",
    "- `median_l_inf` - Median pairwise Chebyshev distance\n",
    "\n",
    "### Topology (Graph Structure)\n",
    "- `n_components` - Number of connected components in adjacency graph\n",
    "- `n_isolated` - Number of nodes with degree = 0\n",
    "- `largest_component_size` - Size of largest connected component\n",
    "- `largest_component_density` - Edge density of largest component (0-1)\n",
    "- `global_density` - Edge density of full graph (0-1)\n",
    "\n",
    "## Approach\n",
    "\n",
    "- Stream HDF5 in 100-trial batches (~2 GB RAM per batch)\n",
    "- For each trial: run `torch.unique()` to get vectors + counts\n",
    "- Compute all statistics using vectorized operations where possible\n",
    "- Use NetworkX only for graph topology (unavoidable, but fast for ~12 nodes)\n",
    "- Save results to CSV: one row per trial, 20+ columns\n",
    "\n",
    "## Output\n",
    "\n",
    "`../data/analysis/synthetic_comprehensive_n10000.csv`\n",
    "\n",
    "**Runtime:** ~2-3 minutes (bottleneck: torch.unique() runs on CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source\n",
    "DATA_H5 = \"../data/tensors/synthetic_snowballs_n10000_sigma1.5e-9.h5\"\n",
    "\n",
    "# Streaming configuration\n",
    "BATCH_SIZE = 100  # Trials per batch (~2 GB RAM per batch)\n",
    "\n",
    "# Reference scale\n",
    "EPSILON = 6e-5  # bfloat16 ULP at Qwen magnitude\n",
    "\n",
    "# Adjacency threshold for topology\n",
    "TOUCHING_THRESHOLD = 2 * EPSILON\n",
    "\n",
    "# Output\n",
    "OUTPUT_CSV = \"../data/analysis/synthetic_comprehensive_n10000.csv\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def compute_gini_coefficient(populations):\n",
    "    \"\"\"\n",
    "    Compute Gini coefficient of inequality.\n",
    "    \n",
    "    Args:\n",
    "        populations: Tensor of black hole populations\n",
    "    \n",
    "    Returns:\n",
    "        float: Gini coefficient (0 = perfect equality, 1 = maximum inequality)\n",
    "    \"\"\"\n",
    "    if len(populations) <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    # Sort populations\n",
    "    sorted_pops = torch.sort(populations.float())[0]\n",
    "    n = len(sorted_pops)\n",
    "    \n",
    "    # Gini formula: G = (2 * sum(i * x_i)) / (n * sum(x_i)) - (n + 1) / n\n",
    "    indices = torch.arange(1, n + 1, dtype=torch.float32)\n",
    "    numerator = 2 * torch.sum(indices * sorted_pops)\n",
    "    denominator = n * torch.sum(sorted_pops)\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    gini = (numerator / denominator) - (n + 1) / n\n",
    "    return gini.item()\n",
    "\n",
    "\n",
    "def compute_l_inf_stats(unique_vectors, epsilon):\n",
    "    \"\"\"\n",
    "    Compute L∞ (Chebyshev) distance statistics.\n",
    "    \n",
    "    Args:\n",
    "        unique_vectors: Tensor of shape [n_unique, hidden_dim]\n",
    "        epsilon: Reference scale for normalization\n",
    "    \n",
    "    Returns:\n",
    "        dict with max_l_inf, mean_l_inf, median_l_inf (all in units of epsilon)\n",
    "    \"\"\"\n",
    "    n = len(unique_vectors)\n",
    "    \n",
    "    if n <= 1:\n",
    "        return {'max_l_inf': 0.0, 'mean_l_inf': 0.0, 'median_l_inf': 0.0}\n",
    "    \n",
    "    # Compute pairwise L∞ distances (vectorized)\n",
    "    v1 = unique_vectors.unsqueeze(1)  # [n, 1, d]\n",
    "    v2 = unique_vectors.unsqueeze(0)  # [1, n, d]\n",
    "    diffs = v1 - v2  # [n, n, d]\n",
    "    l_inf_matrix = torch.abs(diffs).max(dim=2)[0]  # [n, n]\n",
    "    \n",
    "    # Exclude diagonal (self-distances)\n",
    "    mask = ~torch.eye(n, dtype=torch.bool)\n",
    "    l_inf_values = l_inf_matrix[mask]\n",
    "    \n",
    "    # Normalize by epsilon\n",
    "    l_inf_values = l_inf_values / epsilon\n",
    "    \n",
    "    return {\n",
    "        'max_l_inf': l_inf_values.max().item(),\n",
    "        'mean_l_inf': l_inf_values.mean().item(),\n",
    "        'median_l_inf': l_inf_values.median().item(),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_topology(unique_vectors, threshold):\n",
    "    \"\"\"\n",
    "    Compute graph topology statistics.\n",
    "    \n",
    "    Args:\n",
    "        unique_vectors: Tensor of shape [n_unique, hidden_dim]\n",
    "        threshold: Adjacency threshold (L∞ distance)\n",
    "    \n",
    "    Returns:\n",
    "        dict with n_components, n_isolated, largest_component_size, \n",
    "        largest_component_density, global_density\n",
    "    \"\"\"\n",
    "    n = len(unique_vectors)\n",
    "    \n",
    "    if n == 0:\n",
    "        return {\n",
    "            'n_components': 0,\n",
    "            'n_isolated': 0,\n",
    "            'largest_component_size': 0,\n",
    "            'largest_component_density': 0.0,\n",
    "            'global_density': 0.0,\n",
    "        }\n",
    "    \n",
    "    if n == 1:\n",
    "        return {\n",
    "            'n_components': 1,\n",
    "            'n_isolated': 1,\n",
    "            'largest_component_size': 1,\n",
    "            'largest_component_density': 1.0,\n",
    "            'global_density': 1.0,\n",
    "        }\n",
    "    \n",
    "    # Compute pairwise L∞ distances\n",
    "    v1 = unique_vectors.unsqueeze(1)\n",
    "    v2 = unique_vectors.unsqueeze(0)\n",
    "    diffs = v1 - v2\n",
    "    l_inf_matrix = torch.abs(diffs).max(dim=2)[0]\n",
    "    \n",
    "    # Build adjacency matrix (exclude self-loops)\n",
    "    adjacency = (l_inf_matrix <= threshold) & (~torch.eye(n, dtype=torch.bool))\n",
    "    \n",
    "    # Convert to NetworkX graph\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n))\n",
    "    edges = torch.nonzero(adjacency, as_tuple=False).tolist()\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    # Connected components\n",
    "    components = list(nx.connected_components(G))\n",
    "    component_sizes = sorted([len(c) for c in components], reverse=True)\n",
    "    \n",
    "    n_components = len(components)\n",
    "    largest_size = component_sizes[0] if component_sizes else 0\n",
    "    \n",
    "    # Isolated nodes (degree = 0)\n",
    "    n_isolated = sum(1 for node in G.nodes() if G.degree(node) == 0)\n",
    "    \n",
    "    # Density of largest component\n",
    "    if largest_size > 1:\n",
    "        largest_component = max(components, key=len)\n",
    "        subgraph = G.subgraph(largest_component)\n",
    "        n_edges = subgraph.number_of_edges()\n",
    "        max_edges = largest_size * (largest_size - 1) // 2\n",
    "        largest_density = n_edges / max_edges if max_edges > 0 else 0.0\n",
    "    else:\n",
    "        largest_density = 1.0 if largest_size == 1 else 0.0\n",
    "    \n",
    "    # Global density\n",
    "    n_edges = G.number_of_edges()\n",
    "    max_edges = n * (n - 1) // 2\n",
    "    global_density = n_edges / max_edges if max_edges > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'n_components': n_components,\n",
    "        'n_isolated': n_isolated,\n",
    "        'largest_component_size': largest_size,\n",
    "        'largest_component_density': largest_density,\n",
    "        'global_density': global_density,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_trial_statistics(embeddings, epsilon, threshold):\n",
    "    \"\"\"\n",
    "    Compute all statistics for a single trial.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Tensor of shape [n_tokens, hidden_dim]\n",
    "        epsilon: Reference scale\n",
    "        threshold: Adjacency threshold\n",
    "    \n",
    "    Returns:\n",
    "        dict with all 20+ statistics\n",
    "    \"\"\"\n",
    "    # Get unique vectors and counts\n",
    "    unique_vectors, _, counts = torch.unique(\n",
    "        embeddings,\n",
    "        dim=0,\n",
    "        return_inverse=True,\n",
    "        return_counts=True\n",
    "    )\n",
    "    \n",
    "    # Basic counts\n",
    "    n_tokens = len(embeddings)\n",
    "    n_unique = len(unique_vectors)\n",
    "    black_hole_mask = counts >= 2\n",
    "    n_black_holes = black_hole_mask.sum().item()\n",
    "    n_singletons = (~black_hole_mask).sum().item()\n",
    "    total_population = counts.sum().item()\n",
    "    black_hole_population = counts[black_hole_mask].sum().item() if n_black_holes > 0 else 0\n",
    "    \n",
    "    # Per-black-hole statistics\n",
    "    if n_black_holes > 0:\n",
    "        bh_populations = counts[black_hole_mask]\n",
    "        largest_bh = bh_populations.max().item()\n",
    "        smallest_bh = bh_populations.min().item()\n",
    "        mean_bh_size = bh_populations.float().mean().item()\n",
    "        median_bh_size = bh_populations.float().median().item()\n",
    "        \n",
    "        # Top-2 concentration\n",
    "        top_k = min(2, len(bh_populations))\n",
    "        top2_population = bh_populations.topk(top_k)[0].sum().item()\n",
    "        \n",
    "        # Gini coefficient\n",
    "        gini = compute_gini_coefficient(bh_populations)\n",
    "    else:\n",
    "        largest_bh = 0\n",
    "        smallest_bh = 0\n",
    "        mean_bh_size = 0.0\n",
    "        median_bh_size = 0.0\n",
    "        top2_population = 0\n",
    "        gini = 0.0\n",
    "    \n",
    "    # Spatial extent (L∞ distances)\n",
    "    l_inf_stats = compute_l_inf_stats(unique_vectors, epsilon)\n",
    "    \n",
    "    # Topology\n",
    "    topology_stats = compute_topology(unique_vectors, threshold)\n",
    "    \n",
    "    # Combine all statistics\n",
    "    return {\n",
    "        # Basic counts\n",
    "        'n_tokens': n_tokens,\n",
    "        'n_unique': n_unique,\n",
    "        'n_black_holes': n_black_holes,\n",
    "        'n_singletons': n_singletons,\n",
    "        'total_population': total_population,\n",
    "        'black_hole_population': black_hole_population,\n",
    "        \n",
    "        # Per-BH statistics\n",
    "        'largest_bh': largest_bh,\n",
    "        'smallest_bh': smallest_bh,\n",
    "        'mean_bh_size': mean_bh_size,\n",
    "        'median_bh_size': median_bh_size,\n",
    "        'top2_population': top2_population,\n",
    "        'gini_coefficient': gini,\n",
    "        \n",
    "        # Spatial extent\n",
    "        'max_l_inf': l_inf_stats['max_l_inf'],\n",
    "        'mean_l_inf': l_inf_stats['mean_l_inf'],\n",
    "        'median_l_inf': l_inf_stats['median_l_inf'],\n",
    "        \n",
    "        # Topology\n",
    "        'n_components': topology_stats['n_components'],\n",
    "        'n_isolated': topology_stats['n_isolated'],\n",
    "        'largest_component_size': topology_stats['largest_component_size'],\n",
    "        'largest_component_density': topology_stats['largest_component_density'],\n",
    "        'global_density': topology_stats['global_density'],\n",
    "    }\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Stream Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ../data/tensors/synthetic_snowballs_n10000_sigma1.5e-9.h5...\n",
      "\n",
      "✓ Dataset loaded\n",
      "  Total trials: 10,000\n",
      "  Batch size: 100\n",
      "  Number of batches: 100\n",
      "\n",
      "Processing...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db7419335a24192bd56e5af6abb1d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processing complete: 10,000 trials analyzed\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading dataset from {DATA_H5}...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "with h5py.File(DATA_H5, 'r') as f:\n",
    "    n_total_trials = f['embeddings'].shape[0]\n",
    "    n_batches = (n_total_trials + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    print(f\"✓ Dataset loaded\")\n",
    "    print(f\"  Total trials: {n_total_trials:,}\")\n",
    "    print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"  Number of batches: {n_batches}\")\n",
    "    print(f\"\\nProcessing...\\n\")\n",
    "    \n",
    "    for batch_idx in tqdm(range(n_batches), desc=\"Processing batches\"):\n",
    "        # Load batch\n",
    "        batch_start = batch_idx * BATCH_SIZE\n",
    "        batch_end = min(batch_start + BATCH_SIZE, n_total_trials)\n",
    "        \n",
    "        embeddings_batch = torch.from_numpy(f['embeddings'][batch_start:batch_end]).to(torch.float32)\n",
    "        \n",
    "        # Process each trial in batch\n",
    "        for trial_idx in range(len(embeddings_batch)):\n",
    "            embeddings = embeddings_batch[trial_idx]\n",
    "            \n",
    "            # Compute all statistics\n",
    "            stats = compute_trial_statistics(embeddings, EPSILON, TOUCHING_THRESHOLD)\n",
    "            stats['trial_id'] = batch_start + trial_idx\n",
    "            \n",
    "            results.append(stats)\n",
    "        \n",
    "        # Free batch memory\n",
    "        del embeddings_batch\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"\\n✓ Processing complete: {len(results):,} trials analyzed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved to ../data/analysis/synthetic_comprehensive_n10000.csv\n",
      "  Rows: 10,000\n",
      "  Columns: 21\n",
      "  File size: 1.43 MB\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Reorder columns for readability\n",
    "column_order = [\n",
    "    'trial_id',\n",
    "    # Basic counts\n",
    "    'n_tokens', 'n_unique', 'n_black_holes', 'n_singletons',\n",
    "    'total_population', 'black_hole_population',\n",
    "    # Per-BH stats\n",
    "    'largest_bh', 'smallest_bh', 'mean_bh_size', 'median_bh_size',\n",
    "    'top2_population', 'gini_coefficient',\n",
    "    # Spatial\n",
    "    'max_l_inf', 'mean_l_inf', 'median_l_inf',\n",
    "    # Topology\n",
    "    'n_components', 'n_isolated', 'largest_component_size',\n",
    "    'largest_component_density', 'global_density',\n",
    "]\n",
    "\n",
    "df = df[column_order]\n",
    "\n",
    "# Save\n",
    "output_path = Path(OUTPUT_CSV)\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Saved to {output_path}\")\n",
    "print(f\"  Rows: {len(df):,}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "print(f\"  File size: {output_path.stat().st_size / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Preview Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PREVIEW: First 5 Trials\n",
      "======================================================================\n",
      "\n",
      "   trial_id  n_tokens  n_unique  n_black_holes  n_singletons  \\\n",
      "0         0      2100        10             10             0   \n",
      "1         1      2100         9              9             0   \n",
      "2         2      2100        12             10             2   \n",
      "3         3      2100        11              9             2   \n",
      "4         4      2100        12              8             4   \n",
      "\n",
      "   total_population  black_hole_population  largest_bh  smallest_bh  \\\n",
      "0              2100                   2100        1137            2   \n",
      "1              2100                   2100        1122            3   \n",
      "2              2100                   2098        1098            2   \n",
      "3              2100                   2098        1125            2   \n",
      "4              2100                   2096        1118            4   \n",
      "\n",
      "   mean_bh_size  ...  top2_population  gini_coefficient  max_l_inf  \\\n",
      "0    210.000000  ...             1526          0.724381   0.254313   \n",
      "1    233.333328  ...             1541          0.692275   0.254313   \n",
      "2    209.800003  ...             1540          0.722784   0.254313   \n",
      "3    233.111115  ...             1532          0.688275   0.254313   \n",
      "4    262.000000  ...             1534          0.659948   0.254313   \n",
      "\n",
      "   mean_l_inf  median_l_inf  n_components  n_isolated  largest_component_size  \\\n",
      "0    0.095103      0.007947             1           0                      10   \n",
      "1    0.062254      0.007947             1           0                       9   \n",
      "2    0.101448      0.031789             1           0                      12   \n",
      "3    0.091755      0.007947             1           0                      11   \n",
      "4    0.111322      0.031789             1           0                      12   \n",
      "\n",
      "   largest_component_density  global_density  \n",
      "0                        1.0             1.0  \n",
      "1                        1.0             1.0  \n",
      "2                        1.0             1.0  \n",
      "3                        1.0             1.0  \n",
      "4                        1.0             1.0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "======================================================================\n",
      "SUMMARY STATISTICS\n",
      "======================================================================\n",
      "\n",
      "          trial_id  n_tokens      n_unique  n_black_holes  n_singletons  \\\n",
      "count  10000.00000   10000.0  10000.000000   10000.000000   10000.00000   \n",
      "mean    4999.50000    2100.0     11.417500       9.092200       2.32530   \n",
      "std     2886.89568       0.0      1.357563       0.851922       1.36648   \n",
      "min        0.00000    2100.0      7.000000       7.000000       0.00000   \n",
      "25%     2499.75000    2100.0     10.000000       9.000000       1.00000   \n",
      "50%     4999.50000    2100.0     11.000000       9.000000       2.00000   \n",
      "75%     7499.25000    2100.0     12.000000      10.000000       3.00000   \n",
      "max     9999.00000    2100.0     17.000000      13.000000       8.00000   \n",
      "\n",
      "       total_population  black_hole_population    largest_bh   smallest_bh  \\\n",
      "count           10000.0            10000.00000  10000.000000  10000.000000   \n",
      "mean             2100.0             2097.67470   1125.314600      2.950700   \n",
      "std                 0.0                1.36648     22.712015      1.621337   \n",
      "min              2100.0             2092.00000   1042.000000      2.000000   \n",
      "25%              2100.0             2097.00000   1110.000000      2.000000   \n",
      "50%              2100.0             2098.00000   1125.000000      2.000000   \n",
      "75%              2100.0             2099.00000   1141.000000      3.000000   \n",
      "max              2100.0             2100.00000   1215.000000     16.000000   \n",
      "\n",
      "       mean_bh_size  ...  top2_population  gini_coefficient     max_l_inf  \\\n",
      "count  10000.000000  ...      10000.00000      10000.000000  10000.000000   \n",
      "mean     232.726092  ...       1535.69530          0.691889      0.251486   \n",
      "std       21.700518  ...         19.74849          0.027557      0.024840   \n",
      "min      161.384613  ...       1464.00000          0.588315      0.007947   \n",
      "25%      209.899994  ...       1522.00000          0.681178      0.254313   \n",
      "50%      233.111115  ...       1536.00000          0.692218      0.254313   \n",
      "75%      233.333328  ...       1549.00000          0.715150      0.254313   \n",
      "max      300.000000  ...       1620.00000          0.785356      0.254313   \n",
      "\n",
      "         mean_l_inf  median_l_inf  n_components  n_isolated  \\\n",
      "count  10000.000000  10000.000000       10000.0     10000.0   \n",
      "mean       0.098411      0.024170           1.0         0.0   \n",
      "std        0.022008      0.033984           0.0         0.0   \n",
      "min        0.007380      0.007947           1.0         0.0   \n",
      "25%        0.088287      0.007947           1.0         0.0   \n",
      "50%        0.101509      0.007947           1.0         0.0   \n",
      "75%        0.115091      0.031789           1.0         0.0   \n",
      "max        0.143203      0.254313           1.0         0.0   \n",
      "\n",
      "       largest_component_size  largest_component_density  global_density  \n",
      "count            10000.000000                    10000.0         10000.0  \n",
      "mean                11.417500                        1.0             1.0  \n",
      "std                  1.357563                        0.0             0.0  \n",
      "min                  7.000000                        1.0             1.0  \n",
      "25%                 10.000000                        1.0             1.0  \n",
      "50%                 11.000000                        1.0             1.0  \n",
      "75%                 12.000000                        1.0             1.0  \n",
      "max                 17.000000                        1.0             1.0  \n",
      "\n",
      "[8 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PREVIEW: First 5 Trials\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Quick Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SANITY CHECKS\n",
      "======================================================================\n",
      "\n",
      "✓ All trials have n_tokens = 2,100\n",
      "✓ total_population == n_tokens for all trials\n",
      "✓ n_unique == n_black_holes + n_singletons\n",
      "\n",
      "✓ Trials with 1 component: 10,000 / 10,000 (100.0%)\n",
      "✓ Trials with density = 1.0: 10,000 / 10,000 (100.0%)\n",
      "\n",
      "======================================================================\n",
      "ALL SANITY CHECKS PASSED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"SANITY CHECKS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Check 1: All trials have n_tokens = 2100\n",
    "assert (df['n_tokens'] == 2100).all(), \"ERROR: Some trials have wrong token count!\"\n",
    "print(\"✓ All trials have n_tokens = 2,100\")\n",
    "\n",
    "# Check 2: total_population == n_tokens\n",
    "assert (df['total_population'] == df['n_tokens']).all(), \"ERROR: Population doesn't match token count!\"\n",
    "print(\"✓ total_population == n_tokens for all trials\")\n",
    "\n",
    "# Check 3: n_unique == n_black_holes + n_singletons\n",
    "assert (df['n_unique'] == df['n_black_holes'] + df['n_singletons']).all(), \"ERROR: Unique count mismatch!\"\n",
    "print(\"✓ n_unique == n_black_holes + n_singletons\")\n",
    "\n",
    "# Check 4: All trials have 1 connected component\n",
    "n_single_component = (df['n_components'] == 1).sum()\n",
    "print(f\"\\n✓ Trials with 1 component: {n_single_component:,} / {len(df):,} ({n_single_component/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check 5: All trials have density = 1.0\n",
    "n_full_density = (df['largest_component_density'] == 1.0).sum()\n",
    "print(f\"✓ Trials with density = 1.0: {n_full_density:,} / {len(df):,} ({n_full_density/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ALL SANITY CHECKS PASSED\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
