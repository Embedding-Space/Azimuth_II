{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14.2b: Dead Token Topology Evolution\n",
    "\n",
    "**Does the primordial atom crystallize or evaporate?**\n",
    "\n",
    "## The Question\n",
    "\n",
    "As dead tokens evolve during training, do they:\n",
    "- **Evaporate**: Spread out into a diffuse gas of isolated tokens?\n",
    "- **Crystallize**: Form a connected lattice structure like the 2×2×2 hypercube in Qwen 3 4B Instruct?\n",
    "\n",
    "## The Hypothesis\n",
    "\n",
    "Jeffery's picture: The primordial atom starts as a tight cluster (crystal), expands thermally into a gas, but as the centroid moves away from the origin, **bfloat16 precision gets coarser**. The lattice spacing (ULP) increases, causing tokens that were separated to become adjacent. The atom **re-crystallizes** not by moving closer, but by the universe losing resolution around them.\n",
    "\n",
    "## Method\n",
    "\n",
    "For each training step:\n",
    "1. Compute **mean ULP** at the centroid (local lattice spacing)\n",
    "2. Build **adjacency graph**: tokens connected if L∞ distance ≤ mean ULP\n",
    "3. Compute **graph statistics**:\n",
    "   - Graph density (fraction of possible edges)\n",
    "   - Largest connected component size\n",
    "   - Number of isolated tokens (singletons)\n",
    "   - Total number of connected components\n",
    "\n",
    "## Output\n",
    "\n",
    "`data/instrumented_run/dead_token_topology.safetensors` (~500 KB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "INPUT_PATH = \"../data/instrumented_run/dead_token_kinematics.safetensors\"\n",
    "OUTPUT_PATH = \"../data/instrumented_run/dead_token_topology.safetensors\"\n",
    "\n",
    "# Computation\n",
    "USE_GPU = True  # Set to False to force CPU\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using Apple Silicon GPU (MPS)\n",
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file, save_file\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse.csgraph as csgraph\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Detect device\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"✓ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif USE_GPU and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f\"✓ Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"✓ Using CPU\")\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../data/instrumented_run/dead_token_kinematics.safetensors\n",
      "\n",
      "  Recorded steps: 10000\n",
      "  Dead tokens: 51\n",
      "  Hidden dim: 64\n",
      "\n",
      "✓ Data loaded\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading: {INPUT_PATH}\")\n",
    "\n",
    "data = load_file(INPUT_PATH)\n",
    "\n",
    "recorded_steps = data['recorded_steps']\n",
    "dead_token_ids = data['dead_token_ids']\n",
    "positions = data['positions']  # [n_recorded, n_dead, hidden_dim]\n",
    "centroid = data['centroid']  # [n_recorded, hidden_dim]\n",
    "\n",
    "n_recorded = len(recorded_steps)\n",
    "n_dead = len(dead_token_ids)\n",
    "hidden_dim = positions.shape[2]\n",
    "\n",
    "print(f\"\\n  Recorded steps: {n_recorded}\")\n",
    "print(f\"  Dead tokens: {n_dead}\")\n",
    "print(f\"  Hidden dim: {hidden_dim}\")\n",
    "print(f\"\\n✓ Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def compute_ulp_at_point(point_bf16):\n",
    "    \"\"\"\n",
    "    Compute ULP (unit in last place) for each dimension of a bfloat16 point.\n",
    "    Returns mean ULP across dimensions.\n",
    "    \n",
    "    point_bf16: [hidden_dim] tensor in bfloat16\n",
    "    \"\"\"\n",
    "    # ULP = nextafter(x, x+1) - x\n",
    "    next_vals = torch.nextafter(point_bf16, point_bf16 + 1)\n",
    "    ulps = next_vals - point_bf16\n",
    "    return ulps.mean().item()\n",
    "\n",
    "\n",
    "def compute_adjacency_matrix(positions_bf16, ulp_threshold):\n",
    "    \"\"\"\n",
    "    Compute adjacency matrix based on L∞ distance.\n",
    "    Two tokens are adjacent if L∞ distance ≤ ulp_threshold.\n",
    "    \n",
    "    positions_bf16: [n_tokens, hidden_dim] in bfloat16\n",
    "    ulp_threshold: scalar\n",
    "    \n",
    "    Returns: [n_tokens, n_tokens] boolean adjacency matrix (excludes self-loops)\n",
    "    \"\"\"\n",
    "    n_tokens = positions_bf16.shape[0]\n",
    "    \n",
    "    # Pairwise differences: [n_tokens, n_tokens, hidden_dim]\n",
    "    diff = positions_bf16.unsqueeze(0) - positions_bf16.unsqueeze(1)\n",
    "    \n",
    "    # L∞ distance: [n_tokens, n_tokens]\n",
    "    linf_dist = diff.abs().max(dim=2)[0]\n",
    "    \n",
    "    # Adjacency: L∞ > 0 (not same token) and L∞ ≤ threshold\n",
    "    adjacency = (linf_dist > 0) & (linf_dist <= ulp_threshold)\n",
    "    \n",
    "    return adjacency.cpu().numpy()  # Return as numpy for graph analysis\n",
    "\n",
    "\n",
    "def compute_graph_statistics(adjacency):\n",
    "    \"\"\"\n",
    "    Compute topological statistics from adjacency matrix.\n",
    "    \n",
    "    adjacency: [n_tokens, n_tokens] boolean numpy array\n",
    "    \n",
    "    Returns:\n",
    "        density: fraction of possible edges that exist\n",
    "        largest_component_size: size of largest connected component\n",
    "        n_singletons: number of isolated tokens (degree 0)\n",
    "        n_components: total number of connected components\n",
    "    \"\"\"\n",
    "    n_tokens = adjacency.shape[0]\n",
    "    \n",
    "    # Graph density\n",
    "    n_edges = adjacency.sum() / 2  # Divide by 2 because adjacency is symmetric\n",
    "    max_edges = n_tokens * (n_tokens - 1) / 2\n",
    "    density = n_edges / max_edges if max_edges > 0 else 0\n",
    "    \n",
    "    # Connected components\n",
    "    n_components, labels = csgraph.connected_components(adjacency, directed=False)\n",
    "    \n",
    "    # Component sizes\n",
    "    component_sizes = np.bincount(labels)\n",
    "    largest_component_size = component_sizes.max()\n",
    "    \n",
    "    # Singletons (components of size 1)\n",
    "    n_singletons = (component_sizes == 1).sum()\n",
    "    \n",
    "    return density, largest_component_size, n_singletons, n_components\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Topology Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing topology evolution...\n",
      "This may take a few minutes.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:08<00:00, 1226.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Topology computed for 10000 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing topology evolution...\")\n",
    "print(\"This may take a few minutes.\\n\")\n",
    "\n",
    "# Storage for results\n",
    "mean_ulps = np.zeros(n_recorded)\n",
    "densities = np.zeros(n_recorded)\n",
    "largest_components = np.zeros(n_recorded, dtype=np.int32)\n",
    "n_singletons_array = np.zeros(n_recorded, dtype=np.int32)\n",
    "n_components_array = np.zeros(n_recorded, dtype=np.int32)\n",
    "\n",
    "# Process each step\n",
    "for t in tqdm(range(n_recorded)):\n",
    "    # Get positions and centroid at this step\n",
    "    pos_t = positions[t].to(device)  # [n_dead, hidden_dim]\n",
    "    cent_t = centroid[t].to(device)  # [hidden_dim]\n",
    "    \n",
    "    # Quantize to bfloat16\n",
    "    pos_bf16 = pos_t.to(torch.bfloat16)\n",
    "    cent_bf16 = cent_t.to(torch.bfloat16)\n",
    "    \n",
    "    # Compute mean ULP at centroid\n",
    "    mean_ulp = compute_ulp_at_point(cent_bf16)\n",
    "    mean_ulps[t] = mean_ulp\n",
    "    \n",
    "    # Compute adjacency matrix\n",
    "    adjacency = compute_adjacency_matrix(pos_bf16, mean_ulp)\n",
    "    \n",
    "    # Compute graph statistics\n",
    "    density, largest_comp, n_sing, n_comp = compute_graph_statistics(adjacency)\n",
    "    \n",
    "    densities[t] = density\n",
    "    largest_components[t] = largest_comp\n",
    "    n_singletons_array[t] = n_sing\n",
    "    n_components_array[t] = n_comp\n",
    "\n",
    "print(f\"\\n✓ Topology computed for {n_recorded} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOPOLOGY EVOLUTION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ULP evolution:\n",
      "  Initial mean ULP: 0.000000e+00\n",
      "  Final mean ULP: 0.000000e+00\n",
      "  Ratio (final/initial): nan\n",
      "  Max mean ULP: 0.000000e+00 (step 0)\n",
      "\n",
      "Graph density evolution:\n",
      "  Initial density: 0.0000\n",
      "  Final density: 0.0000\n",
      "  Max density: 0.0000 (step 0)\n",
      "  Min density: 0.0000 (step 0)\n",
      "\n",
      "Largest connected component:\n",
      "  Initial size: 1 / 51\n",
      "  Final size: 1 / 51\n",
      "  Min size: 1 (step 0)\n",
      "\n",
      "Singletons (isolated tokens):\n",
      "  Initial: 51 / 51\n",
      "  Final: 51 / 51\n",
      "  Max: 51 (step 0)\n",
      "\n",
      "Number of components:\n",
      "  Initial: 51\n",
      "  Final: 51\n",
      "  Max: 51 (step 0)\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k5/44vd1ct56xj4y9h7x213kvjr0000gn/T/ipykernel_90862/2798655861.py:8: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print(f\"  Ratio (final/initial): {mean_ulps[-1] / mean_ulps[0]:.2f}\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TOPOLOGY EVOLUTION SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"ULP evolution:\")\n",
    "print(f\"  Initial mean ULP: {mean_ulps[0]:.6e}\")\n",
    "print(f\"  Final mean ULP: {mean_ulps[-1]:.6e}\")\n",
    "print(f\"  Ratio (final/initial): {mean_ulps[-1] / mean_ulps[0]:.2f}\")\n",
    "print(f\"  Max mean ULP: {mean_ulps.max():.6e} (step {recorded_steps.numpy()[mean_ulps.argmax()]})\\n\")\n",
    "\n",
    "print(f\"Graph density evolution:\")\n",
    "print(f\"  Initial density: {densities[0]:.4f}\")\n",
    "print(f\"  Final density: {densities[-1]:.4f}\")\n",
    "print(f\"  Max density: {densities.max():.4f} (step {recorded_steps.numpy()[densities.argmax()]})\")\n",
    "print(f\"  Min density: {densities.min():.4f} (step {recorded_steps.numpy()[densities.argmin()]})\\n\")\n",
    "\n",
    "print(f\"Largest connected component:\")\n",
    "print(f\"  Initial size: {largest_components[0]} / {n_dead}\")\n",
    "print(f\"  Final size: {largest_components[-1]} / {n_dead}\")\n",
    "print(f\"  Min size: {largest_components.min()} (step {recorded_steps.numpy()[largest_components.argmin()]})\\n\")\n",
    "\n",
    "print(f\"Singletons (isolated tokens):\")\n",
    "print(f\"  Initial: {n_singletons_array[0]} / {n_dead}\")\n",
    "print(f\"  Final: {n_singletons_array[-1]} / {n_dead}\")\n",
    "print(f\"  Max: {n_singletons_array.max()} (step {recorded_steps.numpy()[n_singletons_array.argmax()]})\\n\")\n",
    "\n",
    "print(f\"Number of components:\")\n",
    "print(f\"  Initial: {n_components_array[0]}\")\n",
    "print(f\"  Final: {n_components_array[-1]}\")\n",
    "print(f\"  Max: {n_components_array.max()} (step {recorded_steps.numpy()[n_components_array.argmax()]})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving to: ../data/instrumented_run/dead_token_topology.safetensors\n",
      "\n",
      "✓ Saved successfully\n",
      "  File size: 274.4 KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving to: {OUTPUT_PATH}\")\n",
    "\n",
    "save_dict = {\n",
    "    'recorded_steps': recorded_steps,\n",
    "    'dead_token_ids': dead_token_ids,\n",
    "    'mean_ulps': torch.tensor(mean_ulps, dtype=torch.float32),\n",
    "    'densities': torch.tensor(densities, dtype=torch.float32),\n",
    "    'largest_components': torch.tensor(largest_components, dtype=torch.int32),\n",
    "    'n_singletons': torch.tensor(n_singletons_array, dtype=torch.int32),\n",
    "    'n_components': torch.tensor(n_components_array, dtype=torch.int32),\n",
    "}\n",
    "\n",
    "save_file(save_dict, OUTPUT_PATH)\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "file_size_kb = os.path.getsize(OUTPUT_PATH) / 1024\n",
    "\n",
    "print(f\"\\n✓ Saved successfully\")\n",
    "print(f\"  File size: {file_size_kb:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "**Crystallization signature:**\n",
    "- Mean ULP increases as centroid moves away from origin (coarser lattice)\n",
    "- Graph density increases or stays high\n",
    "- Largest component ≈ n_dead (most tokens stay connected)\n",
    "- Few singletons\n",
    "\n",
    "**Evaporation signature:**\n",
    "- Mean ULP stays constant or decreases\n",
    "- Graph density decreases to near zero\n",
    "- Largest component shrinks\n",
    "- Many singletons\n",
    "\n",
    "**Jeffery's hypothesis:**\n",
    "Tokens spread out in *physical* space (expansion we saw), but as ULP grows, they become adjacent again in *topological* space. The universe loses resolution and everything re-crystallizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
