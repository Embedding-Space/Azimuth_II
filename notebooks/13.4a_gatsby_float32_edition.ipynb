{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13.4a: Gatsby Float32 Edition\n",
    "\n",
    "**The Critical Experiment**\n",
    "\n",
    "## The Question\n",
    "\n",
    "Does **float32 initialization + bfloat16 training + gradient escape** naturally produce:\n",
    "1. The topology we see in Qwen (complete graph, L∞ ≤ 2ε)?\n",
    "2. The demographics we see in Qwen (13 black holes with specific populations)?\n",
    "\n",
    "## The Setup\n",
    "\n",
    "Train a toy GPT-2 on The Great Gatsby corpus:\n",
    "- 128 ASCII tokens\n",
    "- **~50 dead tokens** (control chars, math symbols never in Gatsby)\n",
    "- Initialize embeddings in **float32**: random_unit_vector + Gaussian(0, σ)\n",
    "- Train in **bfloat16** for 10,000 steps\n",
    "- Live tokens escape via gradients\n",
    "- Dead tokens stay frozen at f32→bf16 quantization boundaries\n",
    "\n",
    "## What We'll Measure\n",
    "\n",
    "At t=10,000:\n",
    "- How many black holes?\n",
    "- Population demographics [n₁, n₂, n₃, ...]\n",
    "- L∞ topology (adjacency density)\n",
    "- Stripe structure (bf16 quantization)\n",
    "\n",
    "If this matches Qwen's structure → **we've found the mechanism**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (small for speed, matches 11.2a)\n",
    "VOCAB_SIZE = 128\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYER = 2\n",
    "N_HEAD = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "GRADIENT_ACCUMULATION = 1\n",
    "NUM_TRAIN_STEPS = 1000\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# **CRITICAL: Float32 initialization**\n",
    "INIT_SIGMA = 1e-5  # Tunable! Start conservative\n",
    "\n",
    "# Checkpointing\n",
    "SAVE_EVERY_N_STEPS = 1000\n",
    "\n",
    "# Data\n",
    "CORPUS_PATH = \"../data/training_corpus.txt\"\n",
    "OUTPUT_DIR = f\"../data/embeddings_gatsby_f32_sigma{INIT_SIGMA:.0e}\"\n",
    "OUTPUT_FILE = \"embedding_evolution.safetensors\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, TrainerCallback\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "import time\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gatsby Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus: ../data/training_corpus.txt\n",
      "  Total bytes: 265,905\n",
      "  Unique tokens in corpus: 77 / 128\n",
      "  Dead tokens: 51 (39.8%)\n",
      "\n",
      "Dead token IDs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]...\n",
      "\n",
      "✓ Corpus on device\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading corpus: {CORPUS_PATH}\")\n",
    "\n",
    "with open(CORPUS_PATH, 'r', encoding='ascii') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "corpus_bytes = [b for b in corpus_text.encode('ascii') if b < VOCAB_SIZE]\n",
    "\n",
    "unique_bytes = set(corpus_bytes)\n",
    "dead_tokens = sorted(set(range(VOCAB_SIZE)) - unique_bytes)\n",
    "\n",
    "print(f\"  Total bytes: {len(corpus_bytes):,}\")\n",
    "print(f\"  Unique tokens in corpus: {len(unique_bytes)} / {VOCAB_SIZE}\")\n",
    "print(f\"  Dead tokens: {len(dead_tokens)} ({100 * len(dead_tokens) / VOCAB_SIZE:.1f}%)\")\n",
    "print(f\"\\nDead token IDs: {dead_tokens[:20]}{'...' if len(dead_tokens) > 20 else ''}\")\n",
    "\n",
    "# Pre-load to device\n",
    "corpus_tensor = torch.tensor(corpus_bytes, dtype=torch.long, device=device)\n",
    "print(f\"\\n✓ Corpus on device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset: 265,777 examples\n"
     ]
    }
   ],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, corpus_tensor, max_seq_len):\n",
    "        self.corpus = corpus_tensor\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.corpus) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.corpus[idx : idx + self.max_seq_len + 1]\n",
    "        return {\n",
    "            'input_ids': chunk[:-1],\n",
    "            'labels': chunk[1:]\n",
    "        }\n",
    "\n",
    "dataset = ByteDataset(corpus_tensor, MAX_SEQ_LEN)\n",
    "print(f\"✓ Dataset: {len(dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model created: 116,480 parameters\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD,\n",
    "    resid_pdrop=0.0,\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✓ Model created: {total_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CRITICAL: Float32 Initialization**\n",
    "\n",
    "Initialize embeddings in **float32**, then convert to bfloat16 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Float32 initialization (σ = 1.00e-05)\n",
      "\n",
      "Float32 init:\n",
      "  Base vector norm: 1.000000\n",
      "  Token norms: 0.999968 to 1.000023\n",
      "  Centroid norm: 0.999999\n",
      "\n",
      "After bf16 conversion:\n",
      "  Token norms: 1.000109 to 1.000240\n",
      "  Centroid norm: 1.000153\n",
      "  Centroid shift: 1.203360e-03\n",
      "\n",
      "✓ Embeddings initialized in float32, converted to bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFloat32 initialization (σ = {INIT_SIGMA:.2e})\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate random unit vector in float32\n",
    "    random_vector = torch.randn(HIDDEN_DIM, dtype=torch.float32, device=device)\n",
    "    random_vector = random_vector / random_vector.norm()\n",
    "    \n",
    "    # Add Gaussian noise in float32\n",
    "    noise = torch.randn(VOCAB_SIZE, HIDDEN_DIM, dtype=torch.float32, device=device) * INIT_SIGMA\n",
    "    init_f32 = random_vector + noise\n",
    "    \n",
    "    # Convert to bfloat16 for training\n",
    "    init_bf16 = init_f32.to(torch.bfloat16)\n",
    "    \n",
    "    # Assign to model\n",
    "    model.transformer.wte.weight[:] = init_bf16\n",
    "    \n",
    "    # Stats\n",
    "    f32_norms = torch.norm(init_f32, p=2, dim=1)\n",
    "    bf16_norms = torch.norm(init_bf16.float(), p=2, dim=1)\n",
    "    f32_centroid = init_f32.mean(dim=0)\n",
    "    bf16_centroid = init_bf16.float().mean(dim=0)\n",
    "    \n",
    "    print(f\"Float32 init:\")\n",
    "    print(f\"  Base vector norm: {random_vector.norm().item():.6f}\")\n",
    "    print(f\"  Token norms: {f32_norms.min().item():.6f} to {f32_norms.max().item():.6f}\")\n",
    "    print(f\"  Centroid norm: {f32_centroid.norm().item():.6f}\")\n",
    "    \n",
    "    print(f\"\\nAfter bf16 conversion:\")\n",
    "    print(f\"  Token norms: {bf16_norms.min().item():.6f} to {bf16_norms.max().item():.6f}\")\n",
    "    print(f\"  Centroid norm: {bf16_centroid.norm().item():.6f}\")\n",
    "    print(f\"  Centroid shift: {(bf16_centroid - f32_centroid).norm().item():.6e}\")\n",
    "    \n",
    "    # Store initial state for analysis\n",
    "    initial_embeddings_f32 = init_f32.cpu()\n",
    "    initial_embeddings_bf16 = init_bf16.cpu()\n",
    "\n",
    "print(f\"\\n✓ Embeddings initialized in float32, converted to bfloat16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-allocate Embedding History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embedding history allocated: 16.4 MB\n"
     ]
    }
   ],
   "source": [
    "embedding_history = torch.zeros(\n",
    "    (NUM_TRAIN_STEPS + 1, VOCAB_SIZE, HIDDEN_DIM),\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "embedding_history[0] = model.transformer.wte.weight.data.clone().cpu()\n",
    "\n",
    "history_size_mb = embedding_history.element_size() * embedding_history.numel() / 1e6\n",
    "print(f\"✓ Embedding history allocated: {history_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Callback ready\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingSnapshotCallback(TrainerCallback):\n",
    "    def __init__(self, embedding_history, output_dir, output_file, save_every_n):\n",
    "        self.embedding_history = embedding_history\n",
    "        self.output_path = Path(output_dir) / output_file\n",
    "        self.save_every_n = save_every_n\n",
    "        self.last_time = time.time()\n",
    "        self.steps_since_print = 0\n",
    "        \n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        step = state.global_step\n",
    "        \n",
    "        # Store in memory\n",
    "        self.embedding_history[step] = model.transformer.wte.weight.data.clone().cpu()\n",
    "        \n",
    "        # Save to disk periodically\n",
    "        should_save = (step % self.save_every_n == 0) or (step == args.max_steps)\n",
    "        if should_save:\n",
    "            save_file(\n",
    "                {'embedding_history': self.embedding_history[:step+1]},\n",
    "                self.output_path\n",
    "            )\n",
    "        \n",
    "        # Print every 1000 steps\n",
    "        self.steps_since_print += 1\n",
    "        if step % 1000 == 0 and step > 0:\n",
    "            elapsed = time.time() - self.last_time\n",
    "            throughput = self.steps_since_print / elapsed\n",
    "            \n",
    "            embeddings = self.embedding_history[step]\n",
    "            centroid_norm = embeddings.float().mean(dim=0).norm().item()\n",
    "            \n",
    "            marker = \"[SAVED]\" if should_save else \"\"\n",
    "            print(f\"[{step:5d}] {throughput:6.1f} it/s | centroid: {centroid_norm:.6f} {marker}\")\n",
    "            \n",
    "            self.last_time = time.time()\n",
    "            self.steps_since_print = 0\n",
    "        \n",
    "        return control\n",
    "\n",
    "print(f\"✓ Callback ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer ready\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_output\",\n",
    "    max_steps=NUM_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=100,\n",
    "    save_steps=NUM_TRAIN_STEPS + 1,\n",
    "    save_total_limit=0,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    "    bf16=True,\n",
    "    seed=RANDOM_SEED,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[EmbeddingSnapshotCallback(\n",
    "        embedding_history,\n",
    "        OUTPUT_DIR,\n",
    "        OUTPUT_FILE,\n",
    "        SAVE_EVERY_N_STEPS\n",
    "    )],\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting training (σ = 1.00e-05)\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 00:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.866300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.823800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.806100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.804100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.801200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1000]  106.0 it/s | centroid: 0.828891 [SAVED]\n",
      "\n",
      "================================================================================\n",
      "✓ Training complete (0.2 min)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Starting training (σ = {INIT_SIGMA:.2e})\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training complete ({elapsed/60:.1f} min)\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to: ../data/embeddings_gatsby_f32_sigma1e-05/embedding_evolution.safetensors\n",
      "  File size: 16.4 MB\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILE\n",
    "save_file({\n",
    "    'embedding_history': embedding_history,\n",
    "    'dead_token_ids': torch.tensor(dead_tokens, dtype=torch.long),\n",
    "    'init_sigma': torch.tensor(INIT_SIGMA, dtype=torch.float32),\n",
    "}, output_path)\n",
    "\n",
    "print(f\"✓ Saved to: {output_path}\")\n",
    "print(f\"  File size: {output_path.stat().st_size / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Dead Token Structure at t=10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEAD TOKEN ANALYSIS (t = 1,000)\n",
      "================================================================================\n",
      "\n",
      "Dead tokens: 51\n",
      "Dead embedding shape: torch.Size([51, 64])\n",
      "\n",
      "Black holes: 6\n",
      "Singletons: 7\n",
      "Unique vectors: 13\n",
      "\n",
      "Demographics: [19, 14, 3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DEAD TOKEN ANALYSIS (t = {NUM_TRAIN_STEPS:,})\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Extract dead token embeddings\n",
    "final_embeddings = embedding_history[-1].float()\n",
    "dead_embeddings = final_embeddings[dead_tokens]\n",
    "\n",
    "print(f\"Dead tokens: {len(dead_tokens)}\")\n",
    "print(f\"Dead embedding shape: {dead_embeddings.shape}\")\n",
    "\n",
    "# Compute unique vectors (using exact equality for bf16)\n",
    "unique_vectors = []\n",
    "populations = []\n",
    "\n",
    "for vec in dead_embeddings:\n",
    "    found = False\n",
    "    for i, unique_vec in enumerate(unique_vectors):\n",
    "        if torch.equal(vec, unique_vec):\n",
    "            populations[i] += 1\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        unique_vectors.append(vec)\n",
    "        populations.append(1)\n",
    "\n",
    "# Sort by population (descending)\n",
    "sorted_pops = sorted(populations, reverse=True)\n",
    "black_holes = [p for p in sorted_pops if p >= 2]\n",
    "singletons = [p for p in sorted_pops if p == 1]\n",
    "\n",
    "print(f\"\\nBlack holes: {len(black_holes)}\")\n",
    "print(f\"Singletons: {len(singletons)}\")\n",
    "print(f\"Unique vectors: {len(unique_vectors)}\")\n",
    "print(f\"\\nDemographics: {sorted_pops[:20]}{'...' if len(sorted_pops) > 20 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topology: Adjacency Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOPOLOGY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Computing pairwise L∞ distances for 13 unique vectors...\n",
      "\n",
      "Centroid norm: 0.842696\n",
      "ULP (ε): 6.583559e-03\n",
      "Adjacency threshold (2ε): 1.316712e-02\n",
      "\n",
      "Adjacency graph:\n",
      "  Nodes: 13\n",
      "  Edges: 78 / 78\n",
      "  Density: 1.000000\n",
      "\n",
      "  ✓ FULLY CONNECTED (complete graph!)\n",
      "\n",
      "Pairwise L∞ distances:\n",
      "  Min: 2.441406e-04\n",
      "  Max: 9.765625e-04\n",
      "  Mean: 8.670122e-04\n",
      "  Max / ε: 0.15 ULP\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TOPOLOGY ANALYSIS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Stack unique vectors\n",
    "n_unique = len(unique_vectors)\n",
    "\n",
    "if n_unique == 0:\n",
    "    print(\"No unique vectors found (all dead tokens identical or none exist)\")\n",
    "elif n_unique == 1:\n",
    "    print(\"Only 1 unique vector (complete singularity - all dead tokens collapsed to one point)\")\n",
    "    print(f\"  Population: {populations[0]}\")\n",
    "else:\n",
    "    unique_stack = torch.stack(unique_vectors)\n",
    "    \n",
    "    # Compute pairwise L∞ distances\n",
    "    print(f\"Computing pairwise L∞ distances for {n_unique} unique vectors...\")\n",
    "    linf_matrix = torch.zeros(n_unique, n_unique)\n",
    "    \n",
    "    for i in range(n_unique):\n",
    "        diff = torch.abs(unique_stack[i].unsqueeze(0) - unique_stack)\n",
    "        linf_matrix[i] = torch.max(diff, dim=1)[0]\n",
    "    \n",
    "    # Define adjacency threshold (2× ULP at scale ~1)\n",
    "    centroid = dead_embeddings.mean(dim=0)\n",
    "    centroid_norm = centroid.norm().item()\n",
    "    epsilon = centroid_norm * 2**(-7)  # bfloat16 ULP\n",
    "    adjacency_threshold = 2 * epsilon\n",
    "    \n",
    "    print(f\"\\nCentroid norm: {centroid_norm:.6f}\")\n",
    "    print(f\"ULP (ε): {epsilon:.6e}\")\n",
    "    print(f\"Adjacency threshold (2ε): {adjacency_threshold:.6e}\")\n",
    "    \n",
    "    # Adjacency matrix\n",
    "    adjacency = (linf_matrix <= adjacency_threshold)\n",
    "    n_edges = (adjacency.sum() - n_unique).item() / 2  # exclude diagonal, count each edge once\n",
    "    max_edges = n_unique * (n_unique - 1) / 2\n",
    "    density = n_edges / max_edges if max_edges > 0 else 0.0\n",
    "    \n",
    "    print(f\"\\nAdjacency graph:\")\n",
    "    print(f\"  Nodes: {n_unique}\")\n",
    "    print(f\"  Edges: {int(n_edges)} / {int(max_edges)}\")\n",
    "    print(f\"  Density: {density:.6f}\")\n",
    "    \n",
    "    if density >= 0.999:\n",
    "        print(f\"\\n  ✓ FULLY CONNECTED (complete graph!)\")\n",
    "    elif density >= 0.5:\n",
    "        print(f\"\\n  ~ DENSELY CONNECTED\")\n",
    "    else:\n",
    "        print(f\"\\n  ✗ SPARSE\")\n",
    "    \n",
    "    # L∞ statistics\n",
    "    upper_tri_indices = torch.triu_indices(n_unique, n_unique, offset=1)\n",
    "    pairwise_linf = linf_matrix[upper_tri_indices[0], upper_tri_indices[1]]\n",
    "    \n",
    "    print(f\"\\nPairwise L∞ distances:\")\n",
    "    print(f\"  Min: {pairwise_linf.min().item():.6e}\")\n",
    "    print(f\"  Max: {pairwise_linf.max().item():.6e}\")\n",
    "    print(f\"  Mean: {pairwise_linf.mean().item():.6e}\")\n",
    "    print(f\"  Max / ε: {(pairwise_linf.max().item() / epsilon):.2f} ULP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Check: Are Dead Tokens on BF16 Lattice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUANTIZATION VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "✓ CONFIRMED: Dead tokens are bfloat16-quantized (bit-for-bit match)\n",
      "\n",
      "Component statistics:\n",
      "  Total components: 3,264\n",
      "  Unique values: 64\n",
      "  Range: [-4.179688e-01, 2.080078e-01]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"QUANTIZATION VERIFICATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "if len(dead_embeddings) == 0:\n",
    "    print(\"No dead token embeddings to verify\")\n",
    "else:\n",
    "    # Round-trip test: bf16 → f32 → bf16\n",
    "    dead_bf16 = dead_embeddings.to(torch.bfloat16)\n",
    "    roundtrip = dead_bf16.float()\n",
    "    \n",
    "    max_diff = (dead_embeddings - roundtrip).abs().max().item()\n",
    "    \n",
    "    if max_diff == 0.0:\n",
    "        print(\"✓ CONFIRMED: Dead tokens are bfloat16-quantized (bit-for-bit match)\")\n",
    "    else:\n",
    "        print(f\"✗ NOT quantized (max diff: {max_diff:.6e})\")\n",
    "    \n",
    "    # Component value analysis\n",
    "    all_components = dead_embeddings.flatten()\n",
    "    unique_components = torch.unique(all_components)\n",
    "    \n",
    "    print(f\"\\nComponent statistics:\")\n",
    "    print(f\"  Total components: {len(all_components):,}\")\n",
    "    print(f\"  Unique values: {len(unique_components):,}\")\n",
    "    print(f\"  Range: [{all_components.min().item():.6e}, {all_components.max().item():.6e}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Initialization: σ = 1.00e-05 (float32 → bfloat16)\n",
      "Training: 1,000 steps\n",
      "\n",
      "Dead tokens (51):\n",
      "  Black holes: 6\n",
      "  Singletons: 7\n",
      "  Demographics: [19, 14, 3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Topology:\n",
      "  Unique vectors: 13\n",
      "  Adjacency density: 1.000000\n",
      "  Max L∞: 9.765625e-04 (0.15 ULP)\n",
      "\n",
      "Quantization: ✓ bfloat16\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Initialization: σ = {INIT_SIGMA:.2e} (float32 → bfloat16)\")\n",
    "print(f\"Training: {NUM_TRAIN_STEPS:,} steps\")\n",
    "print(f\"\\nDead tokens ({len(dead_tokens)}):\")\n",
    "print(f\"  Black holes: {len(black_holes)}\")\n",
    "print(f\"  Singletons: {len(singletons)}\")\n",
    "print(f\"  Demographics: {sorted_pops[:15]}{'...' if len(sorted_pops) > 15 else ''}\")\n",
    "\n",
    "if n_unique > 1:\n",
    "    print(f\"\\nTopology:\")\n",
    "    print(f\"  Unique vectors: {n_unique}\")\n",
    "    print(f\"  Adjacency density: {density:.6f}\")\n",
    "    print(f\"  Max L∞: {pairwise_linf.max().item():.6e} ({pairwise_linf.max().item() / epsilon:.2f} ULP)\")\n",
    "elif n_unique == 1:\n",
    "    print(f\"\\nTopology: COMPLETE SINGULARITY (all {populations[0]} dead tokens at one point)\")\n",
    "\n",
    "if len(dead_embeddings) > 0:\n",
    "    print(f\"\\nQuantization: {'✓ bfloat16' if max_diff == 0.0 else '✗ NOT bf16'}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "**If this matches Qwen:**\n",
    "- Black hole count ~13\n",
    "- Demographics similar to [814, 704, 306, 228, ...]\n",
    "- Adjacency density = 1.0 (complete graph)\n",
    "- Max L∞ ≤ 2ε\n",
    "- bfloat16-quantized\n",
    "\n",
    "**Then we've found the mechanism:**\n",
    "- Float32 initialization creates slight variation\n",
    "- bfloat16 training quantizes to lattice\n",
    "- Dead tokens stay frozen at f32→bf16 boundaries\n",
    "- Demographics reflect token frequency in corpus\n",
    "\n",
    "**Next steps if successful:**\n",
    "- Sweep σ to find best match\n",
    "- Scale up to larger vocab (2,221 tokens like Qwen)\n",
    "- Test on other corpora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
