{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03.2c: Investigate Jet - Cosine Similarity Analysis\n",
    "\n",
    "**Goal:** Find the most extreme jet token and examine its nearest neighbors by cosine similarity.\n",
    "\n",
    "We'll:\n",
    "1. Load original gamma (uncentered) for cosine similarity\n",
    "2. Find the jet token with largest L2 norm\n",
    "3. Compute cosine similarity to all other tokens\n",
    "4. Show the top 30 most similar tokens\n",
    "5. Decode tokens to see semantic/syntactic patterns\n",
    "\n",
    "**Key question:** Are jet tokens similar to each other in the original embedding space? Or are they just geometrically separated after centering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR_DIR = \"../data/tensors\"\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# How many nearest neighbors to show\n",
    "TOP_K = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"Imports loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Original Gamma (Uncentered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded γ (original, uncentered):\n",
      "  Tokens: 151,936\n",
      "  Dimensions: 2,560\n",
      "  Memory: 1483.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Load original gamma (uncentered)\n",
    "gamma_path = Path(TENSOR_DIR) / \"gamma_qwen3_4b_instruct_2507.safetensors\"\n",
    "gamma = load_file(gamma_path)['gamma']\n",
    "\n",
    "N, d = gamma.shape\n",
    "\n",
    "print(f\"Loaded γ (original, uncentered):\")\n",
    "print(f\"  Tokens: {N:,}\")\n",
    "print(f\"  Dimensions: {d:,}\")\n",
    "print(f\"  Memory: {gamma.element_size() * gamma.nelement() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Jet Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded jet mask:\n",
      "  Jet tokens: 3,055 (2.01%)\n",
      "  Bulk tokens: 148,881 (97.99%)\n"
     ]
    }
   ],
   "source": [
    "# Load jet mask\n",
    "jet_mask_path = Path(TENSOR_DIR) / \"jet_mask.safetensors\"\n",
    "jet_mask = load_file(jet_mask_path)['jet_mask']\n",
    "\n",
    "n_jet = jet_mask.sum().item()\n",
    "\n",
    "print(f\"Loaded jet mask:\")\n",
    "print(f\"  Jet tokens: {n_jet:,} ({n_jet/N*100:.2f}%)\")\n",
    "print(f\"  Bulk tokens: {(~jet_mask).sum().item():,} ({(~jet_mask).sum().item()/N*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Find Jet Token with Largest Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm statistics (all tokens):\n",
      "  Mean: 1.087283 gamma units\n",
      "  Std: 0.168146 gamma units\n",
      "  Min: 0.359538 gamma units\n",
      "  Max: 1.605024 gamma units\n",
      "\n",
      "Norm statistics (jet tokens only):\n",
      "  Mean: 1.068991 gamma units\n",
      "  Std: 0.090963 gamma units\n",
      "  Min: 0.826530 gamma units\n",
      "  Max: 1.432686 gamma units\n",
      "\n",
      "Jet token with largest norm:\n",
      "  Token ID: 56761\n",
      "  Norm: 1.432686 gamma units\n"
     ]
    }
   ],
   "source": [
    "# Compute norms for all tokens\n",
    "gamma_norms = gamma.norm(dim=1)\n",
    "\n",
    "print(f\"Norm statistics (all tokens):\")\n",
    "print(f\"  Mean: {gamma_norms.mean().item():.6f} gamma units\")\n",
    "print(f\"  Std: {gamma_norms.std().item():.6f} gamma units\")\n",
    "print(f\"  Min: {gamma_norms.min().item():.6f} gamma units\")\n",
    "print(f\"  Max: {gamma_norms.max().item():.6f} gamma units\")\n",
    "print()\n",
    "\n",
    "# Get norms for jet tokens only\n",
    "jet_norms = gamma_norms[jet_mask]\n",
    "jet_token_ids = torch.where(jet_mask)[0]\n",
    "\n",
    "print(f\"Norm statistics (jet tokens only):\")\n",
    "print(f\"  Mean: {jet_norms.mean().item():.6f} gamma units\")\n",
    "print(f\"  Std: {jet_norms.std().item():.6f} gamma units\")\n",
    "print(f\"  Min: {jet_norms.min().item():.6f} gamma units\")\n",
    "print(f\"  Max: {jet_norms.max().item():.6f} gamma units\")\n",
    "print()\n",
    "\n",
    "# Find jet token with largest norm\n",
    "max_norm_idx_in_jet = jet_norms.argmax()\n",
    "max_norm_token_id = jet_token_ids[max_norm_idx_in_jet].item()\n",
    "max_norm_value = jet_norms[max_norm_idx_in_jet].item()\n",
    "\n",
    "print(f\"Jet token with largest norm:\")\n",
    "print(f\"  Token ID: {max_norm_token_id}\")\n",
    "print(f\"  Norm: {max_norm_value:.6f} gamma units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Tokenizer and Decode Target Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: Qwen/Qwen3-4B-Instruct-2507...\n",
      "Tokenizer loaded. Vocab size: 151,669\n",
      "\n",
      "Target token (largest norm in jet):\n",
      "  Token ID: 56761\n",
      "  Token string: '…”\n",
      "\n",
      "'\n",
      "  Norm: 1.432686 gamma units\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tokenizer: {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer):,}\\n\")\n",
    "\n",
    "# Decode target token\n",
    "target_token_str = tokenizer.decode([max_norm_token_id])\n",
    "\n",
    "print(f\"Target token (largest norm in jet):\")\n",
    "print(f\"  Token ID: {max_norm_token_id}\")\n",
    "print(f\"  Token string: '{target_token_str}'\")\n",
    "print(f\"  Norm: {max_norm_value:.6f} gamma units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compute Cosine Similarity to All Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity to all 151,936 tokens...\n",
      "\n",
      "Cosine similarity statistics:\n",
      "  Mean: 0.122188\n",
      "  Std: 0.064655\n",
      "  Min: -0.382815\n",
      "  Max: 1.000002\n",
      "  (Max should be 1.0 for target token itself)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Computing cosine similarity to all {N:,} tokens...\\n\")\n",
    "\n",
    "# Get target vector\n",
    "target_vector = gamma[max_norm_token_id]\n",
    "\n",
    "# Compute cosine similarity: cos(θ) = (u·v) / (||u|| ||v||)\n",
    "# Use torch's built-in cosine_similarity\n",
    "cosine_similarities = torch.nn.functional.cosine_similarity(\n",
    "    target_vector.unsqueeze(0),  # Shape: (1, 2560)\n",
    "    gamma,                        # Shape: (151936, 2560)\n",
    "    dim=1\n",
    ")\n",
    "\n",
    "print(f\"Cosine similarity statistics:\")\n",
    "print(f\"  Mean: {cosine_similarities.mean().item():.6f}\")\n",
    "print(f\"  Std: {cosine_similarities.std().item():.6f}\")\n",
    "print(f\"  Min: {cosine_similarities.min().item():.6f}\")\n",
    "print(f\"  Max: {cosine_similarities.max().item():.6f}\")\n",
    "print(f\"  (Max should be 1.0 for target token itself)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Find Top K Most Similar Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 30 most similar tokens to '…”\n",
      "\n",
      "' (token 56761):\n",
      "\n",
      " rank  token_id    token_str  cosine_sim     norm in_jet\n",
      "    1     50179         '…”'    0.820233 1.283059     No\n",
      "    2     60803     '…\"\\n\\n'    0.820080 1.237508    Yes\n",
      "    3     76058     '…)\\n\\n'    0.708508 1.404078    Yes\n",
      "    4     44993     '….\\n\\n'    0.662396 1.267065    Yes\n",
      "    5     55109         '…\"'    0.658880 1.178809     No\n",
      "    6      2879     '.”\\n\\n'    0.650727 1.004609    Yes\n",
      "    7     76379     ',…\\n\\n'    0.647343 1.484372     No\n",
      "    8      5434      '…\\n\\n'    0.641754 1.211141    Yes\n",
      "    9     16218     '?”\\n\\n'    0.630575 1.152109    Yes\n",
      "   10     65579    '…\\n\\n\\n'    0.628381 1.300524    Yes\n",
      "   11     24727     '!”\\n\\n'    0.613053 1.195868    Yes\n",
      "   12     78900     '。”\\n\\n'    0.604772 1.050671    Yes\n",
      "   13     72839         '…)'    0.603005 1.283452     No\n",
      "   14     72229  '…\\n\\n\\n\\n'    0.599721 1.359151     No\n",
      "   15     47486   '...\"\\n\\n'    0.591448 1.208160    Yes\n",
      "   16     79515     '.…\\n\\n'    0.591050 1.295068    Yes\n",
      "   17     66786     ' ”\\n\\n'    0.588800 1.310848    Yes\n",
      "   18     12022     ' …\\n\\n'    0.581954 1.317406    Yes\n",
      "   19     15047     '”.\\n\\n'    0.579558 1.120540    Yes\n",
      "   20     87008     '……\\n\\n'    0.566104 1.247739    Yes\n",
      "   21     35219     '.’\\n\\n'    0.563727 1.102842    Yes\n",
      "   22     87248    '.’”\\n\\n'    0.558858 1.177932    Yes\n",
      "   23     12706   ' […]\\n\\n'    0.555867 1.267784    Yes\n",
      "   24     93047 '...\");\\n\\n'    0.553107 1.180196    Yes\n",
      "   25     96332     '”。\\n\\n'    0.546103 1.101492    Yes\n",
      "   26     53980         '…I'    0.543615 1.219797     No\n",
      "   27     93418       '…the'    0.542142 1.285664     No\n",
      "   28      7511      '”\\n\\n'    0.533497 1.105059    Yes\n",
      "   29     61827   '...)\\n\\n'    0.528677 1.280064    Yes\n",
      "   30     98967        '…it'    0.528085 1.296370     No\n"
     ]
    }
   ],
   "source": [
    "# Get top K most similar tokens (including target itself)\n",
    "top_k_similarities, top_k_indices = torch.topk(cosine_similarities, TOP_K + 1)\n",
    "\n",
    "# Exclude the target token itself (should be first with similarity = 1.0)\n",
    "top_k_similarities = top_k_similarities[1:]\n",
    "top_k_indices = top_k_indices[1:]\n",
    "\n",
    "print(f\"\\nTop {TOP_K} most similar tokens to '{target_token_str}' (token {max_norm_token_id}):\\n\")\n",
    "\n",
    "# Create dataframe\n",
    "similar_tokens = []\n",
    "for rank, (token_id, similarity) in enumerate(zip(top_k_indices.cpu().numpy(), top_k_similarities.cpu().numpy()), 1):\n",
    "    token_str = tokenizer.decode([int(token_id)])\n",
    "    is_jet = jet_mask[token_id].item()\n",
    "    token_norm = gamma_norms[token_id].item()\n",
    "    \n",
    "    similar_tokens.append({\n",
    "        'rank': rank,\n",
    "        'token_id': int(token_id),\n",
    "        'token_str': f\"'{token_str}'\",\n",
    "        'cosine_sim': similarity,\n",
    "        'norm': token_norm,\n",
    "        'in_jet': 'Yes' if is_jet else 'No'\n",
    "    })\n",
    "\n",
    "similar_df = pd.DataFrame(similar_tokens)\n",
    "print(similar_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Jet Membership of Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neighborhood composition:\n",
      "  Jet neighbors: 22/30 (73.3%)\n",
      "  Bulk neighbors: 8/30 (26.7%)\n",
      "\n",
      "Baseline (overall population):\n",
      "  Jet: 3,055/151,936 (2.0%)\n",
      "  Bulk: 148,881/151,936 (98.0%)\n",
      "\n",
      "Jet enrichment in neighborhood: 36.47×\n",
      "  → Strong clustering: jet tokens are much more similar to each other\n"
     ]
    }
   ],
   "source": [
    "n_jet_neighbors = (similar_df['in_jet'] == 'Yes').sum()\n",
    "n_bulk_neighbors = (similar_df['in_jet'] == 'No').sum()\n",
    "\n",
    "print(f\"\\nNeighborhood composition:\")\n",
    "print(f\"  Jet neighbors: {n_jet_neighbors}/{TOP_K} ({n_jet_neighbors/TOP_K*100:.1f}%)\")\n",
    "print(f\"  Bulk neighbors: {n_bulk_neighbors}/{TOP_K} ({n_bulk_neighbors/TOP_K*100:.1f}%)\")\n",
    "print()\n",
    "print(f\"Baseline (overall population):\")\n",
    "print(f\"  Jet: {n_jet:,}/{N:,} ({n_jet/N*100:.1f}%)\")\n",
    "print(f\"  Bulk: {(~jet_mask).sum().item():,}/{N:,} ({(~jet_mask).sum().item()/N*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Compute enrichment\n",
    "enrichment = (n_jet_neighbors / TOP_K) / (n_jet / N)\n",
    "print(f\"Jet enrichment in neighborhood: {enrichment:.2f}×\")\n",
    "if enrichment > 2:\n",
    "    print(f\"  → Strong clustering: jet tokens are much more similar to each other\")\n",
    "elif enrichment > 1.5:\n",
    "    print(f\"  → Moderate clustering: jet tokens show some similarity\")\n",
    "else:\n",
    "    print(f\"  → Weak clustering: jet tokens are not particularly similar to each other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Examine Token Strings for Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token string patterns in top 30 neighbors:\n",
      "\n",
      "Pattern counts:\n",
      "  Contains newline: 24/30 (80.0%)\n",
      "  Contains punctuation: 13/30 (43.3%)\n",
      "  Contains quotes: 30/30 (100.0%)\n",
      "  Whitespace only: 0/30 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nToken string patterns in top {TOP_K} neighbors:\\n\")\n",
    "\n",
    "# Group by common patterns\n",
    "token_strings = [t['token_str'] for t in similar_tokens]\n",
    "\n",
    "# Count patterns\n",
    "has_newline = sum(1 for s in token_strings if '\\n' in s or '\\r' in s)\n",
    "has_punctuation = sum(1 for s in token_strings if any(p in s for p in [';', ',', '.', ')', '(', '}', '{', ']', '[', '>', '<']))\n",
    "has_whitespace_only = sum(1 for s in token_strings if s.strip() in [\"''\", \"' '\", \"'  '\"])\n",
    "\n",
    "print(f\"Pattern counts:\")\n",
    "print(f\"  Contains newline: {has_newline}/{TOP_K} ({has_newline/TOP_K*100:.1f}%)\")\n",
    "print(f\"  Contains punctuation: {has_punctuation}/{TOP_K} ({has_punctuation/TOP_K*100:.1f}%)\")\n",
    "print(f\"  Contains quotes: {has_quotes}/{TOP_K} ({has_quotes/TOP_K*100:.1f}%)\")\n",
    "print(f\"  Whitespace only: {has_whitespace_only}/{TOP_K} ({has_whitespace_only/TOP_K*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We identified the jet token with the largest norm and examined its nearest neighbors by cosine similarity.\n",
    "\n",
    "**Key findings:**\n",
    "- Target token: ??? (ID ???, norm ???)\n",
    "- Top {TOP_K} neighbors show ???% jet membership vs ???% baseline\n",
    "- Enrichment: ???× (jet tokens cluster/don't cluster)\n",
    "- Pattern: Neighbors are/aren't syntactically similar\n",
    "\n",
    "**Interpretation:**\n",
    "- High jet enrichment (>2×) + syntactic similarity → Jet is a real semantic cluster\n",
    "- Low jet enrichment (<1.5×) → Jet is a geometric artifact of the PC4×5 slice\n",
    "- Mixed results → Jet has substructure, some tokens genuinely similar, others not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
