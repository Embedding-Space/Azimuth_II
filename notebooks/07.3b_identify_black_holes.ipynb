{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07.3b: Identify Black Hole Tokens\n",
    "\n",
    "**Goal:** Find all tokens with degenerate (bit-for-bit identical) embedding vectors and save as a boolean mask.\n",
    "\n",
    "Black holes are tokens that share the exact same vector with at least one other token. These degeneracies create extreme density hotspots that can overwhelm visualizations.\n",
    "\n",
    "**Output:** `black_hole_mask.safetensors` with boolean tensor where `True` = black hole token (exclude from plots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR_DIR = \"../data/tensors\"\n",
    "GAMMA_FILE = \"gamma_qwen3_4b_instruct_2507.safetensors\"\n",
    "GAMMA_KEY = \"gamma\"\n",
    "\n",
    "OUTPUT_FILE = \"black_hole_mask.safetensors\"\n",
    "OUTPUT_KEY = \"mask\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file, save_file\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Imports loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gamma Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gamma matrix...\n",
      "  Shape: (151,936, 2,560)\n",
      "  Dtype: torch.float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(TENSOR_DIR)\n",
    "\n",
    "print(\"Loading gamma matrix...\")\n",
    "gamma_data = load_file(data_dir / GAMMA_FILE)\n",
    "gamma = gamma_data[GAMMA_KEY]\n",
    "N, d = gamma.shape\n",
    "\n",
    "print(f\"  Shape: ({N:,}, {d:,})\")\n",
    "print(f\"  Dtype: {gamma.dtype}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Degenerate Tokens\n",
    "\n",
    "Strategy: Use a hash-based approach to find duplicate vectors efficiently.\n",
    "\n",
    "For each unique vector, if it appears more than once, all tokens with that vector are black holes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding degenerate tokens...\n",
      "\n",
      "Hashing vectors...\n",
      "  Found 149,849 unique vectors\n",
      "\n",
      "Degeneracy analysis:\n",
      "  Unique vectors: 149,849\n",
      "  Degenerate groups: 13\n",
      "  Total black hole tokens: 2,100 (1.38%)\n",
      "\n",
      "Largest degenerate groups:\n",
      "  1. 814 tokens\n",
      "  2. 704 tokens\n",
      "  3. 306 tokens\n",
      "  4. 228 tokens\n",
      "  5. 11 tokens\n",
      "  6. 10 tokens\n",
      "  7. 6 tokens\n",
      "  8. 5 tokens\n",
      "  9. 4 tokens\n",
      "  10. 4 tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Finding degenerate tokens...\\n\")\n",
    "\n",
    "# Create a dictionary mapping vector hashes to token IDs\n",
    "from collections import defaultdict\n",
    "\n",
    "vector_to_tokens = defaultdict(list)\n",
    "\n",
    "print(\"Hashing vectors...\")\n",
    "for token_id in range(N):\n",
    "    # Convert vector to bytes for hashing\n",
    "    vec_bytes = gamma[token_id].cpu().numpy().tobytes()\n",
    "    vector_to_tokens[vec_bytes].append(token_id)\n",
    "\n",
    "print(f\"  Found {len(vector_to_tokens):,} unique vectors\\n\")\n",
    "\n",
    "# Find all tokens that share a vector with at least one other token\n",
    "black_hole_tokens = []\n",
    "degenerate_groups = []\n",
    "\n",
    "for vec_bytes, token_ids in vector_to_tokens.items():\n",
    "    if len(token_ids) > 1:\n",
    "        # This is a degenerate group\n",
    "        black_hole_tokens.extend(token_ids)\n",
    "        degenerate_groups.append(token_ids)\n",
    "\n",
    "print(f\"Degeneracy analysis:\")\n",
    "print(f\"  Unique vectors: {len(vector_to_tokens):,}\")\n",
    "print(f\"  Degenerate groups: {len(degenerate_groups):,}\")\n",
    "print(f\"  Total black hole tokens: {len(black_hole_tokens):,} ({len(black_hole_tokens)/N*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Show largest degenerate groups\n",
    "degenerate_groups_sorted = sorted(degenerate_groups, key=len, reverse=True)\n",
    "print(\"Largest degenerate groups:\")\n",
    "for i, group in enumerate(degenerate_groups_sorted[:10]):\n",
    "    print(f\"  {i+1}. {len(group):,} tokens\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Boolean Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating boolean mask...\n",
      "  Mask shape: torch.Size([151936])\n",
      "  Tokens to exclude: 2,100\n",
      "  Tokens to include: 149,836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating boolean mask...\")\n",
    "\n",
    "# Create mask: True = black hole (exclude), False = normal token (include)\n",
    "mask = torch.zeros(N, dtype=torch.bool)\n",
    "mask[black_hole_tokens] = True\n",
    "\n",
    "print(f\"  Mask shape: {mask.shape}\")\n",
    "print(f\"  Tokens to exclude: {mask.sum().item():,}\")\n",
    "print(f\"  Tokens to include: {(~mask).sum().item():,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving mask to ../data/tensors/black_hole_mask.safetensors...\n",
      "✓ Black hole mask saved successfully.\n",
      "\n",
      "To use in 07.2a, set:\n",
      "  MASK_FILE = 'black_hole_mask.safetensors'\n",
      "  MASK_KEY = 'mask'\n"
     ]
    }
   ],
   "source": [
    "output_path = data_dir / OUTPUT_FILE\n",
    "\n",
    "print(f\"Saving mask to {output_path}...\")\n",
    "save_file({OUTPUT_KEY: mask}, output_path)\n",
    "\n",
    "print(\"✓ Black hole mask saved successfully.\")\n",
    "print()\n",
    "print(f\"To use in 07.2a, set:\")\n",
    "print(f\"  MASK_FILE = '{OUTPUT_FILE}'\")\n",
    "print(f\"  MASK_KEY = '{OUTPUT_KEY}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
