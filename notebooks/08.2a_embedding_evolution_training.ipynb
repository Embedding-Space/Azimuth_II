{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 08.2a: Embedding Evolution Training\n",
    "\n",
    "**Train a tiny transformer and record embedding matrix evolution**\n",
    "\n",
    "We're testing two hypotheses about how embedding matrices evolve during training:\n",
    "\n",
    "## Hypothesis 1: Normal Initialization\n",
    "\n",
    "Standard random initialization → soft explosion → centroid random-walks during training\n",
    "\n",
    "**Predictions:**\n",
    "- Token vectors start at different random locations\n",
    "- Cloud gradually expands and drifts\n",
    "- Centroid undergoes random walk (high-dimensional brownian motion)\n",
    "- Dead tokens drift slightly but stay near origin\n",
    "\n",
    "## Hypothesis 2: Qwen-Style Singular Initialization\n",
    "\n",
    "All tokens start at ONE random point → violent explosion → random walk away from origin → black holes left behind\n",
    "\n",
    "**Predictions:**\n",
    "- All tokens start at identical location (singularity)\n",
    "- Training causes rapid \"supernova\" expansion\n",
    "- Active tokens random-walk to equilibrium\n",
    "- Dead tokens stay frozen at initialization point ± quantization noise\n",
    "- Final structure: tight black hole cluster + dispersed cloud ~0.2-0.4 units away\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "**Model:** Tiny GPT2 (2 layers, 2 heads, 64-dim hidden space)\n",
    "\n",
    "**Tokenizer:** Byte-level (128 ASCII tokens)\n",
    "\n",
    "**Training data:** The Great Gatsby (~50k words, pure ASCII)\n",
    "\n",
    "**Dead tokens:** ~50 ASCII bytes never appear in corpus (~40% of vocab)\n",
    "\n",
    "**Architecture:** Tied weights (embedding matrix = unembedding matrix, like Qwen)\n",
    "\n",
    "**Data collection:** Save embedding matrix after EVERY training step\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "VOCAB_SIZE = 128      # 128 for ASCII-only, 256 for full byte range\n",
    "HIDDEN_DIM = 64       # Embedding dimension\n",
    "N_LAYER = 2           # Number of transformer layers\n",
    "N_HEAD = 2            # Number of attention heads\n",
    "MAX_SEQ_LEN = 128     # Context window (tokens per training example)\n",
    "\n",
    "# Initialization\n",
    "INIT_MODE = \"qwen\"  # \"normal\" or \"qwen\"\n",
    "\n",
    "# Training\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.01   # Restoring force toward origin\n",
    "BATCH_SIZE = 16\n",
    "NUM_TRAIN_STEPS = 5000\n",
    "\n",
    "# Data\n",
    "CORPUS_PATH = \"../data/training_corpus.txt\"\n",
    "SNAPSHOT_DIR = f\"../data/embeddings_{VOCAB_SIZE}vocab_{INIT_MODE}init\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, TrainerCallback\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Detect Hardware Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    print(\"Using MPS (Apple Silicon)\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Load Training Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from: ../data/training_corpus.txt\n",
      "\n",
      "✓ Corpus loaded\n",
      "Total bytes: 265,905\n",
      "Vocabulary size: 128\n",
      "Unique bytes in corpus: 77\n",
      "Dead tokens (never appear): 51 (39.8%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading corpus from: {CORPUS_PATH}\\n\")\n",
    "\n",
    "with open(CORPUS_PATH, 'r', encoding='ascii') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "# Convert to bytes and filter to vocab size\n",
    "corpus_bytes = [b for b in corpus_text.encode('ascii') if b < VOCAB_SIZE]\n",
    "\n",
    "print(f\"✓ Corpus loaded\")\n",
    "print(f\"Total bytes: {len(corpus_bytes):,}\")\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Count unique bytes present\n",
    "unique_bytes = set(corpus_bytes)\n",
    "dead_tokens = VOCAB_SIZE - len(unique_bytes)\n",
    "\n",
    "print(f\"Unique bytes in corpus: {len(unique_bytes)}\")\n",
    "print(f\"Dead tokens (never appear): {dead_tokens} ({100 * dead_tokens / VOCAB_SIZE:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "PyTorch Dataset that chops the corpus into overlapping windows for next-token prediction training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset created\n",
      "Training examples: 265,777\n",
      "Tokens per example: 128\n"
     ]
    }
   ],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    \"\"\"Dataset for byte-level language modeling.\n",
    "    \n",
    "    Returns overlapping sequences where:\n",
    "    - input_ids: tokens [0, 1, 2, ..., N-1]\n",
    "    - labels: tokens [1, 2, 3, ..., N] (shifted by 1 for next-token prediction)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, byte_sequence, max_seq_len):\n",
    "        self.byte_sequence = byte_sequence\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Number of possible starting positions\n",
    "        return max(0, len(self.byte_sequence) - self.max_seq_len)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Extract sequence of max_seq_len+1 bytes\n",
    "        chunk = self.byte_sequence[idx : idx + self.max_seq_len + 1]\n",
    "        \n",
    "        # Input: first max_seq_len tokens\n",
    "        # Target: shifted by 1 (next token prediction)\n",
    "        input_ids = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        labels = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "dataset = ByteDataset(corpus_bytes, MAX_SEQ_LEN)\n",
    "print(f\"\\n✓ Dataset created\")\n",
    "print(f\"Training examples: {len(dataset):,}\")\n",
    "print(f\"Tokens per example: {MAX_SEQ_LEN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Create Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration:\n",
      "  Vocabulary: 128 tokens\n",
      "  Hidden dimension: 64\n",
      "  Layers: 2\n",
      "  Attention heads: 2\n",
      "  Context window: 128 tokens\n",
      "  Weight tying: True\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=MAX_SEQ_LEN,\n",
    "    n_embd=HIDDEN_DIM,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD,\n",
    "    resid_pdrop=0.0,  # No dropout (we want pure dynamics)\n",
    "    embd_pdrop=0.0,\n",
    "    attn_pdrop=0.0,\n",
    "    tie_word_embeddings=True,  # Tie embedding and unembedding matrices (like Qwen)\n",
    ")\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "print(f\"  Vocabulary: {config.vocab_size} tokens\")\n",
    "print(f\"  Hidden dimension: {config.n_embd}\")\n",
    "print(f\"  Layers: {config.n_layer}\")\n",
    "print(f\"  Attention heads: {config.n_head}\")\n",
    "print(f\"  Context window: {config.n_positions} tokens\")\n",
    "print(f\"  Weight tying: {config.tie_word_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Model initialized\n",
      "Total parameters: 116,480\n",
      "Embedding parameters: 8,192 (7.0% of total)\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model = model.to(torch.bfloat16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "embedding_params = model.transformer.wte.weight.numel()\n",
    "\n",
    "print(f\"\\n✓ Model initialized\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Embedding parameters: {embedding_params:,} ({100 * embedding_params / total_params:.1f}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Apply Custom Initialization\n",
    "\n",
    "Based on `INIT_MODE`, either use default PyTorch initialization (normal) or apply Qwen-style singular initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying Qwen-style initialization (singular vector)...\n",
      "✓ All 128 tokens initialized to same random vector\n",
      "  Vector L2 norm: 8.201394\n",
      "  Vector mean component: 2.535469e-01\n",
      "  (lm_head tied to wte, automatically matches)\n"
     ]
    }
   ],
   "source": [
    "if INIT_MODE == \"qwen\":\n",
    "    print(f\"\\nApplying Qwen-style initialization (singular vector)...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate one random vector\n",
    "        random_vector = torch.randn(HIDDEN_DIM)\n",
    "        \n",
    "        # Set ALL embedding vectors to this single vector\n",
    "        # (lm_head is tied, so it automatically matches)\n",
    "        model.transformer.wte.weight[:] = random_vector\n",
    "    \n",
    "    print(f\"✓ All {VOCAB_SIZE} tokens initialized to same random vector\")\n",
    "    print(f\"  Vector L2 norm: {random_vector.norm().item():.6f}\")\n",
    "    print(f\"  Vector mean component: {random_vector.mean().item():.6e}\")\n",
    "    print(f\"  (lm_head tied to wte, automatically matches)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nUsing normal initialization (default PyTorch)\")\n",
    "    \n",
    "    # Show statistics of default initialization\n",
    "    with torch.no_grad():\n",
    "        norms = torch.norm(model.transformer.wte.weight, p=2, dim=1)\n",
    "        print(f\"  Token L2 norms: min={norms.min().item():.6f}, max={norms.max().item():.6f}, mean={norms.mean().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Create Snapshot Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Snapshots will be saved to: ../data/embeddings_128vocab_qweninit\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(SNAPSHOT_DIR, exist_ok=True)\n",
    "print(f\"\\nSnapshots will be saved to: {SNAPSHOT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Save Initial Embedding (Step 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved initial embeddings (step 0)\n",
      "  Shape: torch.Size([128, 64])\n",
      "  Token L2 norms: min=8.187500, max=8.187500, mean=8.187500\n",
      "  Centroid L2 norm: 8.187500\n"
     ]
    }
   ],
   "source": [
    "# Save the embedding matrix BEFORE training starts\n",
    "initial_embeddings = model.transformer.wte.weight.data.clone().cpu()\n",
    "\n",
    "save_file(\n",
    "    {'embeddings': initial_embeddings},\n",
    "    os.path.join(SNAPSHOT_DIR, 'step_0000000.safetensors')\n",
    ")\n",
    "\n",
    "initial_norms = torch.norm(initial_embeddings, p=2, dim=1)\n",
    "initial_centroid = initial_embeddings.mean(dim=0)\n",
    "initial_centroid_norm = initial_centroid.norm().item()\n",
    "\n",
    "print(f\"✓ Saved initial embeddings (step 0)\")\n",
    "print(f\"  Shape: {initial_embeddings.shape}\")\n",
    "print(f\"  Token L2 norms: min={initial_norms.min().item():.6f}, max={initial_norms.max().item():.6f}, mean={initial_norms.mean().item():.6f}\")\n",
    "print(f\"  Centroid L2 norm: {initial_centroid_norm:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Define Snapshot Callback\n",
    "\n",
    "Custom callback that saves the embedding matrix after every training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Snapshot callback defined\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingSnapshotCallback(TrainerCallback):\n",
    "    \"\"\"Save embedding matrix after every training step.\"\"\"\n",
    "    \n",
    "    def __init__(self, snapshot_dir):\n",
    "        self.snapshot_dir = snapshot_dir\n",
    "    \n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        step = state.global_step\n",
    "        \n",
    "        # Extract embedding matrix and move to CPU for saving\n",
    "        embeddings = model.transformer.wte.weight.data.clone().cpu()\n",
    "        \n",
    "        # Save with zero-padded step number\n",
    "        filename = f\"step_{step:07d}.safetensors\"\n",
    "        save_file(\n",
    "            {'embeddings': embeddings},\n",
    "            os.path.join(self.snapshot_dir, filename)\n",
    "        )\n",
    "        \n",
    "        # Print progress every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            centroid = embeddings.mean(dim=0)\n",
    "            centroid_norm = centroid.norm().item()\n",
    "            print(f\"[Step {step:5d}] Saved snapshot | Centroid L2 norm: {centroid_norm:.6f}\")\n",
    "        \n",
    "        return control\n",
    "\n",
    "print(\"✓ Snapshot callback defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Device: mps\n",
      "  Steps: 5,000\n",
      "  Batch size: 16\n",
      "  Learning rate: 0.001\n",
      "  Weight decay: 0.01\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_output\",\n",
    "    max_steps=NUM_TRAIN_STEPS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_steps=100,\n",
    "    save_steps=10000,  # We're saving embeddings manually, so don't need frequent full checkpoints\n",
    "    save_total_limit=1,  # Only keep latest full checkpoint\n",
    "    seed=RANDOM_SEED,\n",
    "    dataloader_num_workers=0,\n",
    "    use_cpu=(DEVICE == \"cpu\"),  # Only force CPU if we detected CPU\n",
    "    # Trainer will auto-detect and use MPS or CUDA if available\n",
    "    bf16=True,  # Use bfloat16 if supported\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Steps: {training_args.max_steps:,}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Weight decay: {training_args.weight_decay}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer initialized with snapshot callback\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    callbacks=[EmbeddingSnapshotCallback(SNAPSHOT_DIR)],\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized with snapshot callback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Train!\n",
    "\n",
    "This will take several minutes. The callback will save embedding snapshots after every step and print progress every 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jefferyharrell/Projects/Azimuth_II/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 00:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.454500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.990800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.947600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.930600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.918400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.905300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.897800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.891200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.876400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.866600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.873500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.869800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.868400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.865800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.872400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.866300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.864500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.864100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.865100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.866400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.865300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.869100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step   100] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step   200] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step   300] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step   400] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step   500] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step   600] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step   700] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step   800] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step   900] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  1000] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  1100] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  1200] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  1300] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  1400] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  1500] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  1600] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  1700] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  1800] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  1900] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  2000] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  2100] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  2200] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  2300] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  2400] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  2500] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  2600] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  2700] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  2800] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  2900] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  3000] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  3100] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  3200] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  3300] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  3400] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  3500] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  3600] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  3700] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  3800] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  3900] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  4000] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  4100] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  4200] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  4300] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  4400] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  4500] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  4600] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  4700] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  4800] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  4900] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "[Step  5000] Saved snapshot | Centroid L2 norm: 8.187500\n",
      "\n",
      "================================================================================\n",
      "✓ Training complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Starting training...\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Training complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Verify Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cell-29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Snapshot verification:\n",
      "  Total snapshots saved: 5,001\n",
      "  Expected: 5,001 (including step 0)\n",
      "  ✓ All snapshots saved successfully\n",
      "\n",
      "First 5 snapshots:\n",
      "  step_0000000.safetensors\n",
      "  step_0000001.safetensors\n",
      "  step_0000002.safetensors\n",
      "  step_0000003.safetensors\n",
      "  step_0000004.safetensors\n",
      "\n",
      "Last 5 snapshots:\n",
      "  step_0004996.safetensors\n",
      "  step_0004997.safetensors\n",
      "  step_0004998.safetensors\n",
      "  step_0004999.safetensors\n",
      "  step_0005000.safetensors\n"
     ]
    }
   ],
   "source": [
    "snapshot_files = sorted(glob.glob(os.path.join(SNAPSHOT_DIR, \"*.safetensors\")))\n",
    "\n",
    "print(f\"\\nSnapshot verification:\")\n",
    "print(f\"  Total snapshots saved: {len(snapshot_files):,}\")\n",
    "print(f\"  Expected: {NUM_TRAIN_STEPS + 1:,} (including step 0)\")\n",
    "\n",
    "if len(snapshot_files) == NUM_TRAIN_STEPS + 1:\n",
    "    print(f\"  ✓ All snapshots saved successfully\")\n",
    "else:\n",
    "    print(f\"  ⚠ Snapshot count mismatch!\")\n",
    "\n",
    "# Show first and last few filenames\n",
    "print(f\"\\nFirst 5 snapshots:\")\n",
    "for f in snapshot_files[:5]:\n",
    "    print(f\"  {os.path.basename(f)}\")\n",
    "\n",
    "print(f\"\\nLast 5 snapshots:\")\n",
    "for f in snapshot_files[-5:]:\n",
    "    print(f\"  {os.path.basename(f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## Analyze Final Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cell-31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final embeddings (step 5000):\n",
      "  Token L2 norms: min=8.187500, max=8.250000, mean=8.187500\n",
      "  Centroid L2 norm: 8.187500\n",
      "\n",
      "Centroid displacement: 0.365234\n",
      "  (Distance between initial and final centroid locations)\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "# Load final embeddings\n",
    "final_embeddings = load_file(snapshot_files[-1])['embeddings']\n",
    "\n",
    "final_norms = torch.norm(final_embeddings, p=2, dim=1)\n",
    "final_centroid = final_embeddings.mean(dim=0)\n",
    "final_centroid_norm = final_centroid.norm().item()\n",
    "\n",
    "print(f\"\\nFinal embeddings (step {NUM_TRAIN_STEPS}):\")\n",
    "print(f\"  Token L2 norms: min={final_norms.min().item():.6f}, max={final_norms.max().item():.6f}, mean={final_norms.mean().item():.6f}\")\n",
    "print(f\"  Centroid L2 norm: {final_centroid_norm:.6f}\")\n",
    "\n",
    "# Compare to initial\n",
    "centroid_displacement = (final_centroid - initial_centroid).norm().item()\n",
    "print(f\"\\nCentroid displacement: {centroid_displacement:.6f}\")\n",
    "print(f\"  (Distance between initial and final centroid locations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cell-33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE\n",
      "================================================================================\n",
      "Model: GPT2 (2 layers, 2 heads, 64-dim)\n",
      "Vocabulary: 128 tokens\n",
      "Dead tokens: 51 (39.8%)\n",
      "Initialization: qwen\n",
      "Training steps: 5,000\n",
      "Snapshots saved: 5,001\n",
      "\n",
      "Output directory: ../data/embeddings_128vocab_qweninit\n",
      "\n",
      "Initial centroid norm: 8.187500\n",
      "Final centroid norm: 8.187500\n",
      "Centroid displacement: 0.365234\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Model: GPT2 ({N_LAYER} layers, {N_HEAD} heads, {HIDDEN_DIM}-dim)\")\n",
    "print(f\"Vocabulary: {VOCAB_SIZE} tokens\")\n",
    "print(f\"Dead tokens: {dead_tokens} ({100 * dead_tokens / VOCAB_SIZE:.1f}%)\")\n",
    "print(f\"Initialization: {INIT_MODE}\")\n",
    "print(f\"Training steps: {NUM_TRAIN_STEPS:,}\")\n",
    "print(f\"Snapshots saved: {len(snapshot_files):,}\")\n",
    "print(f\"\\nOutput directory: {SNAPSHOT_DIR}\")\n",
    "print(f\"\\nInitial centroid norm: {initial_centroid_norm:.6f}\")\n",
    "print(f\"Final centroid norm: {final_centroid_norm:.6f}\")\n",
    "print(f\"Centroid displacement: {centroid_displacement:.6f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
