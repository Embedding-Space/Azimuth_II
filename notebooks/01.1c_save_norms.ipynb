{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01.1c: Save Token Norms\n",
    "\n",
    "**Goal:** Compute norms for all tokens in gamma space and save to CSV for analysis.\n",
    "\n",
    "This is a simple **generator** notebook: compute the radial distances (norms) for all 151,936 tokens in gamma-prime (centered) space and save them with token indices.\n",
    "\n",
    "Output: CSV with columns `[token_id, norm_gamma]`\n",
    "\n",
    "This data will be used in 01.2d to investigate the spike in the radial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR_DIR = \"../data/tensors\"\n",
    "OUTPUT_DIR = \"../data/results\"\n",
    "OUTPUT_FILE = \"token_norms_gamma.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print(\"Imports loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Centered Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded γ' (gamma_centered):\n",
      "  Tokens: 151,936\n",
      "  Dimensions: 2,560\n"
     ]
    }
   ],
   "source": [
    "gamma_centered_path = Path(TENSOR_DIR) / \"gamma_centered_qwen3_4b_instruct_2507.safetensors\"\n",
    "gamma_centered = load_file(gamma_centered_path)['gamma_centered']\n",
    "\n",
    "N, d = gamma_centered.shape\n",
    "\n",
    "print(f\"Loaded γ' (gamma_centered):\")\n",
    "print(f\"  Tokens: {N:,}\")\n",
    "print(f\"  Dimensions: {d:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing norms for all tokens...\n",
      "\n",
      "Computed 151,936 norms\n",
      "  Mean: 1.040134 gamma units\n",
      "  Std: 0.188920 gamma units\n",
      "  Min: 0.153098 gamma units\n",
      "  Max: 1.568522 gamma units\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing norms for all tokens...\")\n",
    "norms = gamma_centered.norm(dim=1)\n",
    "\n",
    "print(f\"\\nComputed {len(norms):,} norms\")\n",
    "print(f\"  Mean: {norms.mean().item():.6f} gamma units\")\n",
    "print(f\"  Std: {norms.std().item():.6f} gamma units\")\n",
    "print(f\"  Min: {norms.min().item():.6f} gamma units\")\n",
    "print(f\"  Max: {norms.max().item():.6f} gamma units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame:\n",
      "  Shape: (151936, 2)\n",
      "\n",
      "First few rows:\n",
      "   token_id  norm_gamma\n",
      "0         0    1.172701\n",
      "1         1    1.221996\n",
      "2         2    1.097519\n",
      "3         3    1.075008\n",
      "4         4    1.109532\n",
      "5         5    1.164412\n",
      "6         6    1.208837\n",
      "7         7    1.218959\n",
      "8         8    1.005412\n",
      "9         9    1.208332\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with token IDs and norms\n",
    "df = pd.DataFrame({\n",
    "    'token_id': range(N),\n",
    "    'norm_gamma': norms.cpu().numpy()\n",
    "})\n",
    "\n",
    "print(f\"Created DataFrame:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 151,936 token norms to: ../data/results/token_norms_gamma.csv\n",
      "File size: 2412.4 KB\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if needed\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILE\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(df):,} token norms to: {output_path}\")\n",
    "print(f\"File size: {output_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Successfully computed and saved all token norms in gamma space.\n",
    "\n",
    "Output file: `data/results/token_norms_gamma.csv`\n",
    "\n",
    "Columns:\n",
    "- `token_id`: Integer index (0 to 151,935)\n",
    "- `norm_gamma`: Radial distance from centroid in gamma units\n",
    "\n",
    "This data is ready for analysis in 01.2d to investigate the central spike."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
