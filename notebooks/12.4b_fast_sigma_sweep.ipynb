{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.4b: Fast Sigma Sweep (Compute-and-Discard)\n",
    "\n",
    "**Goal:** Generate synthetic snowballs, compute statistics, and immediately discard embeddings.\n",
    "\n",
    "## The Fast Approach\n",
    "\n",
    "Instead of saving 21.5 GB of embeddings to disk, we:\n",
    "1. Generate batch of embeddings on GPU\n",
    "2. Compute all statistics we care about\n",
    "3. **Throw away embeddings** and keep only the tiny statistics\n",
    "4. Repeat\n",
    "\n",
    "This is ~100× faster because we skip all I/O overhead.\n",
    "\n",
    "## Output\n",
    "\n",
    "- CSV with comprehensive statistics per trial\n",
    "- Element-wise distribution plot (histogram + Q-Q plot)\n",
    "- Scatter plot: σ vs singleton count (if sweeping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "SIGMA = 3.281911e-03   # Measured from Qwen's dead tokens\n",
    "N_TRIALS = 1000        # Number of independent snowballs to generate\n",
    "N_TOKENS = 2100        # Qwen's dead token count\n",
    "HIDDEN_DIM = 2560      # Qwen's embedding dimension\n",
    "BATCH_SIZE = 10        # Trials per batch (adjust based on GPU memory)\n",
    "\n",
    "# Epsilon (bfloat16 precision at Qwen's scale)\n",
    "EPSILON = 5.9604645e-05  # From 01.3c\n",
    "TOUCHING_THRESHOLD = 2 * EPSILON  # Adjacency criterion\n",
    "\n",
    "# Output\n",
    "OUTPUT_CSV = \"../data/analysis/synthetic_comprehensive_sigma3.281911e-03_n10000.csv\"\n",
    "OUTPUT_DIR = \"../data/results/\"\n",
    "\n",
    "# Figure settings\n",
    "DPI = 150\n",
    "COLORMAP = 'inferno'\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MPS (Metal Performance Shaders) available\n",
      "  Device: mps\n",
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats as scipy_stats\n",
    "import networkx as nx\n",
    "import gc\n",
    "import time\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Setup device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"✓ MPS (Metal Performance Shaders) available\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"⚠ MPS not available, using CPU only\")\n",
    "\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Qwen Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen black hole centroid...\n",
      "\n",
      "✓ Centroid loaded to mps\n",
      "  Shape: torch.Size([2560])\n",
      "  Norm: 0.166061\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Qwen black hole centroid...\\n\")\n",
    "\n",
    "centroid_data = load_file(\"../data/tensors/black_hole_centroid_qwen3_4b.safetensors\")\n",
    "qwen_centroid = centroid_data['centroid'].to(torch.float32).to(device)\n",
    "\n",
    "print(f\"✓ Centroid loaded to {device}\")\n",
    "print(f\"  Shape: {qwen_centroid.shape}\")\n",
    "print(f\"  Norm: {qwen_centroid.norm().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Statistics functions defined\n"
     ]
    }
   ],
   "source": [
    "def compute_gini_coefficient(populations):\n",
    "    \"\"\"Compute Gini coefficient of inequality.\"\"\"\n",
    "    if len(populations) <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    sorted_pops = torch.sort(populations.float())[0]\n",
    "    n = len(sorted_pops)\n",
    "    \n",
    "    indices = torch.arange(1, n + 1, dtype=torch.float32)\n",
    "    numerator = 2 * torch.sum(indices * sorted_pops)\n",
    "    denominator = n * torch.sum(sorted_pops)\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    gini = (numerator / denominator) - (n + 1) / n\n",
    "    return gini.item()\n",
    "\n",
    "\n",
    "def compute_l_inf_stats(unique_vectors, epsilon):\n",
    "    \"\"\"Compute L∞ (Chebyshev) distance statistics.\"\"\"\n",
    "    n = len(unique_vectors)\n",
    "    \n",
    "    if n <= 1:\n",
    "        return {'max_l_inf': 0.0, 'mean_l_inf': 0.0, 'median_l_inf': 0.0}\n",
    "    \n",
    "    # Compute pairwise L∞ distances using cdist (memory efficient)\n",
    "    l_inf_matrix = torch.cdist(unique_vectors, unique_vectors, p=float('inf'))\n",
    "    \n",
    "    # Exclude diagonal (self-distances)\n",
    "    mask = ~torch.eye(n, dtype=torch.bool)\n",
    "    l_inf_values = l_inf_matrix[mask]\n",
    "    \n",
    "    # Normalize by epsilon\n",
    "    l_inf_values = l_inf_values / epsilon\n",
    "    \n",
    "    return {\n",
    "        'max_l_inf': l_inf_values.max().item(),\n",
    "        'mean_l_inf': l_inf_values.mean().item(),\n",
    "        'median_l_inf': l_inf_values.median().item(),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_topology(unique_vectors, threshold):\n",
    "    \"\"\"Compute graph topology statistics using NetworkX.\"\"\"\n",
    "    n = len(unique_vectors)\n",
    "    \n",
    "    if n == 0:\n",
    "        return {\n",
    "            'n_components': 0,\n",
    "            'n_isolated': 0,\n",
    "            'largest_component_size': 0,\n",
    "            'largest_component_density': 0.0,\n",
    "            'global_density': 0.0,\n",
    "        }\n",
    "    \n",
    "    if n == 1:\n",
    "        return {\n",
    "            'n_components': 1,\n",
    "            'n_isolated': 1,\n",
    "            'largest_component_size': 1,\n",
    "            'largest_component_density': 1.0,\n",
    "            'global_density': 1.0,\n",
    "        }\n",
    "    \n",
    "    # Compute pairwise L∞ distances using cdist (memory efficient)\n",
    "    l_inf_matrix = torch.cdist(unique_vectors, unique_vectors, p=float('inf'))\n",
    "    \n",
    "    # Build adjacency matrix (exclude self-loops)\n",
    "    adjacency = (l_inf_matrix <= threshold) & (~torch.eye(n, dtype=torch.bool))\n",
    "    \n",
    "    # Convert to NetworkX graph\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n))\n",
    "    edges = torch.nonzero(adjacency, as_tuple=False).tolist()\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    # Connected components\n",
    "    components = list(nx.connected_components(G))\n",
    "    component_sizes = sorted([len(c) for c in components], reverse=True)\n",
    "    \n",
    "    n_components = len(components)\n",
    "    largest_size = component_sizes[0] if component_sizes else 0\n",
    "    \n",
    "    # Isolated nodes (degree = 0)\n",
    "    n_isolated = sum(1 for node in G.nodes() if G.degree(node) == 0)\n",
    "    \n",
    "    # Density of largest component\n",
    "    if largest_size > 1:\n",
    "        largest_component = max(components, key=len)\n",
    "        subgraph = G.subgraph(largest_component)\n",
    "        n_edges = subgraph.number_of_edges()\n",
    "        max_edges = largest_size * (largest_size - 1) // 2\n",
    "        largest_density = n_edges / max_edges if max_edges > 0 else 0.0\n",
    "    else:\n",
    "        largest_density = 1.0 if largest_size == 1 else 0.0\n",
    "    \n",
    "    # Global density\n",
    "    n_edges = G.number_of_edges()\n",
    "    max_edges = n * (n - 1) // 2\n",
    "    global_density = n_edges / max_edges if max_edges > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'n_components': n_components,\n",
    "        'n_isolated': n_isolated,\n",
    "        'largest_component_size': largest_size,\n",
    "        'largest_component_density': largest_density,\n",
    "        'global_density': global_density,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_trial_statistics(embeddings, epsilon, touching_threshold):\n",
    "    \"\"\"Compute all statistics for a single trial.\"\"\"\n",
    "    # Move to CPU for torch.unique (not MPS-compatible)\n",
    "    embeddings_cpu = embeddings.cpu()\n",
    "    \n",
    "    # Find unique vectors and counts\n",
    "    unique_vectors, inverse_indices, counts = torch.unique(\n",
    "        embeddings_cpu,\n",
    "        dim=0,\n",
    "        return_inverse=True,\n",
    "        return_counts=True\n",
    "    )\n",
    "    \n",
    "    # Basic counts\n",
    "    n_tokens = len(embeddings)\n",
    "    n_unique = len(unique_vectors)\n",
    "    \n",
    "    # Black holes (count >= 2)\n",
    "    black_hole_mask = counts >= 2\n",
    "    n_black_holes = black_hole_mask.sum().item()\n",
    "    black_hole_population = counts[black_hole_mask].sum().item() if n_black_holes > 0 else 0\n",
    "    \n",
    "    # Singletons (count == 1)\n",
    "    singleton_mask = counts == 1\n",
    "    n_singletons = singleton_mask.sum().item()\n",
    "    \n",
    "    # Population stats\n",
    "    total_population = counts.sum().item()\n",
    "    \n",
    "    stats = {\n",
    "        'n_tokens': n_tokens,\n",
    "        'n_unique': n_unique,\n",
    "        'n_black_holes': n_black_holes,\n",
    "        'n_singletons': n_singletons,\n",
    "        'total_population': total_population,\n",
    "        'black_hole_population': black_hole_population,\n",
    "    }\n",
    "    \n",
    "    # Per-black-hole statistics\n",
    "    if n_black_holes > 0:\n",
    "        bh_populations = counts[black_hole_mask]\n",
    "        sorted_pops = torch.sort(bh_populations, descending=True)[0]\n",
    "        \n",
    "        stats['largest_bh'] = sorted_pops[0].item()\n",
    "        stats['smallest_bh'] = sorted_pops[-1].item()\n",
    "        stats['mean_bh_size'] = bh_populations.float().mean().item()\n",
    "        stats['median_bh_size'] = bh_populations.float().median().item()\n",
    "        stats['top2_population'] = sorted_pops[:2].sum().item() if len(sorted_pops) >= 2 else sorted_pops[0].item()\n",
    "        stats['gini_coefficient'] = compute_gini_coefficient(bh_populations)\n",
    "    else:\n",
    "        stats['largest_bh'] = 0\n",
    "        stats['smallest_bh'] = 0\n",
    "        stats['mean_bh_size'] = 0.0\n",
    "        stats['median_bh_size'] = 0.0\n",
    "        stats['top2_population'] = 0\n",
    "        stats['gini_coefficient'] = 0.0\n",
    "    \n",
    "    # Spatial extent\n",
    "    l_inf_stats = compute_l_inf_stats(unique_vectors, epsilon)\n",
    "    stats.update(l_inf_stats)\n",
    "    \n",
    "    # Topology\n",
    "    topology_stats = compute_topology(unique_vectors, touching_threshold)\n",
    "    stats.update(topology_stats)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "print(\"✓ Statistics functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Compute Statistics (Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 1,000 synthetic snowballs (compute-and-discard)...\n",
      "  σ = 3.282e-03\n",
      "  Batch size: 10\n",
      "  Shape per trial: [2100, 2560]\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e324f998e84a4572bfa8763abfa63591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m     all_elements_sample.append(centered.flatten().cpu())\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Compute all statistics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m stats = \u001b[43mcompute_trial_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPSILON\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOUCHING_THRESHOLD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m stats[\u001b[33m'\u001b[39m\u001b[33mtrial_id\u001b[39m\u001b[33m'\u001b[39m] = global_trial_idx\n\u001b[32m     43\u001b[39m results.append(stats)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36mcompute_trial_statistics\u001b[39m\u001b[34m(embeddings, epsilon, touching_threshold)\u001b[39m\n\u001b[32m    167\u001b[39m     stats[\u001b[33m'\u001b[39m\u001b[33mgini_coefficient\u001b[39m\u001b[33m'\u001b[39m] = \u001b[32m0.0\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# Spatial extent\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m l_inf_stats = \u001b[43mcompute_l_inf_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m stats.update(l_inf_stats)\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# Topology\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mcompute_l_inf_stats\u001b[39m\u001b[34m(unique_vectors, epsilon)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Normalize by epsilon\u001b[39;00m\n\u001b[32m     35\u001b[39m l_inf_values = l_inf_values / epsilon\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_l_inf\u001b[39m\u001b[33m'\u001b[39m: l_inf_values.max().item(),\n\u001b[32m     39\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmean_l_inf\u001b[39m\u001b[33m'\u001b[39m: l_inf_values.mean().item(),\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmedian_l_inf\u001b[39m\u001b[33m'\u001b[39m: \u001b[43ml_inf_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.item(),\n\u001b[32m     41\u001b[39m }\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(f\"\\nGenerating {N_TRIALS:,} synthetic snowballs (compute-and-discard)...\")\n",
    "print(f\"  σ = {SIGMA:.3e}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Shape per trial: [{N_TOKENS}, {HIDDEN_DIM}]\\n\")\n",
    "\n",
    "results = []\n",
    "n_batches = (N_TRIALS + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "# Track all element values for distribution plot\n",
    "all_elements_sample = []  # Sample for plotting (don't keep all 5.4M values)\n",
    "sample_every_n_trials = max(1, N_TRIALS // 1000)  # Keep ~1000 trials worth\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for batch_idx in tqdm(range(n_batches), desc=\"Processing batches\"):\n",
    "    # Determine batch size\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, N_TRIALS)\n",
    "    current_batch_size = end_idx - start_idx\n",
    "    \n",
    "    # Generate batch on device\n",
    "    noise = torch.randn(current_batch_size, N_TOKENS, HIDDEN_DIM, dtype=torch.float32, device=device) * SIGMA\n",
    "    embeddings_batch = qwen_centroid.unsqueeze(0).unsqueeze(0) + noise\n",
    "    \n",
    "    # Quantize to bfloat16 then back to float32\n",
    "    embeddings_batch = embeddings_batch.to(torch.bfloat16).to(torch.float32)\n",
    "    \n",
    "    # Process each trial in batch\n",
    "    for trial_idx in range(current_batch_size):\n",
    "        embeddings = embeddings_batch[trial_idx]\n",
    "        \n",
    "        # Sample elements for distribution plot\n",
    "        global_trial_idx = start_idx + trial_idx\n",
    "        if global_trial_idx % sample_every_n_trials == 0:\n",
    "            # Keep centered noise (not absolute embeddings)\n",
    "            centered = embeddings - qwen_centroid.unsqueeze(0)\n",
    "            all_elements_sample.append(centered.flatten().cpu())\n",
    "        \n",
    "        # Compute all statistics\n",
    "        stats = compute_trial_statistics(embeddings, EPSILON, TOUCHING_THRESHOLD)\n",
    "        stats['trial_id'] = global_trial_idx\n",
    "        \n",
    "        results.append(stats)\n",
    "    \n",
    "    # Free batch memory\n",
    "    del noise, embeddings_batch\n",
    "    gc.collect()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ Processed {N_TRIALS:,} trials in {elapsed_time:.1f}s\")\n",
    "print(f\"  ({N_TRIALS/elapsed_time:.1f} trials/sec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "output_path = Path(OUTPUT_CSV)\n",
    "df_results.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_path}\")\n",
    "print(f\"  Rows: {len(df_results):,}\")\n",
    "print(f\"  Columns: {len(df_results.columns)}\")\n",
    "print(f\"  Size: {output_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY STATISTICS (n = {N_TRIALS:,} trials)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Key metrics\n",
    "key_metrics = [\n",
    "    'n_unique', 'n_black_holes', 'n_singletons',\n",
    "    'black_hole_population', 'largest_bh',\n",
    "    'gini_coefficient', 'max_l_inf',\n",
    "    'n_components', 'largest_component_density', 'global_density'\n",
    "]\n",
    "\n",
    "for metric in key_metrics:\n",
    "    mean = df_results[metric].mean()\n",
    "    std = df_results[metric].std()\n",
    "    min_val = df_results[metric].min()\n",
    "    max_val = df_results[metric].max()\n",
    "    \n",
    "    if std < 0.001:\n",
    "        print(f\"{metric:30s} mean={mean:.1f},  std={std:.3f},  range=[{min_val:.1f}, {max_val:.1f}]\")\n",
    "    else:\n",
    "        print(f\"{metric:30s} {mean:.1f} ± {std:.1f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1: Element-wise Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate sampled elements\n",
    "all_elements = torch.cat(all_elements_sample).numpy()\n",
    "\n",
    "print(f\"\\nElement-wise distribution (sampled from {len(all_elements_sample)} trials)\")\n",
    "print(f\"  Total elements: {len(all_elements):,}\")\n",
    "print(f\"  Mean: {all_elements.mean():.3e}\")\n",
    "print(f\"  Std: {all_elements.std():.3e}\")\n",
    "\n",
    "# Compute distribution stats\n",
    "measured_mean = all_elements.mean()\n",
    "measured_std = all_elements.std()\n",
    "skewness = scipy_stats.skew(all_elements)\n",
    "kurtosis = scipy_stats.kurtosis(all_elements)\n",
    "\n",
    "print(f\"  Skewness: {skewness:+.3f}\")\n",
    "print(f\"  Kurtosis: {kurtosis:+.3f}\")\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), dpi=DPI)\n",
    "\n",
    "# Histogram\n",
    "ax = axes[0]\n",
    "ax.hist(all_elements, bins=100, color='steelblue', alpha=0.7, edgecolor='black', density=True)\n",
    "\n",
    "# Overlay Gaussian with measured parameters\n",
    "x = np.linspace(all_elements.min(), all_elements.max(), 1000)\n",
    "gaussian = scipy_stats.norm.pdf(x, loc=measured_mean, scale=measured_std)\n",
    "ax.plot(x, gaussian, 'r-', linewidth=2, label=f'N({measured_mean:.2e}, {measured_std:.2e}²)')\n",
    "\n",
    "ax.set_xlabel('Value', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Density', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Element-wise Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "ax = axes[1]\n",
    "scipy_stats.probplot(all_elements, dist=\"norm\", plot=ax)\n",
    "ax.set_title('Q-Q Plot (Normal Distribution)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Synthetic Snowball Distribution (σ = {SIGMA:.3e})\\n(n = {len(all_elements_sample):,} sampled trials, d = 2,560 dimensions)',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "output_path = Path(OUTPUT_DIR) / f\"synthetic_distribution_sigma{SIGMA:.3e}.png\"\n",
    "plt.savefig(output_path, dpi=DPI, bbox_inches='tight')\n",
    "print(f\"\\n✓ Saved: {output_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size_kb = output_path.stat().st_size / 1024\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FAST GENERATION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Trials: {N_TRIALS:,}\")\n",
    "print(f\"σ = {SIGMA:.3e}\")\n",
    "print(f\"\\nGeneration time: {elapsed_time:.1f}s ({N_TRIALS/elapsed_time:.1f} trials/sec)\")\n",
    "print(f\"\\nOutput CSV: {OUTPUT_CSV}\")\n",
    "print(f\"Size: {file_size_kb:.1f} KB (vs ~21 GB if we saved embeddings!)\")\n",
    "print(f\"\\nSpeedup: ~{(21 * 1024 * 1024) / file_size_kb:.0f}× smaller\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
