{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14.2a: Dead Token Statistical Kinematics\n",
    "\n",
    "**Compute center-of-mass frame quantities for dead tokens**\n",
    "\n",
    "## The Goal\n",
    "\n",
    "Transform dead token dynamics into center-of-mass frame to separate:\n",
    "- **Bulk motion**: Coherent movement of the primordial atom as a whole\n",
    "- **Thermal motion**: Random internal motion (actual \"temperature\")\n",
    "\n",
    "This notebook is a **generator**: it computes and saves derived quantities for analysis notebooks to use.\n",
    "\n",
    "## What We Compute\n",
    "\n",
    "1. **Positions**: Reconstruct absolute embedding positions from deltas\n",
    "2. **Centroid**: Mean position of all dead tokens at each step\n",
    "3. **Bulk velocity**: Step-to-step displacement of centroid\n",
    "4. **Thermal velocities**: Token velocities in center-of-mass frame (deltas minus bulk velocity)\n",
    "\n",
    "## Output\n",
    "\n",
    "`data/instrumented_run/dead_token_kinematics.safetensors` (~100 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "INPUT_PATH = \"../data/instrumented_run/gradient_delta_history.safetensors\"\n",
    "OUTPUT_PATH = \"../data/instrumented_run/dead_token_kinematics.safetensors\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../data/instrumented_run/gradient_delta_history.safetensors\n",
      "\n",
      "  Recorded steps: 10000\n",
      "  Dead tokens: 51\n",
      "  Step range: 0 to 10000\n",
      "\n",
      "✓ Data loaded\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading: {INPUT_PATH}\")\n",
    "\n",
    "data = load_file(INPUT_PATH)\n",
    "\n",
    "recorded_steps = data['recorded_steps']\n",
    "dead_token_ids = data['dead_token_ids']\n",
    "deltas = data['deltas']  # [n_recorded, vocab_size, hidden_dim]\n",
    "\n",
    "n_recorded = len(recorded_steps)\n",
    "n_dead = len(dead_token_ids)\n",
    "\n",
    "print(f\"\\n  Recorded steps: {n_recorded}\")\n",
    "print(f\"  Dead tokens: {n_dead}\")\n",
    "print(f\"  Step range: {recorded_steps[0]} to {recorded_steps[-1]}\")\n",
    "print(f\"\\n✓ Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Dead Token Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dead token deltas...\n",
      "✓ Dead token deltas: torch.Size([10000, 51, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting dead token deltas...\")\n",
    "\n",
    "# Index into deltas to get only dead tokens\n",
    "dead_deltas = deltas[:, dead_token_ids, :]  # [n_recorded, n_dead, hidden_dim]\n",
    "\n",
    "print(f\"✓ Dead token deltas: {dead_deltas.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Absolute Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing absolute positions...\n",
      "✓ Positions computed: torch.Size([10000, 51, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing absolute positions...\")\n",
    "\n",
    "# Cumulative sum of deltas gives absolute position at each step\n",
    "positions = torch.cumsum(dead_deltas, dim=0)  # [n_recorded, n_dead, hidden_dim]\n",
    "\n",
    "print(f\"✓ Positions computed: {positions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Centroid Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroid trajectory...\n",
      "✓ Centroid computed: torch.Size([10000, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing centroid trajectory...\")\n",
    "\n",
    "# Mean position over all dead tokens at each step\n",
    "centroid = positions.mean(dim=1)  # [n_recorded, hidden_dim]\n",
    "\n",
    "print(f\"✓ Centroid computed: {centroid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Bulk Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bulk velocity...\n",
      "✓ Bulk velocity computed: torch.Size([10000, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing bulk velocity...\")\n",
    "\n",
    "# Step-to-step displacement of centroid\n",
    "# Note: bulk_velocity[0] will be centroid[0] - 0 (assuming start at origin)\n",
    "bulk_velocity = torch.zeros_like(centroid)\n",
    "bulk_velocity[0] = centroid[0]\n",
    "bulk_velocity[1:] = centroid[1:] - centroid[:-1]\n",
    "\n",
    "print(f\"✓ Bulk velocity computed: {bulk_velocity.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Thermal Velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing thermal velocities...\n",
      "✓ Thermal velocities computed: torch.Size([10000, 51, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing thermal velocities...\")\n",
    "\n",
    "# Velocity of each token minus the bulk velocity\n",
    "# dead_deltas is velocity in lab frame, bulk_velocity is centroid velocity\n",
    "# Broadcast bulk_velocity across the token dimension\n",
    "thermal_velocities = dead_deltas - bulk_velocity.unsqueeze(1)  # [n_recorded, n_dead, hidden_dim]\n",
    "\n",
    "print(f\"✓ Thermal velocities computed: {thermal_velocities.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SANITY CHECKS\n",
      "================================================================================\n",
      "\n",
      "Thermal velocities should sum to zero (by construction):\n",
      "  Max norm of sum: 5.215406e-07\n",
      "  Mean norm of sum: 4.724886e-09\n",
      "  (Should be ~machine epsilon)\n",
      "\n",
      "Centroid trajectory:\n",
      "  Initial norm: 0.008006\n",
      "  Final norm: 0.632026\n",
      "  Min norm: 0.008006 (step 0)\n",
      "  Max norm: 0.632026 (step 1408)\n",
      "\n",
      "Velocity comparison at t=0:\n",
      "  Bulk velocity magnitude: 8.006386e-03\n",
      "  RMS thermal velocity: 1.373231e-05\n",
      "  Ratio (bulk/thermal): 583.03\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SANITY CHECKS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Check that thermal velocities sum to zero (by construction)\n",
    "thermal_sum = thermal_velocities.sum(dim=1)  # [n_recorded, hidden_dim]\n",
    "thermal_sum_norm = torch.norm(thermal_sum, dim=1).numpy()  # [n_recorded]\n",
    "\n",
    "print(f\"Thermal velocities should sum to zero (by construction):\")\n",
    "print(f\"  Max norm of sum: {thermal_sum_norm.max():.6e}\")\n",
    "print(f\"  Mean norm of sum: {thermal_sum_norm.mean():.6e}\")\n",
    "print(f\"  (Should be ~machine epsilon)\\n\")\n",
    "\n",
    "# Initial centroid norm\n",
    "centroid_norm = torch.norm(centroid, dim=1).numpy()\n",
    "print(f\"Centroid trajectory:\")\n",
    "print(f\"  Initial norm: {centroid_norm[0]:.6f}\")\n",
    "print(f\"  Final norm: {centroid_norm[-1]:.6f}\")\n",
    "print(f\"  Min norm: {centroid_norm.min():.6f} (step {centroid_norm.argmin()})\")\n",
    "print(f\"  Max norm: {centroid_norm.max():.6f} (step {centroid_norm.argmax()})\\n\")\n",
    "\n",
    "# Bulk vs thermal velocity magnitudes at t=0\n",
    "bulk_vel_norm = torch.norm(bulk_velocity, dim=1).numpy()\n",
    "thermal_vel_norms = torch.norm(thermal_velocities, dim=2).numpy()  # [n_recorded, n_dead]\n",
    "thermal_rms = np.sqrt((thermal_vel_norms**2).mean(axis=1))  # [n_recorded]\n",
    "\n",
    "print(f\"Velocity comparison at t=0:\")\n",
    "print(f\"  Bulk velocity magnitude: {bulk_vel_norm[0]:.6e}\")\n",
    "print(f\"  RMS thermal velocity: {thermal_rms[0]:.6e}\")\n",
    "print(f\"  Ratio (bulk/thermal): {bulk_vel_norm[0] / thermal_rms[0]:.2f}\\n\")\n",
    "\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving to: ../data/instrumented_run/dead_token_kinematics.safetensors\n",
      "\n",
      "✓ Saved successfully\n",
      "  File size: 254.0 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving to: {OUTPUT_PATH}\")\n",
    "\n",
    "save_dict = {\n",
    "    'recorded_steps': recorded_steps,\n",
    "    'dead_token_ids': dead_token_ids,\n",
    "    'positions': positions,\n",
    "    'centroid': centroid,\n",
    "    'bulk_velocity': bulk_velocity,\n",
    "    'thermal_velocities': thermal_velocities,\n",
    "}\n",
    "\n",
    "save_file(save_dict, OUTPUT_PATH)\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "file_size_mb = os.path.getsize(OUTPUT_PATH) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n✓ Saved successfully\")\n",
    "print(f\"  File size: {file_size_mb:.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth-ii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
