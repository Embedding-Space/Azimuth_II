{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07.3l: Thai Tokenization Test - Dead Vocabulary Hypothesis\n",
    "\n",
    "**Goal:** Test whether the 1,579 degenerate Thai tokens are **unreachable dead vocabulary** by tokenizing real Thai text.\n",
    "\n",
    "**Hypothesis:** The degenerate Thai tokens exist in the vocabulary but are never emitted by the tokenizer. When tokenizing real Thai text:\n",
    "- Degenerate Thai tokens will have ~0% usage\n",
    "- Non-degenerate Thai tokens will have high usage\n",
    "\n",
    "**Method:**\n",
    "1. Fetch Thai Wikipedia article on Thailand (ประเทศไทย) - comprehensive, real-world Thai text\n",
    "2. Tokenize it with Qwen's tokenizer\n",
    "3. Extract unique token IDs that were emitted\n",
    "4. Compare against:\n",
    "   - Degenerate Thai tokens (1,579 tokens)\n",
    "   - Non-degenerate Thai tokens (988 tokens)\n",
    "5. Compute overlap percentages\n",
    "\n",
    "**Smoking gun metric:** If <5% of emitted Thai tokens are degenerate, the hypothesis is confirmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSOR_DIR = \"../data/tensors\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# Thai token IDs (from 07.3k)\n",
    "THAI_IDS_FILE = \"thai_token_ids.safetensors\"\n",
    "\n",
    "# Wikipedia article\n",
    "WIKI_TITLE = \"ประเทศไทย\"  # Thailand in Thai\n",
    "WIKI_LANG = \"th\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import requests\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Imports loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "vocab_size = len(tokenizer)\n",
    "print(f\"  Vocabulary size: {vocab_size:,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Thai Token Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(TENSOR_DIR)\n",
    "\n",
    "print(\"Loading Thai token classifications from 07.3k...\")\n",
    "thai_data = load_file(data_dir / THAI_IDS_FILE)\n",
    "\n",
    "all_thai_ids = thai_data['all_thai_ids'].numpy()\n",
    "thai_degenerate_ids = thai_data['thai_degenerate_ids'].numpy()\n",
    "thai_non_degenerate_ids = thai_data['thai_non_degenerate_ids'].numpy()\n",
    "\n",
    "print(f\"  All Thai tokens:           {len(all_thai_ids):,}\")\n",
    "print(f\"  Degenerate Thai:           {len(thai_degenerate_ids):,}\")\n",
    "print(f\"  Non-degenerate Thai:       {len(thai_non_degenerate_ids):,}\")\n",
    "print()\n",
    "\n",
    "# Convert to sets for fast membership testing\n",
    "thai_degenerate_set = set(thai_degenerate_ids)\n",
    "thai_non_degenerate_set = set(thai_non_degenerate_ids)\n",
    "all_thai_set = set(all_thai_ids)\n",
    "\n",
    "print(\"Thai token sets created.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Thai Wikipedia Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikipedia_text(title, lang=\"en\"):\n",
    "    \"\"\"Fetch plain text from Wikipedia article.\"\"\"\n",
    "    url = f\"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{title}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Get extract (summary)\n",
    "        extract = data.get('extract', '')\n",
    "        \n",
    "        # Also try to get full text if available\n",
    "        # For full article, we need different endpoint\n",
    "        full_url = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'titles': title,\n",
    "            'prop': 'extracts',\n",
    "            'explaintext': True,\n",
    "            'exsectionformat': 'plain'\n",
    "        }\n",
    "        \n",
    "        full_response = requests.get(full_url, params=params)\n",
    "        full_response.raise_for_status()\n",
    "        full_data = full_response.json()\n",
    "        \n",
    "        # Extract text from pages\n",
    "        pages = full_data.get('query', {}).get('pages', {})\n",
    "        for page_id, page_data in pages.items():\n",
    "            if 'extract' in page_data:\n",
    "                return page_data['extract']\n",
    "        \n",
    "        # Fallback to summary\n",
    "        return extract\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Wikipedia article: {e}\")\n",
    "        return None\n",
    "\n",
    "print(f\"Fetching Thai Wikipedia article: '{WIKI_TITLE}'...\")\n",
    "thai_text = fetch_wikipedia_text(WIKI_TITLE, WIKI_LANG)\n",
    "\n",
    "if thai_text:\n",
    "    print(f\"✓ Article fetched successfully\")\n",
    "    print(f\"  Length: {len(thai_text):,} characters\")\n",
    "    print(f\"  Preview (first 200 chars): {thai_text[:200]}...\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"✗ Failed to fetch article\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Thai Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if thai_text:\n",
    "    print(\"Tokenizing Thai text...\")\n",
    "    \n",
    "    # Tokenize\n",
    "    token_ids = tokenizer.encode(thai_text, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"  Total tokens: {len(token_ids):,}\")\n",
    "    \n",
    "    # Get unique token IDs\n",
    "    unique_token_ids = np.unique(token_ids)\n",
    "    n_unique = len(unique_token_ids)\n",
    "    \n",
    "    print(f\"  Unique tokens: {n_unique:,}\")\n",
    "    print()\n",
    "    \n",
    "    # Create set for fast lookup\n",
    "    emitted_set = set(unique_token_ids)\n",
    "    \n",
    "    print(\"Tokenization complete.\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"No text to tokenize.\")\n",
    "    unique_token_ids = np.array([])\n",
    "    emitted_set = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Token Usage: Thai vs Non-Thai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(emitted_set) > 0:\n",
    "    print(\"=\"*100)\n",
    "    print(\"TOKEN USAGE ANALYSIS\")\n",
    "    print(\"=\"*100)\n",
    "    print()\n",
    "    \n",
    "    # Thai tokens in emitted set\n",
    "    emitted_thai = emitted_set & all_thai_set\n",
    "    emitted_non_thai = emitted_set - all_thai_set\n",
    "    \n",
    "    n_emitted_thai = len(emitted_thai)\n",
    "    n_emitted_non_thai = len(emitted_non_thai)\n",
    "    \n",
    "    print(f\"Emitted tokens breakdown:\")\n",
    "    print(f\"  Total unique:    {len(emitted_set):,}\")\n",
    "    print(f\"  Thai tokens:     {n_emitted_thai:,}  ({100*n_emitted_thai/len(emitted_set):.1f}%)\")\n",
    "    print(f\"  Non-Thai tokens: {n_emitted_non_thai:,}  ({100*n_emitted_non_thai/len(emitted_set):.1f}%)\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"No tokens emitted.\")\n",
    "    emitted_thai = set()\n",
    "    n_emitted_thai = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical Test: Degenerate vs Non-Degenerate Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_emitted_thai > 0:\n",
    "    print(\"=\"*100)\n",
    "    print(\"HYPOTHESIS TEST: DEAD VOCABULARY\")\n",
    "    print(\"=\"*100)\n",
    "    print()\n",
    "    \n",
    "    # Overlap with degenerate Thai\n",
    "    emitted_degenerate = emitted_set & thai_degenerate_set\n",
    "    n_emitted_degenerate = len(emitted_degenerate)\n",
    "    \n",
    "    # Overlap with non-degenerate Thai\n",
    "    emitted_non_degenerate = emitted_set & thai_non_degenerate_set\n",
    "    n_emitted_non_degenerate = len(emitted_non_degenerate)\n",
    "    \n",
    "    print(f\"Emitted Thai tokens ({n_emitted_thai} total):\")\n",
    "    print(f\"  From degenerate set:     {n_emitted_degenerate:,}  ({100*n_emitted_degenerate/n_emitted_thai:.1f}% of emitted Thai)\")\n",
    "    print(f\"  From non-degenerate set: {n_emitted_non_degenerate:,}  ({100*n_emitted_non_degenerate/n_emitted_thai:.1f}% of emitted Thai)\")\n",
    "    print()\n",
    "    \n",
    "    # Usage rates\n",
    "    degenerate_usage_rate = 100 * n_emitted_degenerate / len(thai_degenerate_set)\n",
    "    non_degenerate_usage_rate = 100 * n_emitted_non_degenerate / len(thai_non_degenerate_set)\n",
    "    \n",
    "    print(f\"Usage rates:\")\n",
    "    print(f\"  Degenerate Thai:     {n_emitted_degenerate:,} / {len(thai_degenerate_set):,} used  ({degenerate_usage_rate:.1f}%)\")\n",
    "    print(f\"  Non-degenerate Thai: {n_emitted_non_degenerate:,} / {len(thai_non_degenerate_set):,} used  ({non_degenerate_usage_rate:.1f}%)\")\n",
    "    print()\n",
    "    \n",
    "    # Verdict\n",
    "    pct_emitted_degenerate = 100 * n_emitted_degenerate / n_emitted_thai\n",
    "    \n",
    "    print(f\"**Smoking gun metric:** {pct_emitted_degenerate:.1f}% of emitted Thai tokens are degenerate\")\n",
    "    print()\n",
    "    \n",
    "    if pct_emitted_degenerate < 5:\n",
    "        print(f\"✓ HYPOTHESIS CONFIRMED: Degenerate tokens are dead vocabulary!\")\n",
    "        print(f\"  - Only {pct_emitted_degenerate:.1f}% of emitted Thai tokens came from degenerate set\")\n",
    "        print(f\"  - The tokenizer strongly prefers non-degenerate tokens\")\n",
    "        print(f\"  - The 1,579 degenerate Thai tokens are unreachable in practice\")\n",
    "    elif pct_emitted_degenerate < 20:\n",
    "        print(f\"~ HYPOTHESIS PARTIALLY CONFIRMED: Degenerate tokens are rarely used\")\n",
    "        print(f\"  - {pct_emitted_degenerate:.1f}% of emitted Thai tokens came from degenerate set\")\n",
    "        print(f\"  - Most tokenization uses non-degenerate tokens, but some degenerate tokens appear\")\n",
    "    else:\n",
    "        print(f\"✗ HYPOTHESIS REJECTED: Degenerate tokens are actively used\")\n",
    "        print(f\"  - {pct_emitted_degenerate:.1f}% of emitted Thai tokens came from degenerate set\")\n",
    "        print(f\"  - The degenerate tokens are NOT dead vocabulary\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"No Thai tokens emitted - cannot test hypothesis.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Emitted Degenerate Tokens (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_emitted_thai > 0 and n_emitted_degenerate > 0:\n",
    "    print(\"=\"*100)\n",
    "    print(f\"EMITTED DEGENERATE TOKENS ({n_emitted_degenerate} found)\")\n",
    "    print(\"=\"*100)\n",
    "    print()\n",
    "    \n",
    "    # Show all of them (or first 50 if too many)\n",
    "    degenerate_list = sorted(list(emitted_degenerate))\n",
    "    show_count = min(50, len(degenerate_list))\n",
    "    \n",
    "    print(f\"First {show_count} degenerate tokens that were emitted:\")\n",
    "    print()\n",
    "    for token_id in degenerate_list[:show_count]:\n",
    "        s = tokenizer.decode([token_id])\n",
    "        s_display = repr(s)[1:-1]\n",
    "        if len(s_display) > 40:\n",
    "            s_display = s_display[:37] + \"...\"\n",
    "        print(f\"  [{token_id:6d}] {s_display}\")\n",
    "    \n",
    "    if len(degenerate_list) > show_count:\n",
    "        print(f\"  ... and {len(degenerate_list) - show_count} more\")\n",
    "    print()\n",
    "elif n_emitted_thai > 0:\n",
    "    print(\"✓ No degenerate tokens were emitted during tokenization.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Emitted Non-Degenerate Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_emitted_thai > 0 and n_emitted_non_degenerate > 0:\n",
    "    print(\"=\"*100)\n",
    "    print(f\"EMITTED NON-DEGENERATE TOKENS (sample of {n_emitted_non_degenerate})\")\n",
    "    print(\"=\"*100)\n",
    "    print()\n",
    "    \n",
    "    # Show first 30\n",
    "    non_degenerate_list = sorted(list(emitted_non_degenerate))\n",
    "    show_count = min(30, len(non_degenerate_list))\n",
    "    \n",
    "    print(f\"First {show_count} non-degenerate tokens:\")\n",
    "    print()\n",
    "    for token_id in non_degenerate_list[:show_count]:\n",
    "        s = tokenizer.decode([token_id])\n",
    "        s_display = repr(s)[1:-1]\n",
    "        if len(s_display) > 40:\n",
    "            s_display = s_display[:37] + \"...\"\n",
    "        print(f\"  [{token_id:6d}] {s_display}\")\n",
    "    \n",
    "    if len(non_degenerate_list) > show_count:\n",
    "        print(f\"  ... and {len(non_degenerate_list) - show_count} more\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Usage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_emitted_thai > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), dpi=100)\n",
    "    \n",
    "    # Pie chart: Emitted Thai composition\n",
    "    labels = ['Degenerate', 'Non-degenerate']\n",
    "    sizes = [n_emitted_degenerate, n_emitted_non_degenerate]\n",
    "    colors = ['red', 'blue']\n",
    "    explode = (0.1, 0) if n_emitted_degenerate > 0 else (0, 0)\n",
    "    \n",
    "    ax1.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "            shadow=True, startangle=90)\n",
    "    ax1.set_title(f'Emitted Thai Tokens Composition\\n({n_emitted_thai} total)', fontsize=14)\n",
    "    \n",
    "    # Bar chart: Usage rates\n",
    "    categories = ['Degenerate\\n(1,579 tokens)', 'Non-degenerate\\n(988 tokens)']\n",
    "    usage_rates = [degenerate_usage_rate, non_degenerate_usage_rate]\n",
    "    colors_bar = ['red', 'blue']\n",
    "    \n",
    "    ax2.bar(categories, usage_rates, color=colors_bar, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_ylabel('Usage Rate (%)', fontsize=12)\n",
    "    ax2.set_title('Thai Token Usage Rates', fontsize=14)\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (cat, rate) in enumerate(zip(categories, usage_rates)):\n",
    "        ax2.text(i, rate + 2, f'{rate:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_emitted_thai > 0:\n",
    "    print(\"=\"*100)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print()\n",
    "    \n",
    "    print(f\"**Source:** Thai Wikipedia article '{WIKI_TITLE}'\")\n",
    "    print(f\"  Characters: {len(thai_text):,}\")\n",
    "    print(f\"  Tokens: {len(token_ids):,}\")\n",
    "    print(f\"  Unique tokens: {len(emitted_set):,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"**Thai token usage:**\")\n",
    "    print(f\"  Emitted Thai tokens: {n_emitted_thai:,}  ({100*n_emitted_thai/len(emitted_set):.1f}% of all tokens)\")\n",
    "    print(f\"    From degenerate:     {n_emitted_degenerate:,}  ({pct_emitted_degenerate:.1f}% of Thai)\")\n",
    "    print(f\"    From non-degenerate: {n_emitted_non_degenerate:,}  ({100*n_emitted_non_degenerate/n_emitted_thai:.1f}% of Thai)\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"**Usage rates:**\")\n",
    "    print(f\"  Degenerate tokens:     {degenerate_usage_rate:.1f}% used ({n_emitted_degenerate} / {len(thai_degenerate_set):,})\")\n",
    "    print(f\"  Non-degenerate tokens: {non_degenerate_usage_rate:.1f}% used ({n_emitted_non_degenerate} / {len(thai_non_degenerate_set):,})\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"**Conclusion:**\")\n",
    "    if pct_emitted_degenerate < 5:\n",
    "        print(f\"  The degenerate Thai tokens are DEAD VOCABULARY.\")\n",
    "        print(f\"  Only {pct_emitted_degenerate:.1f}% of Thai tokenization uses them.\")\n",
    "        print(f\"  The tokenizer systematically avoids the 1,579 degenerate tokens,\")\n",
    "        print(f\"  preferring the 988 non-degenerate tokens instead.\")\n",
    "        print()\n",
    "        print(f\"  This explains why they collapsed during training: they never get emitted,\")\n",
    "        print(f\"  so their embeddings never get gradient updates, and they drift to\")\n",
    "        print(f\"  initialization noise at the same point in embedding space.\")\n",
    "    elif pct_emitted_degenerate < 20:\n",
    "        print(f\"  The degenerate Thai tokens are RARELY USED.\")\n",
    "        print(f\"  {pct_emitted_degenerate:.1f}% of Thai tokenization uses them.\")\n",
    "        print(f\"  The hypothesis is partially supported: the tokenizer prefers non-degenerate\")\n",
    "        print(f\"  tokens but occasionally emits degenerate ones.\")\n",
    "    else:\n",
    "        print(f\"  The degenerate Thai tokens are ACTIVELY USED.\")\n",
    "        print(f\"  {pct_emitted_degenerate:.1f}% of Thai tokenization uses them.\")\n",
    "        print(f\"  The hypothesis is rejected: these tokens are not dead vocabulary.\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"Cannot complete analysis - no Thai tokens emitted.\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
